[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a quarto website for the hypertidy family of R packages for multi-dimensional and spatial data.\nHypertidy is an approach to spatial or multi-dimensional data in R based on the following principles:\nExamples of these principles are seen in these R packages.\nvapour\nraadtools and angstroms\ntidync, lazyrraster,\nsilicate, anglr, rbgm"
  },
  {
    "objectID": "about.html#gridded-data",
    "href": "about.html#gridded-data",
    "title": "About",
    "section": "Gridded data",
    "text": "Gridded data\nHypertidy recognizes that not all gridded data fit into the GIS raster conventions. Gridded data comes in many forms, geographic with longitude-latitude or projected spaces, with time and or depth dimensions, with different orderings of axes (i.e. time-first, the latitude-longitude), and with generally any arbitrary space. A space is simply a set of axes with particular units and projection, and yes we mean “space” and “projection” in the more general mathematical sense. Date-time data is a projection, there is a mapping of a particular set of values to the real line and the position on that line for particular instant is defined by the axis units and epoch.\nMesh and grid share the same meaning in some contexts."
  },
  {
    "objectID": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "href": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "title": "About",
    "section": "Structured vs. unstructured, topology vs. geometry",
    "text": "Structured vs. unstructured, topology vs. geometry\nArray-based data have a straight-forward relationship between a set of axes that have discrete steps. These are “structured grids”. Unstructured grids include triangulations, non-regularly binned histograms, tetrahedral meshes and ragged arrays.\nAn unstructured mesh (grid) is able to represent any data structure, but structured meshes have some advantages because of the regular indexing relationship between dimensions.\nGIS vector constructs “polygons”, “lines”, “points” are special case optimizations of the unstructured grid case. Polygons really are topologically identical to lines, and they are a dead-end in the broader scheme of dimensionality. Points and lines can are topologicaly 0-dimensional and 1-dimensional respectively, and this shape-constraint is the same no matter what geometric dimension they are defined in. A line can twist around a 4D space with x, y, z, t coordinates at its segment nodes or it can be constrained to single dimension with only one of those coordinates specifying its position. The topology of the line is completely independent of the geometry, if we treat the line as composed of topological primitives.\nPolygons are not composed of topological primitives, but they can be treated as being composed of line primitives."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "quarto-blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nThere’s several meanings floating around when you say the “GDAL warper”. It can mean\n\n\n\n\n\n\nApr 25, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nPLEASE NOTE (April 2022): this post has been migrated from an old site, and some details\n\n\n\n\n\n\nMay 29, 2019\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nFor some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\n\n\n\n\n\n\nSep 1, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nNOTE: this post has been resurrected from a 2017 post (April 2022).\n\n\n\n\n\n\nJul 25, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncode\n\n\n\n\nIn R, matrices are ordered row-wise:\n\n\n\n\n\n\nApr 17, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "",
    "text": "PLEASE NOTE (April 2022): this post has been migrated from an old site, and some details may have changed. There might an update to this post to reflect the rgl package as it is now. —\nThis post describes the mesh3d format used in the rgl package and particularly how colour properties are stored and used. There are recent changes to this behaviour (see ‘meshColor’), and previously the situation was not clearly documented."
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#rgl",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#rgl",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "rgl",
    "text": "rgl\nThe rgl package has long provided interactive 3D graphics for R. The neat thing for me about 3D graphics is the requirement for mesh forms of data, and the fact that meshes are extremely useful for very many tasks. When we plot data in 3D we necessarily have to convert the usual spatial types into mesh forms. You can see me discuss that in more detail in this talk."
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#the-mesh3d-format",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#the-mesh3d-format",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "The mesh3d format",
    "text": "The mesh3d format\nHere is an example of a mesh3d object, it stores two polygonal areas in a form ready for 3D graphics.\n\nmesh0 <- structure(list(vb = structure(c(0, 0, 0, 1, 0, 1, 0, 1, 0.75, \n                                1, 0, 1, 1, 0.8, 0, 1, 0.5, 0.7, 0, 1, 0.8, 0.6, 0, 1, 0.69, \n                                0, 0, 1, 0.2, 0.2, 0, 1, 0.5, 0.2, 0, 1, 0.5, 0.4, 0, 1, 0.3, \n                                0.6, 0, 1, 0.2, 0.4, 0, 1, 1.1, 0.63, 0, 1, 1.23, 0.3, 0, 1), .Dim = c(4L, 14L)), \n               it = structure(c(1L, 8L, 12L, 9L, 8L, 1L, 7L, 6L, 5L, \n                                5L, 4L, 3L, 2L, 1L, 12L, 9L, 1L, 7L, 5L, 3L, 2L, 2L, 12L, 11L, \n                                10L, 9L, 7L, 5L, 2L, 11L, 10L, 7L, 5L, 5L, 11L, 10L, 6L, 7L, \n                                14L, 14L, 13L, 6L), .Dim = c(3L, 14L)), \n               primitivetype = \"triangle\", \n               material = list(), \n               normals = NULL, \n               texcoords = NULL), \n               class = c(\"mesh3d\", \"shape3d\"))\n\n\nstr(mesh0)\n\nList of 6\n $ vb           : num [1:4, 1:14] 0 0 0 1 0 1 0 1 0.75 1 ...\n $ it           : int [1:3, 1:14] 1 8 12 9 8 1 7 6 5 5 ...\n $ primitivetype: chr \"triangle\"\n $ material     : list()\n $ normals      : NULL\n $ texcoords    : NULL\n - attr(*, \"class\")= chr [1:2] \"mesh3d\" \"shape3d\"\n\n\n(It’s not obvious about the polygons, please bear with me).\nThe following characterizes the structure.\n\ntwo matrix arrays vb and it\nvb has 4 rows and 14 columns, and contains floating point numbers\nit has 3 rows and 14 columns, and contains integers (starting at 1)\na primitivetype which is “triangle”\nan empty list of material propertes (this is the missing link for the polygons)\na NULL value for normals and texcoords, these won’t be discussed further (but see ?quadmesh::quadmesh for texture coordinates from spatial)\na class, this object is a mesh3d and inherits from shape3d\n\nThe vb array is the vertices, these are the corner coordinates of the elements of the mesh.\n\nplot(t(mesh0$vb), main = \"t(vb) - vertices\", xlab = \"X\", ylab = \"Y\")\n\n\n\n\nThe elements of this mesh are triangles, and these are specified by the index array it. Elements of a mesh are called primitives, hence the primitivetype here.\n\nplot(t(mesh0$vb), main = \"t(vb[, it]) - primitives\", xlab = \"X\", ylab = \"Y\")\npolygon(t(mesh0$vb[, rbind(mesh0$it, NA)]), col = rgb(0.6, 0.6, 0.6, 0.5))"
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#transpose",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#transpose",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Transpose",
    "text": "Transpose\nThese matrix arrays are transpose the way we usually use them in R, for now just remember that you must t()ranspose them for normal plotting, e.g. plot(t(mesh0$vb[1:2, ])) will give the expected scatter plot of the vertices. The reason these arrays are transpose is because each coordinate value is then contiguous in memory, each Y value is right next to its counterpart X, and Z (and W), and vb[it, ] provides a flat vector of XYZW values in a continuous block - this is a very important efficiency, and help explains why computer graphics use elements in a mesh form like this."
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#colours",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#colours",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Colours",
    "text": "Colours\nUnsurprisingly, if we set the material property to a constant we get a constant colour.\n\nwidgetfun <- function() {\n  view3d(0, phi = 8)\n  rglwidget()\n}\nmesh0$material$color <- \"red\"\nlibrary(rgl)\nclear3d()\nshade3d(mesh0, lit = FALSE); \nwidgetfun()\n\n\n\n\n\nIn the usual R way our singleton colour value is magically recycled across every part of the shape, and it’s all red. But, is it recycled by vertices or by primitive? Until recently it was only possible to tell by trying (or reading the source code).\nHere I think it’s easy to see that the two colours are specified at the vertices, and they bleed across each triangle accordingly. We also get a warning that the behaviour has recently changed.\n\nclear3d()\nmesh0$material$color <- c(\"firebrick\", \"black\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0)\nwidgetfun()\n\n\n\n\n\nThe default is to meshColor = \"vertices\", so let’s specify faces.\n\nclear3d()\nmesh0$material$color <- c(\"firebrick\", \"dodgerblue\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0, meshColor = \"faces\")\nwidgetfun()\n\n\n\n\n\nSometimes we get neighbouring triangles with the same colour, so let’s also add the edges.\n\nmesh0$vb[3, ] <- 0.01  ## vertical bias avoids z-fighting\n## material properties here override the recycling of internal colours\n## onto edges\nwire3d(mesh0, lwd = 5, color = \"black\")\nwidgetfun()\n\n\n\n\n\nIf we go a bit further we can see the original arrangement for this shape, two individual polygons that share a single edge.\nThis only works because I happen to know how this was created, and I know how this control of behaviour occurs in new rgl.\nThere are 12 triangles in the first polygon, and 2 in the second. (The original polygons can be seen here (left panel)).\n\nclear3d()\nmesh0$material$color <- rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2))\nshade3d(mesh0, meshColor = \"faces\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nIf we treat the colours as applying to each vertex, then we needed to propagate it to each vertex around each face (triangle), and this is what rgl now calls legacy behaviour.\n\nclear3d()\nmesh0$material$color <- rep(rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2)), each = 3)\nshade3d(mesh0, meshColor = \"legacy\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nWe cannot recreate this effect with meshColor = \"vertices\", because each of our vertices is actually unique. (It could be done by making the vb array every repeated vertex, and updating the index array but I can’t summon this up atm).\n\nclear3d()\nmesh0$material$color <- rep_len(c(\"firebrick\", \"dodgerblue\"), length.out = ncol(mesh0$vb))\nshade3d(mesh0, meshColor = \"vertices\", lit = FALSE)\nwidgetfun()"
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#primitives",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#primitives",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Primitives",
    "text": "Primitives\nThe other kind of element supported by mesh3d is a quad, specified by an ib array with 4 rows (ib versus it, 4 vertices versus 3) and the primitivetype = \"quad\".\nThe it values are an index into, i.e. the column number of the vertex array. The vertices, or coordinates, are stored by column in this structure, whereas normally we would store a coordinate per row.\nWhen I first explored mesh3d I was looking at a quad type mesh - and I was completely confused. Both vb and ib had four rows, and so while I understood that a quad must have 4 vertices (4 index values for every primitive), I did not understand why the vertices also had four rows.\n(There are other kinds of primitives in common use are edge, point, tetrahedron - but rgl has no formal class for these - in practice the edge type is referred to as segment in rgl, and tetrahedra are approximated by enclosing their shape with triangles)."
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Why does the vertex array have 4 rows?",
    "text": "Why does the vertex array have 4 rows?\nAll mesh3d objects have a vb array, and it always includes 4 rows.\nThe reason there are 4 rows in the vertex array is that these are homogeneous coordinates which …\n\nare ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied\n\n… yeah. For our purposes just think\n\nX, Y, Z in the usual sense and set W = 1.\n\n(Do not set W = 0 because your data will vanish to infinity when plotted with rgl, which is what those math folks are saying more or less)."
  },
  {
    "objectID": "posts/2019-05-29-recent-rgl-format-changes/index.html#quads",
    "href": "posts/2019-05-29-recent-rgl-format-changes/index.html#quads",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "QUADS",
    "text": "QUADS\nNow let’s get a quad type mesh from the real world.\n\n## remotes::install_github(\"hypertidy/ceramic\")\nlibrary(ceramic)\ntopo <- cc_elevation(raster::extent(-72, -69, -34, -32), zoom = 6)\n\nPreparing to download: 1 tiles at zoom = 6 from \nhttps://api.mapbox.com/v4/mapbox.terrain-rgb/\n\nqm <- quadmesh::quadmesh(topo)\n\nstr(qm)\n\nList of 8\n $ vb             : num [1:4, 1:60225] -8015493 -3761925 0 1 -8014270 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"x\" \"y\" \"z\" \"1\"\n  .. ..$ : NULL\n $ ib             : int [1:4, 1:59732] 1 2 277 276 2 3 278 277 3 4 ...\n $ primitivetype  : chr \"quad\"\n $ material       : list()\n $ normals        : NULL\n $ texcoords      : NULL\n $ raster_metadata:List of 7\n  ..$ xmn  : num -8015493\n  ..$ xmx  : num -7680393\n  ..$ ymn  : num -4028537\n  ..$ ymx  : num -3761925\n  ..$ ncols: int 274\n  ..$ nrows: int 218\n  ..$ crs  : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n $ crs            : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n - attr(*, \"class\")= chr [1:3] \"quadmesh\" \"mesh3d\" \"shape3d\"\n\n\nThis topographic raster from near Santiago is now a mesh3d subclassed to quadmesh. This adds two properties raster_metadata and crs, which under limited conditions allows reconstruction of the original raster data. To drop back to a generic mesh3d the easiest is to reproject the data.\n\n##remotes::install_github(\"hypertidy/reproj\")\nlibrary(reproj)\nqm_ll <- reproj(qm, \"+proj=longlat +datum=WGS84\")\n\nWarning in reproj.quadmesh(qm, \"+proj=longlat +datum=WGS84\"): quadmesh raster\ninformation cannot be preserved after reprojection, dropping to mesh3d class\n\n\nThis is a lossless reprojection, as it is equivalent to sf::sf_project(t(qm$vb[1:2, ]), from = qm$crs, to = \"+proj=longlat +datum=WGS84\") or with rgdal::project(, qm$crs, inv = TRUE).\nWe can plot this in the usual way with rgl, or see upcoming features in the mapdeck package.\n\nclear3d()\nshade3d(qm_ll, lit = TRUE, col = \"grey\")\naspect3d(1, 1, 0.1); \nview3d(0, phi = -60)\nrglwidget()\n\n\n\n\n\nTo put colours on this, we can do it by faces\n\nclear3d()\nqm_ll$material$color <- colourvalues::color_values(raster::values(topo))\nshade3d(qm_ll, meshColor = \"faces\", lit = TRUE)\nrglwidget()\n\n\n\n\n\n(each face is discretely coloured), or by vertex in the legacy mode.\nNot run, to save the size of the document.\n\nclear3d()\nqm_ll$material$color <-colourvalues::color_values(qm_ll$vb[3, qm_ll$ib])\n                                                   \nshade3d(qm_ll, meshColor = \"legacy\", lit = TRUE)\nrglwidget()"
  },
  {
    "objectID": "posts/2014-04-17-r-matrices-and-image/index.html",
    "href": "posts/2014-04-17-r-matrices-and-image/index.html",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m <- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] <- 0\nm[2, 1] <- -10\nm[3, 2] <- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17-r-matrices-and-image/index.html#data-placement-with-image",
    "href": "posts/2014-04-17-r-matrices-and-image/index.html#data-placement-with-image",
    "title": "R matrices and image",
    "section": "Data placement with image()",
    "text": "Data placement with image()\nThis is fairly obvious, each cell is painted as a discrete block with cell centres evenly spaced between 0 and 1.\n\nm <- matrix(1:12, 3)\nimage(m)\n\n\n\n\nWe didn’t give it any coordinates to position the image, so it made some up.\n\nimage(m, main = \"input coordinates are cell centres\")\nxx <- seq.int(0, 1, length.out = nrow(m))\nyy <- seq.int(0, 1, length.out = ncol(m))\nabline(h = yy, v = xx, lty = 2)\n\n\n\n\nThis lends itself to a convenient data structure.\n\ndat <- list(x = xx, y = yy, z = m)\nimage(dat)\ntext(expand.grid(xx, yy), lab = as.vector(m))\n\n\n\n\n\n## points(expand.grid(xx, yy))\n\nThe function image() has some hidden tricks.\n\nxcorner <- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner <- seq.int(0, 1, length.out = ncol(m) + 1L)\nprint(xcorner)\n\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\nprint(ycorner)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\n## [1] 0.00 0.25 0.50 0.75 1.00\n\nimage(xcorner, ycorner, m, main = \"input coordinates are cell corners\")\nabline(h = ycorner, v = xcorner)\n\n\n\n\nWe can even use non-regular coordinates.\n\nycorner <- 1.5^seq_along(ycorner)\nimage(xcorner, ycorner, m)\nabline(h = ycorner, v = xcorner)"
  },
  {
    "objectID": "posts/2014-04-17-r-matrices-and-image/index.html#under-the-hood",
    "href": "posts/2014-04-17-r-matrices-and-image/index.html#under-the-hood",
    "title": "R matrices and image",
    "section": "Under the hood",
    "text": "Under the hood\n\nprint(image.default)\n\nfunction (x = seq(0, 1, length.out = nrow(z)), y = seq(0, 1, \n    length.out = ncol(z)), z, zlim = range(z[is.finite(z)]), \n    xlim = range(x), ylim = range(y), col = hcl.colors(12, \"YlOrRd\", \n        rev = TRUE), add = FALSE, xaxs = \"i\", yaxs = \"i\", xlab, \n    ylab, breaks, oldstyle = FALSE, useRaster, ...) \n{\n    if (missing(z)) {\n        if (!missing(x)) {\n            if (is.list(x)) {\n                z <- x$z\n                y <- x$y\n                x <- x$x\n            }\n            else {\n                if (is.null(dim(x))) \n                  stop(\"argument must be matrix-like\")\n                z <- x\n                x <- seq.int(0, 1, length.out = nrow(z))\n            }\n            if (missing(xlab)) \n                xlab <- \"\"\n            if (missing(ylab)) \n                ylab <- \"\"\n        }\n        else stop(\"no 'z' matrix specified\")\n    }\n    else if (is.list(x)) {\n        xn <- deparse1(substitute(x))\n        if (missing(xlab)) \n            xlab <- paste0(xn, \"$x\")\n        if (missing(ylab)) \n            ylab <- paste0(xn, \"$y\")\n        y <- x$y\n        x <- x$x\n    }\n    else {\n        if (missing(xlab)) \n            xlab <- if (missing(x)) \n                \"\"\n            else deparse1(substitute(x))\n        if (missing(ylab)) \n            ylab <- if (missing(y)) \n                \"\"\n            else deparse1(substitute(y))\n    }\n    if (any(!is.finite(x)) || any(!is.finite(y))) \n        stop(\"'x' and 'y' values must be finite and non-missing\")\n    if (any(diff(x) <= 0) || any(diff(y) <= 0)) \n        stop(\"increasing 'x' and 'y' values expected\")\n    if (!is.matrix(z)) \n        stop(\"'z' must be a matrix\")\n    if (!typeof(z) %in% c(\"logical\", \"integer\", \"double\")) \n        stop(\"'z' must be numeric or logical\")\n    if (length(x) > 1 && length(x) == nrow(z)) {\n        dx <- 0.5 * diff(x)\n        x <- c(x[1L] - dx[1L], x[-length(x)] + dx, x[length(x)] + \n            dx[length(x) - 1])\n    }\n    if (length(y) > 1 && length(y) == ncol(z)) {\n        dy <- 0.5 * diff(y)\n        y <- c(y[1L] - dy[1L], y[-length(y)] + dy, y[length(y)] + \n            dy[length(y) - 1L])\n    }\n    if (missing(breaks)) {\n        nc <- length(col)\n        if (!missing(zlim) && (any(!is.finite(zlim)) || diff(zlim) < \n            0)) \n            stop(\"invalid z limits\")\n        if (diff(zlim) == 0) \n            zlim <- if (zlim[1L] == 0) \n                c(-1, 1)\n            else zlim[1L] + c(-0.4, 0.4) * abs(zlim[1L])\n        z <- (z - zlim[1L])/diff(zlim)\n        zi <- if (oldstyle) \n            floor((nc - 1) * z + 0.5)\n        else floor((nc - 1e-05) * z + 1e-07)\n        zi[zi < 0 | zi >= nc] <- NA\n    }\n    else {\n        if (length(breaks) != length(col) + 1) \n            stop(\"must have one more break than colour\")\n        if (any(!is.finite(breaks))) \n            stop(\"'breaks' must all be finite\")\n        if (is.unsorted(breaks)) {\n            warning(\"unsorted 'breaks' will be sorted before use\")\n            breaks <- sort(breaks)\n        }\n        zi <- .bincode(z, breaks, TRUE, TRUE) - 1L\n    }\n    if (!add) \n        plot(xlim, ylim, xlim = xlim, ylim = ylim, type = \"n\", \n            xaxs = xaxs, yaxs = yaxs, xlab = xlab, ylab = ylab, \n            ...)\n    if (length(x) <= 1) \n        x <- par(\"usr\")[1L:2]\n    if (length(y) <= 1) \n        y <- par(\"usr\")[3:4]\n    if (length(x) != nrow(z) + 1 || length(y) != ncol(z) + 1) \n        stop(\"dimensions of z are not length(x)(-1) times length(y)(-1)\")\n    check_irregular <- function(x, y) {\n        dx <- diff(x)\n        dy <- diff(y)\n        (length(dx) && !isTRUE(all.equal(dx, rep(dx[1], length(dx))))) || \n            (length(dy) && !isTRUE(all.equal(dy, rep(dy[1], length(dy)))))\n    }\n    if (missing(useRaster)) {\n        useRaster <- getOption(\"preferRaster\", FALSE)\n        if (useRaster && check_irregular(x, y)) \n            useRaster <- FALSE\n        if (useRaster) {\n            useRaster <- FALSE\n            ras <- dev.capabilities(\"rasterImage\")$rasterImage\n            if (identical(ras, \"yes\")) \n                useRaster <- TRUE\n            if (identical(ras, \"non-missing\")) \n                useRaster <- all(!is.na(zi))\n        }\n    }\n    if (useRaster) {\n        if (check_irregular(x, y)) \n            stop(gettextf(\"%s can only be used with a regular grid\", \n                sQuote(\"useRaster = TRUE\")), domain = NA)\n        if (!is.character(col)) {\n            col <- as.integer(col)\n            if (any(!is.na(col) & col < 0L)) \n                stop(\"integer colors must be non-negative\")\n            col[col < 1L] <- NA_integer_\n            p <- palette()\n            col <- p[((col - 1L)%%length(p)) + 1L]\n        }\n        zc <- col[zi + 1L]\n        dim(zc) <- dim(z)\n        zc <- t(zc)[ncol(zc):1L, , drop = FALSE]\n        rasterImage(as.raster(zc), min(x), min(y), max(x), max(y), \n            interpolate = FALSE)\n    }\n    else .External.graphics(C_image, x, y, zi, col)\n    invisible()\n}\n<bytecode: 0x56211bec3418>\n<environment: namespace:graphics>\n\n\nThis is like looping with rect()\n\nop <- par(mfrow = c(1, 2))\n## life is hard\ncols <- topo.colors(25)\nscale <- round((m - min(m))/diff(range(m)) * (length(cols) - 1) + 1)\nplot(NA, type = \"n\", xlim = range(xcorner), ylim = range(ycorner), asp = 1)\nfor (i in seq_along(xcorner[-1L])) {\n    for (j in seq_along(ycorner[-1L])) {\n        rect(xleft = xcorner[i], ybottom = ycorner[j], xright = xcorner[i + \n            1L], ytop = ycorner[j + 1L], col = cols[scale[i, j]], angle = 45 * \n            (i + j)%%2, density = 20, lwd = 2)\n    }\n    \n}\n\n## life is good\nimage(list(x = xcorner, y = ycorner, z = m), col = topo.colors(25), asp = 1)\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/2014-04-17-r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "href": "posts/2014-04-17-r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "title": "R matrices and image",
    "section": "“Raster graphics” (not the raster package)",
    "text": "“Raster graphics” (not the raster package)\nRelatively recently native image-graphics support was added to R.\nOld style\n\nm <- matrix(1:12, nrow = 3)\nxcorner <- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner <- seq.int(0, 1, length.out = ncol(m) + 1L)\nimage(xcorner, ycorner, m, col = topo.colors(25))"
  },
  {
    "objectID": "posts/2017-09-01-gdal-in-r/index.html",
    "href": "posts/2017-09-01-gdal-in-r/index.html",
    "title": "GDAL in R",
    "section": "",
    "text": "For some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\nI already knew how immensely powerful R was, with its epic flexibity and useability and I could just sense there was a brighter way once I understood many more details. As my experience grew I was able to do amazing tasks like, merge a few shapefiles together into one, or plot a window of data from a georeferenced grid. Previously the best I’d done in this space was VBScript in Manifold GIS, which I could use to automate some data tasks - but the prospects of full automation from raw data files to output, end-to-end with a software tool that anyone could use was absolutely mind-blowing. I was super-powered, I remember earning a carton of beer from a colleague of my father, for munging some SHP or TAB files between AGD66 and GDA94 … or something, and I knew I had a bright future ahead.\nSo what’s the abstraction? GDAL does not care what format the data is in, it could be points, lines, areas, a raster DEM, a time series of remote sensing, or an actual image. It just doesn’t mind, there’s an interpretation in its model for what’s in the file (or data base, or web server) and it will deliver that interpretation to you, very efficiently. If you understand that intepretation you can do a whole lot of amazing stuff. When this works well it’s because the tasks are modular, you have a series of basic tools designed to work together, and it’s up to you as a developer or a user to chain together the pieces to solve your particular problem."
  },
  {
    "objectID": "posts/2017-09-01-gdal-in-r/index.html#where-does-this-get-difficult",
    "href": "posts/2017-09-01-gdal-in-r/index.html#where-does-this-get-difficult",
    "title": "GDAL in R",
    "section": "Where does this get difficult?",
    "text": "Where does this get difficult?\nGDAL is a C++ library, and that’s not accessible to most users or developers. The other key user-interface is the set of command line utilities, these are called gdal_translate, gdalinfo, ogr2ogr, ogrinfo, and many others. The command line is also not that accessible to many users, though it’s more so than C++ - this is why command line is a key topic for Software Carpentry. These interfaces give very high fidelity to the native interpretation provided by the GDAL model.\nGDAL is used from many languages, there’s Python, R, Perl, C#, Fortran, and it is bundled into many, many softwares - a very long list. The original author wrote code for some of the most influential geo-spatial software the world has, and some of that is in GDAL, some is locked up forever in propietary forms. He saw this as a problem and very early on engineered the work to be able to be open, in the do anything with me, including privatize me-license called MIT. Have a look in the source code for gdalwarp, you’ll see the company who was the best at raster reprojection in the late 1990s and early 2000s.\nPython is surely the closest other language to the native interpretation, but then it’s not that simple, and this is not that story …\nR has a very particular interpretation of the GDAL interpretation, it’s called rgdal and if you are familiar with the GDAL model and with R you can see a very clear extra layer there. This extra level is there partly because of when it was done, the goals of the authors, the community response to the amazingly powerful facilities it provided, but also and perhaps mainly because it was very hard. R’s rather peculiar API meant that in the early 2000s the authors had to write in another language, a language between the native GDAL C++ and the R user language - this is the R API, it’s full of SEXP and UNPROTECTs and if you search this issue you’ll see clear signals not to bother - now you should just get on with learning Rcpp.\nThese extra levels are there in the R API, the hard stuff down in the rgdal/src/ folder but also in the R code itself. There’s a bunch of rigorous rules applied there, things to help protect us from that lower level, and to save from making serious analytical mistakes. All of this was very hard work and very well-intended, but it’s clear that it takes us away from the magic of the GDAL abstraction, we have a contract with rgdal, the R code has a contract with the R API, and the rgdal/src has a contract with GDAL. All of these things divorce R users and developers from the original schemes that GDAL provides, because at the R level rgdal itself has to provide certain guarantees and contracts with both R and with R users. I think that is too much for one package.\nAdd to this the complex zoo of formats, the other libraries that GDAL requires for full use. The Windows rgdal on CRAN doesn’t include HDF4, or HDF5, or NetCDF, or DODS - there are many missing things in this list, and it’s not clear if it’s because it’s hard, it’s against the license (ECW might be tricky, MrSID most definitely would be), or because no one has asked or maybe no one knows how, or maybe CRAN doesn’t want to. (Would you know how to find out?) All of these things add up to being way too much for one package. It’s kind of impossible, though now there are many more eyes on the problem and progress is being made. Who should decide these things? How would anyone know it’s even an issue?\nI wonder if many of us see rgdal as the definition of the GDAL abstraction. I see pretty clearly the difference, and while the package has been extremely useful for me I’ve long wanted lower level control and access to the GDAL core itself. (I had rather influential guidance from extremely expert programmers I’ve worked with, and I’ve discussed GDAL with many others, including employees of various companies, and across various projects and with many users. I assume most R users don’t know much about the details, and why would they want to?).\nThere is active work to modernize rgdal, and you should be aware of the immensely successful sf and the soon to be stars project. sf is an R interpretation of the Simple Features Standard (GDAL has an interpretation of that standard too, sf starts there when it reads with GDAL). stars will start with GDAL as a model for gridded data, and it’s not yet fleshed out what the details of that will be. None of these interpretations are permanent, though while the simple features standard is unlikely to change, there is no doubt that GDAL will evolve and include more features that don’t involve that standard. These things do change and very few people, relatively, are engaged in the decisions that are made. (GDAL could definitely benefit from more input, also something I’ve long wanted to do more of).\nIt’s been a long time, but I’ve recently found a way over the key obstacle I had - building a package to compile for use in R with bindings to GDAL itself. I have a lot to thank Roger Bivand and Edzer Pebesma for many years of instruction and guidance in that, in many different ways. I also am extremely grateful to R-Core for the overall environment, and the tireless work done by the CRAN maintainers. I have to mention Jeroen Ooms and Mark Padgham who’ve been extremely helpful very recently. This is something I’ve wanted to be able to do for a really long time, I hope this post helps provide context to why, and I hope it encourages some more interest in the general topic."
  },
  {
    "objectID": "posts/2017-09-01-gdal-in-r/index.html#vapour",
    "href": "posts/2017-09-01-gdal-in-r/index.html#vapour",
    "title": "GDAL in R",
    "section": "vapour",
    "text": "vapour\nMy response to the interpretation layers is vapour, this is my version of a very minimal interpretation of the core GDAL facilities. There’s a function to read the attribute data from geometric features, a function to read the geometry data as raw binary, or various text formats, a function to read only the bounding box of each feature, and there’s a function to read the raw pixel values from a local window within a gridded data set.\nNone of this is new, we have R packages that do these things and the vapour functions will have bugs, and will need maintenance, and maybe no one but me will ever use them. I’ve needed them and I’ve started to learn a whole lot more about what I’m going to do next with them. I recommend that any R user with an interest in geo-spatial or GDAL facilities have a closer look at how they work - and if you know there’s a lower level below the interpretations provide in R you should explore them. Rcpp and the modern tools for R really do make this immensely more easy than in the past (RStudio has intellisense for C++ …).\nI also believe strongly that R is well-placed to write the future of multi-dimensional and hierarchical and complex structured and geo-spatial data. Do you know what that future should look like?"
  },
  {
    "objectID": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html",
    "href": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html",
    "title": "Web services for scientific data in R",
    "section": "",
    "text": "NOTE: this post has been resurrected from a 2017 post (April 2022).\nThis post is in-progress …\nTwo very important packages in R are rerddap and plotdap providing straightforward access to and visualization of time-varying gridded data sets. Traditionally this is handled by individuals who either have or quickly gain expert knowledge about the minute details regarding obtaining data sources, exploring and extracting data from them, manipulating the data for the required purpose and reporting results. Often this task is not the primary goal, it’s simply a requirement for comparison to environmental data or validation of direct measurements or modelled scenarios against independent data.\nThis is a complex area, it touches on big data, web services, complex and sophisticated multi-dimensional scientific data in array formats (primarily NetCDF), map projections, and data aesthetics or scaling methods by which data values are converted into visualizations.\nR is not known to be strong in this area for handling large and complex array-based data, although it has had good support for every piece in the chain many of them were either not designed to work together or or languished without modernization for some time. There are many many approaches to bring it all together but unfortunately no concerted effort to sort it all out. What there is however is a very exciting and productive wave of experimentation and new packages to try, there’s a lot of exploration occurring and a lot of powerful new approaches.\nROpenSci is producing a variety of valuable new R packages for scientific exploration and analysis. It and the RConsortium are both contributing directly into this ecosystem, with the latter helping to foster developments in simple features (polygons, lines and points), interactive map editing and an integration of large raster data handling."
  },
  {
    "objectID": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#cool-right",
    "href": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#cool-right",
    "title": "Web services for scientific data in R",
    "section": "Cool right!?",
    "text": "Cool right!?\nThis is extremely cool, there is a lot of exciting new support for these sometimes challenging sources of data. However, there is unfortunately no concerted vision for integration of multi-dimensional data into the tidyverse and many of the projects created for data getting and data extraction must include their own internal handlers for downloading and caching data sources and converting data into required forms. This is a complex area, but in some places it is harder and more complex than it really needs to be.\nTo put some guides in this discussion, the rest of this post is informed by the following themes.\n\nthe tidyverse is not just cool, it’s totally awesome (also provides a long-term foundation for the future)\n“good software design” facilitates powerful APIs and strong useability: composable, orthogonal components and effortless exploration and workflow development\nnew abstractions are still to be found\n\nThe tidyverse is loudly loved and hated. Critics complain that they already understood long-form data frames and that constant effusive praise on twitter is really annoying. Supporters just sit agog at a constant stream of pointless shiny fashion parading before their eyes … I mean, they fall into a pit of success and never climb out. Developers who choose the tidyverse as framework for their work appreciate its seamless integration of database principles and actual databases, the consistent and systematic syntax of composable single-purpose functions with predictable return types, the modularization and abstractability of magrittr piping, and the exciting and disruptive impact of tidy evaluation seen clearly already in dplyr and family, but which will clearly make it very easy to traverse the boundary between being a user and being a developer. What could be a better environment for the future of science and research?"
  },
  {
    "objectID": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "href": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "title": "Web services for scientific data in R",
    "section": "What about the rasters!",
    "text": "What about the rasters!\nI’ve been using gridded data in R since 2002. I remember clearly learning about the use of the graphics::image function for visualizing 2D kernel density maps created using sm. I have a life-long shudder reflex at the heat.colors palette, and at KDE maps generally. I also remember my first encounter with the NetCDF format which would have looked exactly like this (after waiting half and hour to download this file).\n\nprint(sst.file)\n\n[1] \"ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc\"\n\nlibrary(RNetCDF)\ncon <- open.nc(file.path(dp, sst.file))\nlon <- var.get.nc(con, \"lon\")\nlat <- var.get.nc(con, \"lat\")\nxlim <- c(140, 155)\nylim <- c(-50, -35)\nxsub <- lon >= xlim[1] & lon <= xlim[2]\nysub <- lat >= ylim[1] & lat <= ylim[2]\ntlim  <- \"oh just give up, it's too painful ...\"\ntime <- var.get.nc(con, \"time\")\n## you get the idea, who can be bothered indexing time as well these days\nv <- var.get.nc(con, \"sst\", start = c(which(xsub)[1], length(ysub) - max(which(ysub)), length(time)), count = c(sum(xsub), sum(ysub), 1))\nimage(lon[xsub], rev(lat[ysub]), v[nrow(v):1, ], asp = 1/cos(42 * pi/180))\n\n\n\n\nWhat a hassle! Let’s just use raster. (These aren’t the same but I really don’t care about making sure the old way works, the new way is much better - when it works, which is mostly …).\n\nlibrary(raster)\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nb <- brick(file.path(dp, sst.file))\n\nLoading required namespace: ncdf4\n\nnlayers(b)\n\n[1] 1685\n\nraster(b)\n\nclass      : RasterLayer \ndimensions : 180, 360, 64800  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n\n## what time did you want?\n\nplot(crop(subset(b, nlayers(b) - c(1, 0)), extent(xlim, ylim), snap = \"out\"), col = heat.colors(12))\n\n\n\n\nThese are pretty cruddy data anyway, 1 degree resolution, weekly time steps? Come on man!\nWhy is this data set relevant? For a very long time the Optimally Interpolated Sea Surface Temperature data set, known fondly as Reynolds SST in some circles, was a very important touchstone for those working in marine animal tracking. From the earliest days of tuna tracking by (PDF): Northwest Pacific by light-level geo-locators, a regional or global data set of surface ocean temperatures was a critical comparison for tag-measured water temperatures. The strong and primarily zonal-gradients (i.e. varying by latitude, it gets cold as you move towards the poles) in the oceans provided an informative corrective to “light level geo-location” latitude estimates, especially when plagued by total zonal ambiguity (see Figure 12.3) around the equinoxes.\nToday we can use much finer resoution blended products for the entire globe. Blended means it’s a combination of measured (remote-sensing, bucket off a ship) and modelled observations, that’s been interpolated to “fill gaps”. This is not a simple topic of course, remotely sensed temperatures must consider whether it is day or night, how windy it is, the presence of sea ice, and many other factors - but as a global science community we have developed to the point of delivering a single agreed data set for this property. And now that it’s 2017, you have the chance of downloading all 5000 or so daily files, the total is only 2000 Gb.\nSo nothing’s free right? You want high-resolution, you get a big download bill."
  },
  {
    "objectID": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "href": "posts/2017-07-25-erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "title": "Web services for scientific data in R",
    "section": "Web services for scientific array data",
    "text": "Web services for scientific array data\nAh, no - we don’t have to download every large data set. That’s where ERDDAP comes in!\nThis makes it easy, but I’m still not happy. In this code a raw NetCDF file is downloaded but is not readily useable by other processes, it’s not obvious how to connect directly to the source with NetCDF API, the raster data itself is turned into both a data frame, and turned into a grid ignoring irregularity in the coordinates, the raster is then resized and possibly reprojected, then turned into a polygon layer (eek) and finally delivered to the user as a very simple high level function that accepts standard grammar expressions of the tidyverse.\nWhat follows is some raw but real examples of using an in-development package tidync in the hypertidy family. It’s very much work-in-progress, as is this blog post …\nPlease reach out to discuss any of this if you are interested!\n\n#install.packages(\"rerddap\")\n#devtools::install_github(\"ropensci/plotdap\")\nlibrary(rerddap)\nlibrary(plotdap)\nlibrary(ggplot2)\nsstInfo <- info('jplMURSST41')\n#system.time({  ## 26 seconds\nmurSST <- griddap(sstInfo, latitude = c(22., 51.), longitude = c(-140., -105),\n                  time = c('last','last'), fields = 'analysed_sst')\n\ninfo() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap\n\n#})\nf <- attr(murSST, \"path\")\n#unlink(f)\n\n## the murSST (it's a GHRSST L4  foundational SST product ) is an extremely detailed raster source, it's really the only\n## daily blended (remote sensing + model) and interpolated (no-missing values)\n## Sea Surface Temperature for global general usage that is high resolution.\n## The other daily blended product Optimally Interpolated (OISST) is only 0.25 degree resolution\n## The GHRSST product is available since 2002, whereas OISST is available since\n## 1981 (the start of the AVHRR sensor era)\nmaxpixels <- 50000\ndres <- c(mean(diff(sort(unique(murSST$data$lon)))), mean(diff(sort(unique(murSST$data$lat)))))\nlibrary(raster)\nr <- raster(extent(range(murSST$data$lon) + c(-1, 1) * dres[1]/2, range(murSST$data$lat) + c(-1, 1) * dres[2]/2),\n     res = dres, crs = \"+init=epsg:4326\")\n\ndim(r) <- dim(r)[1:2] %/% sqrt(ceiling(ncell(r) / maxpixels))\n\ndat <- murSST$data %>%\n mutate(bigcell = cellFromXY(r, cbind(lon, lat))) %>%\n    group_by(time, bigcell) %>%\n  summarize(analysed_sst = mean(analysed_sst, na.rm = FALSE)) %>%\n  ungroup() %>%\n  mutate(lon = xFromCell(r, bigcell), lat = yFromCell(r, bigcell))\n\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n\nr[] <- NA\nr[dat$bigcell] <- dat$analysed_sst\nnames(r) <- \"analysed_sst\"\ndat$bigcell <- NA\n\n#m <- sf::st_as_sf(maps::map(\"world\", region = \"USA\"))\nbgMap <- sf::st_as_sf( maps::map('world', plot = FALSE, fill = TRUE))\n\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_raster(data = dat, aes(x = lon, y = lat, fill = analysed_sst))\n\n\n\n## now, what happened before?\n\n#system.time({p <- sf::st_as_sf(raster::rasterToPolygons(r))})\n## should be a bit faster due to use of implicit coordinate mesh\nsystem.time({p <- sf::st_as_sf(spex::polygonize(r, na.rm = TRUE))})\n\n   user  system elapsed \n  0.766   0.004   0.770 \n\n## plot(p, border = NA)\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_sf(data = p, aes(fill = analysed_sst), colour = \"transparent\")\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\nu <- \"http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41\"\n\nlibrary(tidync)\nlibrary(dplyr)\ntnc <- tidync::tidync(u)\n\nnot a file: \n' http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41 '\n\n... attempting remote connection\n\n\nConnection succeeded.\n\ntnc  ## notice there are four variables in this active space\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.707458e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name    length     min    max start count    dmin   dmax unlim coord_dim \n  <chr> <chr>    <dbl>   <dbl>  <dbl> <int> <int>   <dbl>  <dbl> <lgl> <lgl>     \n1 D0    latitu…  17999 -9.00e1 9.00e1     1 17999 -9.00e1 9.00e1 FALSE TRUE      \n2 D1    longit…  36000 -1.80e2 1.8 e2     1 36000 -1.80e2 1.8 e2 FALSE TRUE      \n3 D2    time      7265  1.02e9 1.65e9     1  7265  1.02e9 1.65e9 FALSE TRUE      \n\nhf <- tnc %>% hyper_filter(longitude = longitude >= -140 & longitude <= -105, latitude = latitude >= 22 & latitude <= 51,\n                       time = index == max(index))\nhf\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.707458e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name   length     min    max start count    dmin    dmax unlim coord_dim \n  <chr> <chr>   <dbl>   <dbl>  <dbl> <int> <int>   <dbl>   <dbl> <lgl> <lgl>     \n1 D0    latit…  17999 -9.00e1 9.00e1 11200  2901  2.2 e1  5.1 e1 FALSE TRUE      \n2 D1    longi…  36000 -1.80e2 1.8 e2  4000  3501 -1.4 e2 -1.05e2 FALSE TRUE      \n3 D2    time     7265  1.02e9 1.65e9  7265     1  1.65e9  1.65e9 FALSE TRUE      \n\n## looking ok, so let's go for gold!\n## specify just sst, otherwise we will get all four\n## hyper_tibble gets the raw arrays with ncvar_get(conn, start = , count = ) calls\n## then expands out the axes based on the values from the filtered axis tables\nsystem.time({\n tab <- hf %>% hyper_tibble(select_var = \"analysed_sst\")\n})\n\n   user  system elapsed \n  3.290   2.338  22.892 \n\n# system.time({  ## 210 seconds\n#   hs <- hyper_slice(hf, select_var = \"analysed_sst\")\n# })\n# hyper_index(hf)\n# nc <- ncdf4::nc_open(u)\n# system.time({  ## 144 seconds\n#   l <- ncdf4::ncvar_get(nc, \"analysed_sst\", start = c(4000, 2901, 5531), count = c(3501, 2901, 1))\n# })"
  },
  {
    "objectID": "posts/2022-04-25-gdalwarper-in-R/index.html",
    "href": "posts/2022-04-25-gdalwarper-in-R/index.html",
    "title": "GDAL warper with R",
    "section": "",
    "text": "There’s several meanings floating around when you say the “GDAL warper”. It can mean\nWe can also mean\nWhat I mean is the GDAL C++ API warping library."
  },
  {
    "objectID": "posts/2022-04-25-gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "href": "posts/2022-04-25-gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "title": "GDAL warper with R",
    "section": "The GDAL C++ API warping library",
    "text": "The GDAL C++ API warping library\nGDAL is complex. It deals with very many different formats, and has many tools. In terms of this post, it has very low-level development facilities, written in C++. This means you can usually go as deep as you need into a geospatial problem by writing C++ (or sometimes Python, and sometimes R) code against the library directly. Sometimes you need to modify GDAL itself, which is how open source is supposed to work.\nKey feature of the gdalwarp_lib.cpp GDALWarp() C++ function, versus the GDALWarpDirect(), GDALWarpIndirect() and the lower level GDALWarpMulti() and GDALWarpImage() functions.\nWhen we use gdalwarp_lib, we get handling of multiple-zoom level sources without intervention on our own.\nMost use cases I see of the warper facilities are for whole-sale conversion of large datasets, multiple input files converted to a another projection in a large output file or series of tiles."
  },
  {
    "objectID": "posts/2022-04-25-gdalwarper-in-R/index.html#the-warper-is-a-generalized-rasterio",
    "href": "posts/2022-04-25-gdalwarper-in-R/index.html#the-warper-is-a-generalized-rasterio",
    "title": "GDAL warper with R",
    "section": "The warper is a generalized RasterIO",
    "text": "The warper is a generalized RasterIO\n(rasterio is a famous python package for using GDAL’s raster facilities, it’s not what we mean here).\nRasterIO is a C++ function in the GDAL library, its job is to take a source data set (i.e. a path to GeoTIFF) and to provide you with a window of pixels from that source. A window can be a subset, the entire raster, or a resampling (i.e. fewer pixels than native) of either the entire or a subset of the raster.\nIt’s really, cool and using it looks like this:\nerr = rasterBand->RasterIO(GF_Read, \n                           Xoffset, Yoffset, \n                           nXSize, nYSize,\n                           &double_scanline[0], \n                           outXSize, outYSize, \n                           GDT_Float64,\n                           0, 0, &psExtraArg);\nThere is a lot going on in that function call, but in short the key parts are (these are my argument-variable-names):\n\nrasterBand->RasterIO() is the way we read a pixel values, we have a raster band and we call the member function RasterIO()\nXoffset, Yoffset is the first column and row we should consider for reading (0,0 if we start at the top left corner)\nnXSize, nYSize\noutXSize, outYSize is the dimension of the window we get out\n\nThat last bits, the two kinds of Size is the magic, we can ask to start at a particular row,column and read out a given number of pixels in x and y. But, not only that we can specifiy where to end in the source. If nXSize and outXSize are not equal in value then we have asked for a resampling of the source “only read every nXSize / outXSize values in the x direction. If they are equal, just read every one. Note also that we might ask for an outX/YSize that is larger than the source nXSize/nYSize - and this would give us a resampling to higher resolution. This is really where the resampling algorithm comes in. ‘Nearest neighbour’ would give us copies of pixels, ‘Bilinear’ and interpolation between source pixels for new pixels in between. Other algorithms include ‘Cubic’, ‘Lanczos’, ‘Average’, ‘Sum’.\nThis is the key behind the fast and lazy reads provided by the terra and sf packages in R, this was originally available also in rgdal and raster made heavy use of it.\nBut, what is the offset and the size? We really want to think in geographic coordinates, and this is exactly what crop() does for example. We get a discretized crop, not an exact one because we only get to read by this raster-based mechanism - we are bound to the size and alignment of the source pixels.\nUnder the hood, functions like crop() do the following:\noffsets/scale vs. xlim,ylim\nThe other arguments in RasterIO.\n\nGF_Read controls the mode we are into (read or write or update)\nGDT_Float64 controls the type of data we get out (64 bit doubles here, GDAL will auto-convert if the source is different type)\n0, 0, &psExtraArg are further details we won’t discuss (though, the way resampling is done is controlled in the extra args options)."
  },
  {
    "objectID": "posts/2022-04-25-gdalwarper-in-R/index.html#enter-the-wutang",
    "href": "posts/2022-04-25-gdalwarper-in-R/index.html#enter-the-wutang",
    "title": "GDAL warper with R",
    "section": "Enter the WuTang",
    "text": "Enter the WuTang"
  },
  {
    "objectID": "posts/2022-04-25-gdalwarper-in-R/index.html#enter-the-warper",
    "href": "posts/2022-04-25-gdalwarper-in-R/index.html#enter-the-warper",
    "title": "GDAL warper with R",
    "section": "Enter the Warper!",
    "text": "Enter the Warper!\nDoing discretized extent raster math is boring. With the warper we don’t need to do it.\nThis is because the warper is for changing projections, this is an entire re-modelling of raster data. (in the 2000s it was possible to do but still quite slow and problematic, hence you have very slowly changing standards and perceptions of what is possible/normal/appropriate in software comunities etc.).\nThis is a global longlat data set, but we want a local projection so we adopt one by finding its specification.\n\n\nIf we were using RasterIO we’d have to do this kind of math to run the index function.\nBut with the warper, we can use an extent (but we still need to align it, or we won’t get what the RasterIO facility would faithfully give)."
  }
]