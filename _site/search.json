[
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html",
    "href": "posts/2017-01-10_r-spatial-2017/index.html",
    "title": "R spatial in 2017",
    "section": "",
    "text": "This document is a broad overview of what I see as most relevant to future spatial, for 2017 and beyond. I’ve tried to be as broad as possible, without going into too much detail, but it’s also quite personal and opinionated."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "title": "R spatial in 2017",
    "section": "The state of things",
    "text": "The state of things\nAn enormous amount of activity has been going on in R spatial. The keystone activity is the new simple features for R package sf, and the many responses to “supporting sf” in various packages but there are many other non-obvious linkages.\nPersonally, I have learnt quite a lot recently about the broader context within R and I’m keen to help consolidate some of the “non-central” tools that we use. There are also some surprisingly helpful implications of the new simple features support both for within the central package, and for the ecosystem around it.\n\nsimple features for R via the sf package\nupcoming replacement for raster\npoint clouds\nmapview and leaflet\nsimple features in other packages\n“exotic types” in sf\nspatial data that simple features cannot support\nhtmlwidgets and plotly"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "title": "R spatial in 2017",
    "section": "sf",
    "text": "sf\nThe simple features package sf was one of the first supported projects of the RConsortium and has been created by Edzer Pebesma. This package replaces sp vector data completely, includes a replacement for rgdal and rgeos and there is a long list of important improvements. These are described in full in the package vignettes and blog posts.\nThe key changes relevant here are\n\nno S4 classes, everything is S3 and so is more immediately manipulable/accessible\nmuch better methods support overall, for printing/summary etc.\nthe key objects are list-vectors, and data frames\nsupport for simple features standard, no more ambiguity for multi-polygons/holes, mixed types, NULL geometry, XYZ-M support\nnative support for dplyr verbs, tibbles (some is WIP but group_by/summarize reprojection and geometric union are good examples that already work)\nreprojection and geometry manipulation and file format support is now GDAL 2.0 and GEOS(?), modernized, more reliable, easier\n\nI strongly recommend getting familiar with the sf data structures, it’s really important to understand the hierarchy levels and the ways the vectors (POINT) and matrices (everything else) are stored. POINT is a vector, MULTIPOINT is a matrix, POLYGON is a list of matrices (one island, zero or more holes), LINESTRING is a matrix (same as m-point), MULTIPOLYGON is a list of lists of matrices (a list of POLYGONs, effectively), and MULTILINESTRING is a list of matrices (a list of LINESTRINGs, effectively and structurally the same as a POLYGON).\nIf you want to see how to convert between sf forms and extract the raw coordinates I would look at st_cast in, and the leaflet approach here:\nhttps://github.com/rstudio/leaflet/blob/master/R/normalize-sf.R"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "title": "R spatial in 2017",
    "section": "rasters",
    "text": "rasters\nThe raster package is apparently being significantly upgraded and Edzer has plans for this as well.\nhttp://r-spatial.org/r/2016/09/26/future.html\nThere are some very interesting extensions to raster on CRAN recently, notably velox, fasteraster, and the unrelated but very nice dggridR.\nHDF5 is now fully supported by rhdf5 on Bioconductor, this could also replace many of the NetCDF4 formats supported by ncdf4, and notably can be used to read NetCDF4 files with compound types."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "title": "R spatial in 2017",
    "section": "Point clouds",
    "text": "Point clouds\nA recent contribution on CRAN is rlas and lidR for the LiDaR LAS format, and algorithms for working with point clouds. It is trivial to push these data into sf types, but it won’t always make sense to do so. You could have a MULTIPOINT with X, Y, Z, and M (but none of the other point attributes) or a point (XYZ) with all the attributes in one sf data frame. This package will be useful for driving interest in the exotic sf types.\nIt’s easy and readily doable right now to read LIDAR data with rlas, and plot it interactively with RGB styling and so in with plotly. Keen to try writing a “detect ground” algorithm? Try it! What does st_triangulate do with a XYZ multipoint? (hmm, it fails noisily - you aren’t supposed to do that)."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "title": "R spatial in 2017",
    "section": "mapview and leaflet",
    "text": "mapview and leaflet\nmapview has support from RConsortium to bring user interaction to spatial in R. Currently building in support for sf, which will be accelerated by the recent dev upgrades in leaflet itself and will eventually support the range of sf types, including full support for MULTIPOLYGON.\nleaflet now has a huge number of new extensions thanks to leaflet.extras, and there is ongoing updates around integrating crosstalk which will be very importatnt for interactive map applications."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "title": "R spatial in 2017",
    "section": "simple features in other packages",
    "text": "simple features in other packages\ntmap, mapview, spbabel, stplanr, …. all have internal versions of sf types converted to something else. It’s an interesting time for these packages that extend the sp and sf structures, and there are opportunities to ensure that best practices are being used."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "title": "R spatial in 2017",
    "section": "Exotic types in sf",
    "text": "Exotic types in sf\nThese are TINs, Polyhedral Surfaces (multipatch - basically polygons with shared “internal” edges), curves and various combinations and varieties of these. None of the triangulations use an indexed mesh, which makes them a bit clunky and probably only for very bespoke uses, but they provide interesting territory to explore. Certainly you can use them to build 3D plots in rgl and plotly (show examples, thanks to @timelyportfolio).\nNote that a GEOMETRYCOLLECTION of triangle POLYGONs is effectively the same as a simple features TIN, it doesn’t really add any structure improvement to the way the thing is put together:\nhttps://github.com/r-gris/sfct\nWe can bend the limits of simple features with “mesh” techniques, and the plotly packge provides easy publishing of interactive 3D visualizations of these.\nplotly is already useable for many applications, we can use techniques from rangl to put sf data into it, and we can use that to easily create exotic triangulated surfaces that are pretty inefficient in the simple features form:\nAlso check out Geotiff.js and timevis.\nHere are some rough examples with plotly.\nhttp://rpubs.com/cyclemumner/rangl-poly-topo-plotly\nhttp://rpubs.com/cyclemumner/raster-quads-to-triangles\nhttp://rpubs.com/cyclemumner/rangl-plotly\nI particularly want texture mapping to go with this kind of plotting, but apparently plotly cannot do that.\nhttps://twitter.com/mathinpython/status/818500905905561600"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "title": "R spatial in 2017",
    "section": "The limits of simple features",
    "text": "The limits of simple features\nSimple features can’t fully represent GPS and other track data, indexed meshes (like rgl mesh3d, segmented paths), or custom hierarchies like networks, nested objects like counties within states, or arc-node topology (like TopoJSON), and it can’t store aesthetics with primitives (like ggplot2/ggvis, rgl, plotly and others can). R can do all of these things, in many different ways and converting from and to sf is not too difficult.\nPlease don’t get the wrong idea though, sf is invaluable for developing more general tools that can work with these structures. Using the types in sf is very refreshing if you’ve been frustrated trying to pick apart a Spatial object in the past.\nIn terms of going beyong simple features itself, GDAL is also going in this direction: http://lists.osgeo.org/pipermail/gdal-dev/2016-December/045675.html\nWe already have most of this capability in R, it’s just scattered all over the place. I’m interested to collate together the best workflows and see where this can go."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "title": "R spatial in 2017",
    "section": "My plans",
    "text": "My plans\nI’m pretty comfortable now with sf and using it for what I want, I have converters to build the forms and workflows I need, and the support in mapview and leaflet and plotly provides more than enough to go with. I will work on making this as accessible and general as possible, and work on integrating it to replace my work on tracking data (the trip and SGAT packages), with 3D models (rbgm, quadmesh) and integrating it with htmlwidgets tools.\nI haven’t yet looked at curves, but I’m keen to see this capability in R both for sf and more generally, we could easily represent the forms made possible by TopoJSON (see “Bostock flawed example”), there is curve support in grid.\nWe could provide smart geo-spatial finite element forms (triangulations, quads),\nI’m keen to see how ggvis and ggplot2 represent geometric types for objects that shared vertices, such as intervals and bar charts, and how we can put in indexed data structures (unlike what ggraph is doing, it builds a mesh from a group of coordinates, you can’t provide it with and index-mesh). I think we can build spatial structures that can store all of these things, so we could throw sf at ggplot2, and use the output as a kind of super-form that knows how to wrap it up into an interactive 4D plot, how to display its primitives etc. etc.\nGIS itself needs what we can already do in R, it’s not a target we are aspring to it’s the other way around."
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "",
    "text": "A new version of {ceramic} is now on CRAN, version 0.8.0.\nThe package exists for two purpose\nNOTE: we need a Mapbox key for the Mapbox servers see, please see ceramic::get_api_key().\nThe original versions of ceramic didn’t really separate these tasks but the new version does."
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-data-from-online",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-data-from-online",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Get raster data from online",
    "text": "Get raster data from online\nWe can read satellite imagery or elevation with a central point and a buffer (in metres).\n\nlibrary(ceramic)\n\nLoading required package: terra\n\n\nterra 1.7.23\n\npt &lt;- cbind(147.3257, -42.8826)\n\n(im &lt;- cc_location(pt, buffer = c(15000, 25000)))\n\nclass       : SpatRaster \ndimensions  : 960, 576, 3  (nrow, ncol, nlyr)\nresolution  : 52.08333, 52.08333  (x, y)\nextent      : 16385222, 16415222, -5319119, -5269119  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     0,    0 \nmax values  : 255,   255,  252 \n\n(el &lt;- cc_elevation(pt, buffer = c(15000, 25000)))\n\nclass       : SpatRaster \ndimensions  : 960, 576, 1  (nrow, ncol, nlyr)\nresolution  : 52.08333, 52.08333  (x, y)\nextent      : 16385222, 16415222, -5319119, -5269119  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\nname        :  lyr.1 \nmin value   :   -1.9 \nmax value   : 1264.9 \n\nop &lt;- par(mfcol = c(1, 2))\nplot(el, legend = F); plot(im, add = TRUE); plot(el, legend = F, \n                                                 col = hcl.colors(64)[tail(seq_len(64), 45)])\n\n\n\n\n\n\n\npar(op)\n\nThese raster objects are in terra ‘SpatRaster’ format (older ceramic used raster package).\nThese use the Mapbox ‘mapbox.satellite’ and ‘mapbox-terrain-rgb’ tile servers."
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-tiles-from-online",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-tiles-from-online",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Get raster tiles from online",
    "text": "Get raster tiles from online\nIf we want the actual tiles, we can use the original get_tiles() function. (In older versions cc_location() and cc_elevation() would invoke get_tiles, but no longer).\n\npt &lt;- cbind(147.3257, -42.8826)\n\nimtiles &lt;- get_tiles(pt, buffer = c(15000, 25000))\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nstr(imtiles)\n\nList of 3\n $ files : chr [1:24] \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3722/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3723/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3724/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3725/2586.jpg\" ...\n $ tiles :List of 2\n  ..$ tiles:'data.frame':   24 obs. of  2 variables:\n  .. ..$ x: int [1:24] 3722 3723 3724 3725 3722 3723 3724 3725 3722 3723 ...\n  .. ..$ y: int [1:24] 2586 2586 2586 2586 2587 2587 2587 2587 2588 2588 ...\n  .. ..- attr(*, \"out.attrs\")=List of 2\n  .. .. ..$ dim     : Named int [1:2] 4 6\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"x\" \"y\"\n  .. .. ..$ dimnames:List of 2\n  .. .. .. ..$ x: chr [1:4] \"x=3722\" \"x=3723\" \"x=3724\" \"x=3725\"\n  .. .. .. ..$ y: chr [1:6] \"y=2586\" \"y=2587\" \"y=2588\" \"y=2589\" ...\n  ..$ zoom : int 12\n  ..- attr(*, \"class\")= chr \"tile_grid\"\n $ extent: num [1:4] 16385222 16415222 -5319119 -5269119\n\neltiles &lt;- get_tiles(pt, buffer = c(15000, 25000))\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nhead(gsub(ceramic::ceramic_cache(), \"\",  eltiles$files))\n\n[1] \"/api.mapbox.com/v4/mapbox.satellite/12/3722/2586.jpg\"\n[2] \"/api.mapbox.com/v4/mapbox.satellite/12/3723/2586.jpg\"\n[3] \"/api.mapbox.com/v4/mapbox.satellite/12/3724/2586.jpg\"\n[4] \"/api.mapbox.com/v4/mapbox.satellite/12/3725/2586.jpg\"\n[5] \"/api.mapbox.com/v4/mapbox.satellite/12/3722/2587.jpg\"\n[6] \"/api.mapbox.com/v4/mapbox.satellite/12/3723/2587.jpg\"\n\n\nAnd to see what tiles we have we can materialize their footprint in wk::rct() form, this is way more efficient than having to use polygons.\n\nzoomtiles &lt;- ceramic::ceramic_tiles(imtiles$tiles$zoom)\n## sub out the ones we just triggered\nzoomtiles &lt;- dplyr::filter(zoomtiles, fullname %in% imtiles$files)\nrc &lt;- ceramic::tiles_to_polygon(zoomtiles)\nplot(rc)\nplot(ext(im), add = TRUE)  ## see the image from above is not the tiles, but the buffer around a point\n\n\n\n\n\n\n\nplot(read_tiles(pt, buffer = c(15000, 25000)))  ## but we can read thos tiles exactly\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nplot(rc, add = TRUE, border = \"white\")"
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#diverse-query-inputs",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#diverse-query-inputs",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Diverse query inputs",
    "text": "Diverse query inputs\nFinally, we can use a point and buffer to get imagery, or we can use a spatial object, currently supported are objects from {geos}, {wk}, {terra}, {sf}, {sp}, {raster}, and {stars}.\nHere’s an example.\n\nwkarea &lt;- wk::rct(xmin = 5e6, ymin = -2e6, xmax = 2e6, ymax = 3e6, crs = \"+proj=laea +lon_0=25 +lat_0=31\")\nim2 &lt;- cc_location(wkarea)\nel2 &lt;- cc_elevation(wkarea)\nop &lt;- par(mfcol = c(1, 2))\nplot(el2, legend = F); plot(im2, add = TRUE); plot(el2, col = grey.colors(128), legend = FALSE)\n\n\n\n\n\n\n\npar(op)\n\n(Please note that the result is still in Mercator, the query can be in any projection but we’re not matching that here, only its extent - ceramic version 0.8.0 is merely a stepping stone to some of the things we can do better with GDAL).\nPrevious versions of cc_location() and cc_elevation() were stuck with only ‘zoom’ and ‘max_tiles’ arguments, these make sense when you restrict exactly to the available tiles but when you just want a given area and a resolution, ‘dimension’ is more appropriate.\nBy default, the dimension is chosen relative to the graphics device. But, we can also specify exactly what we want. (Use zero for one of the dimensions to let the system figure out an appropriate size for the query you have).\n\ncc_location(wkarea)\n\nclass       : SpatRaster \ndimensions  : 960, 776, 3  (nrow, ncol, nlyr)\nresolution  : 7094.906, 7094.906  (x, y)\nextent      : 4203326, 9708973, 105375.6, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     0,    0 \nmax values  : 255,   255,  255 \n\ncc_location(wkarea, dimension = c(400, 700))\n\nclass       : SpatRaster \ndimensions  : 700, 400, 3  (nrow, ncol, nlyr)\nresolution  : 13766.3, 9730.157  (x, y)\nextent      : 4203326, 9709847, 105375.6, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     2,    0 \nmax values  : 255,   255,  255 \n\ncc_location(wkarea, dimension = c(1024, 0))\n\nclass       : SpatRaster \ndimensions  : 1267, 1024, 3  (nrow, ncol, nlyr)\nresolution  : 5377.463, 5377.463  (x, y)\nextent      : 4203326, 9709847, 103240.4, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     1,    0 \nmax values  : 255,   255,  255"
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#some-other-improvements",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#some-other-improvements",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Some other improvements",
    "text": "Some other improvements\n\nit’s faster, for loading reads data from image servers directly with the GDAL warper API\nwe have separation of tile downloading and interaction from raster loading via GDAL\nwe don’t materialize tiles as polygons, we have the tile description, or the compact rct representation\nnew functions read_tiles(), unpack_rgb(),\n\nSee the full list of changes here: https://hypertidy.github.io/ceramic/news/index.html.\nceramic is my final CRAN package that had direct dependencies on rgdal, rgeos, or maptools - in some ways it was the most challenging for me and had loomed as a problem for some time. But, I’ve progressed my own tool kit well in the time and I learnt a lot with this update.\nA future version will probably make the separation (are we tiles, or are we loading raster?) more complete, at any rate there are better ways to organize these things now. There are data sources, data readers and writers, data structures, and algorithms. This work is part of a family of tools that aims to make that orthogonality real and accessible."
  },
  {
    "objectID": "posts/2025-09-04_broken-netcdf/index.html",
    "href": "posts/2025-09-04_broken-netcdf/index.html",
    "title": "Coordinates broken in NetCDF",
    "section": "",
    "text": "There’s a NetCDF file at this URL:\n\"https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\nIt’s fairly straightforward:\nnetcdf CS2WFA_25km_201007 {\ndimensions:\n        y = 332 ;\n        x = 316 ;\n        time = 1 ;\nvariables:\n        float time(time) ;\n                time:long_name = \"time\" ;\n                time:units = \"months since 2000-01-01\" ;\n                time:calendar = \"360_day\" ;\n        double lat(y, x) ;\n                lat:long_name = \"latitude\" ;\n                lat:units = \"degrees_north\" ;\n        double lon(y, x) ;\n                lon:long_name = \"longitude\" ;\n                lon:units = \"degrees_east\" ;\n...\n        double sea_ice_concentration(time, y, x) ;\n                sea_ice_concentration:least_significant_digit = 4LL ;\n                sea_ice_concentration:units = \"1\" ;\n                sea_ice_concentration:standard_name = \"sea_ice_area_fraction\" ;\n                sea_ice_concentration:description = \"Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\" ;\n\n// global attributes:\n                :title = \"Antarctic sea ice physical properties obtained from CryoSat-2 using the CS2WFA algorithm\" ;\n                :institution = \"NASA GSFC Cryospheric Sciences Laboratory and University of Maryland-College Park\" ;\n                :history = \"File created on November 15, 2022, 12:09:06\" ;\n\ngroup: sea_ice_thickness {\n  ...\ngroup: density {\n  variables:\n        double ice_density(time, y, x) ;\n                ice_density:least_significant_digit = 4LL ;\n ...\n  } // group density\n}\nThere’s a root group with four variables defined in time,y,x and another two groups with further variables on that same grid.\nEach data array is 316x332x1 and we can tell that every lon,lat pair is stored explicitly. This fits a convention in NetCDF where time is “unlimited” and we’re only looking at a subset of an overall time series, that could well be continually generated day to day still now.\nLet’s investigate the coordinates. We’ll use GDAL via a commonly used wrapper in R, its ‘vsicurl’ protocol to stream from the internet, and declare the driver explicitly (lest we get the less sophisticated HDF5 interpretation).\n\n#dsn &lt;- \"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\ndsn &lt;- \"NETCDF:../CS2WFA_25km_201007.nc\"\nlibrary(terra)\n\nterra 1.8.62\n\nlon &lt;- rast(dsn, \"lon\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nlat &lt;- rast(dsn, \"lat\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nplot(c(lon, lat))\n\n\n\n\n\n\n\n\nWe get a message from terra about “cells not equally spaced”, which means that when reading an array the geospatial context in GDAL didn’t find anything specifying a compact representation of coordinates, but lon and lat are the coordinates as data, so we can safely ignore this message. (There’s another message about ncdf4, but ignore that too it’s terra try(ing) stuff. )\nWe won’t pay attention to “time”, because our file only has one time step. We’re interested in these lon,lat coordinates. What does the data itself look like? Again we choose one of the 3D variables (with degenerate 3rd dimension ‘time’) by name.\n\nice &lt;- rast(dsn, \"sea_ice_concentration\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nplot(ice)\n\n\n\n\n\n\n\n\nThat should look familiar, though notice that we don’t have any spatial information it’s just a matrix in its own index coordinates, 0,316 for x and 0,332 for y.\nLet’s plot the coordinates. Now we can see our data in a much more spatial-ish context.\n\nxyz &lt;- values(c(lon, lat, ice))\nplot(xyz[,1:2], pch = \".\", col = palr::d_pal(xyz[,3]), asp = 2)\nmaps::map(\"world2\", add = TRUE)\n\n\n\n\n\n\n\n\nWe now don’t have a nice gridded dataset, it’s points in longitude latitude. What can we do to plot it in that nice polar aspect with proper spatial referencing?\nLet’s have a look at this dataset from a GDAL perspective. Now we use the API package gdalraster which gives a lot more control over GDAL itself. I’m going to skip over interrogating the syntax for a particular array, like we did with terra and ‘sea_ice_concentration’ above, I construct the subdataset syntax to open.\n\n\"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\n\n[1] \"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\n\nsds &lt;- sprintf(\"%s:sea_ice_concentration\", dsn)\nlibrary(gdalraster)\n\nGDAL 3.12.0dev-4e2b27da8f (released 2025-08-26), GEOS 3.12.1, PROJ 9.3.1\n\n\n\nAttaching package: 'gdalraster'\n\n\nThe following object is masked from 'package:terra':\n\n    rasterize\n\nds &lt;- new(GDALRaster, sds)\nds$info()\n\nDriver: netCDF/Network Common Data Format\nFiles: ../CS2WFA_25km_201007.nc\nSize is 316, 332\nMetadata:\n  NC_GLOBAL#history=File created on November 15, 2022, 12:09:06\n  NC_GLOBAL#institution=NASA GSFC Cryospheric Sciences Laboratory and University of Maryland-College Park\n  NC_GLOBAL#title=Antarctic sea ice physical properties obtained from CryoSat-2 using the CS2WFA algorithm\n  NETCDF_DIM_EXTRA={time}\n  NETCDF_DIM_time_DEF={1,5}\n  NETCDF_DIM_time_VALUES=126\n  sea_ice_concentration#description=Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\n  sea_ice_concentration#least_significant_digit=4\n  sea_ice_concentration#standard_name=sea_ice_area_fraction\n  sea_ice_concentration#units=1\n  time#calendar=360_day\n  time#long_name=time\n  time#units=months since 2000-01-01\nGeolocation:\n  SRS=GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n  X_DATASET=NETCDF:\"../CS2WFA_25km_201007.nc\":lon\n  X_BAND=1\n  Y_DATASET=NETCDF:\"../CS2WFA_25km_201007.nc\":lat\n  Y_BAND=1\n  PIXEL_OFFSET=0\n  PIXEL_STEP=1\n  LINE_OFFSET=0\n  LINE_STEP=1\n  GEOREFERENCING_CONVENTION=PIXEL_CENTER\nCorner Coordinates:\nUpper Left  (    0.0,    0.0)\nLower Left  (    0.0,  332.0)\nUpper Right (  316.0,    0.0)\nLower Right (  316.0,  332.0)\nCenter      (  158.0,  166.0)\nBand 1 Block=316x332 Type=Float64, ColorInterp=Undefined\n  NoData Value=9.969209968386869e+36\n  Unit Type: 1\n  Metadata:\n    NETCDF_VARNAME=sea_ice_concentration\n    least_significant_digit=4\n    units=1\n    standard_name=sea_ice_area_fraction\n    description=Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\n    NETCDF_DIM_time=126\n\n\nThe interesting part of that output is under “Geolocation:”. We can see that while GDAL doesn’t have spatial referencing for this array, it does seem to know that’s it’s possible because of the X_DATASET and Y_DATASET. These can be used by the GDAL warper API (warp means “reprojection” or “reshaping” for an image) in order to resolve to a new spatial gridded dataset … but also note, the same facilities used to “reproject” one regular grid to another are also the same that can be used to generate a regular grid from irregular geolocation coordinates (we have a point for every data point, but those coordinates might be less dense, or might be GCPs or RCPs, and no matter what coordinate system those latent coordinates are using, we can warp to whatever spatial grid we like). So let’s warp.\nWe don’t have to specifying the geolocation arrays, we saw that GDAL already knows this. We’ll write our new gridded dataset conveniently to a temporary virtual “file” so we can avoid more cleanup.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\")\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 127, 902, 1  (nrow, ncol, nlyr)\nresolution  : 0.398722, 0.3987662  (x, y)\nextent      : 0.164645, 359.8119, -89.80441, -39.1611  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb3943dea1d4.tif \nname        : filedb3943dea1d4 \n\n\nWe indeed now have a spatial grid, a raster. But perhaps we don’t like the 0,360 convention. We can set the target extent, GDAL had to figure one out from the input geolocation arrays, and it will usually do a good job but it’s really our responsibility to specify what we want for reproducibility in later workflows.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\", \n                 cl_arg = c(\"-te\", -180, -90, 180, -39, \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 194, 1371, 1  (nrow, ncol, nlyr)\nresolution  : 0.2625821, 0.2628866  (x, y)\nextent      : -180, 180, -90, -39  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb394444fc021.tif \nname        : filedb394444fc021 \n\n\nAnother thing that is also our responsibility is the dimensions of the grid and the resolution, obviously this and extent (or bbox, bounding box) are all interlinked so we augment our bounding box / extent setting with a nice clean resolution.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\", \n                 cl_arg = c(\"-te\", -180, -90, 180, -39, \"-tr\", 0.25, 0.25,\n                            \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\nmaps::map(add = TRUE)\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 204, 1440, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180, 180, -90, -39  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb39443fd3375.tif \nname        : filedb39443fd3375 \n\n\nThat is all well and nice, and we’ve quietly demonstrated some of the key powers of GDAL:\n\nvirtualization, streaming read, read a particular variable\ninvestigate GDAL logic (coordinates are linked to data via latent “geolocation” arrays)\nwarping with heuristics to a CRS\nwarping with a grid specification, and using geolocation arrays to guide the grid-resolving process\nsetting grid specification with any of crs, extent(bbox), resolution, dimension, and allowing GDAL to internally wrap from 0,360 context to -180,180\n\nBut, we still don’t have that nice polar aspect we saw from the raw array above. So we change CRS. ‘EPSG:3412’ is a Polar Stereographic map projection on the south pole, commonly used for standard sea ice products. As a grid this has nice properties, that don’t require edge-wrap for a matrix model, it’s true scale at approximately where the coastline of Antarctica is, and it preserves shape (this means not all cells are exactly the same size, but they’re close enough for where the sea ice is).\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:3412\", \n                cl_arg = c(\"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(polargrid &lt;- rast(tf))\n\n\n\n\n\n\n\npolargrid\n\nclass       : SpatRaster \nsize        : 348, 332, 1  (nrow, ncol, nlyr)\nresolution  : 24999.84, 24999.84  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb39427e3e998.tif \nname        : filedb39427e3e998 \n\n\nTo make sure let’s get some spatial data to plot with this.\n\nv &lt;- vect(\"/vsizip//vsicurl/https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM0.zip\", \n     query = \"SELECT shapeGroup FROM geoBoundariesCGAZ_ADM0 WHERE shapeGroup IN ('ATA')\")\nv &lt;- crop(v, ext(-180, 180, -84, 0)) ## small hack to remove the pole seam\nv &lt;- project(v, \"EPSG:3412\")\nplot(polargrid)\nplot(v, add = TRUE)\n\n\n\n\n\n\n\n\nLooks good! We’ve plotted the original data in the correct polar aspect.\nBut, the data aren’t exactly the same, we’ve added pixels in the heuristic used by GDAL to determine the grid specification from the extent and resolution of the geolocation array longitude and latitudes.\n\nice\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 316, 0, 332  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource      : CS2WFA_25km_201007.nc:sea_ice_concentration \nvarname     : sea_ice_concentration (sea_ice_area_fraction) \nname        : sea_ice_concentration \nunit        :                     1 \n\npolargrid\n\nclass       : SpatRaster \nsize        : 348, 332, 1  (nrow, ncol, nlyr)\nresolution  : 24999.84, 24999.84  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb39427e3e998.tif \nname        : filedb39427e3e998 \n\n\nSo, let’s try to use the same grid.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:3412\", \n                cl_arg = c(\"-ts\", 316, 332,\n                           \n                  \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\npolargrid &lt;- rast(tf)\n\npolargrid\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 26265.66, 26204.65  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb394ad7590c.tif \nname        : filedb394ad7590c \n\n\nWe can’t do that without giving them the same extent, but we still don’t know what that is for the original data. So let’s calculate it.\n\nxy &lt;- project(values(c(lon, lat)), to = \"EPSG:3412\", from = \"EPSG:4326\")\nrange(xy[,2])\n\n[1] -3937527  4337533"
  },
  {
    "objectID": "posts/2025-09-04_broken-netcdf/index.html#a-netcdf-file",
    "href": "posts/2025-09-04_broken-netcdf/index.html#a-netcdf-file",
    "title": "Coordinates broken in NetCDF",
    "section": "",
    "text": "There’s a NetCDF file at this URL:\n\"https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\nIt’s fairly straightforward:\nnetcdf CS2WFA_25km_201007 {\ndimensions:\n        y = 332 ;\n        x = 316 ;\n        time = 1 ;\nvariables:\n        float time(time) ;\n                time:long_name = \"time\" ;\n                time:units = \"months since 2000-01-01\" ;\n                time:calendar = \"360_day\" ;\n        double lat(y, x) ;\n                lat:long_name = \"latitude\" ;\n                lat:units = \"degrees_north\" ;\n        double lon(y, x) ;\n                lon:long_name = \"longitude\" ;\n                lon:units = \"degrees_east\" ;\n...\n        double sea_ice_concentration(time, y, x) ;\n                sea_ice_concentration:least_significant_digit = 4LL ;\n                sea_ice_concentration:units = \"1\" ;\n                sea_ice_concentration:standard_name = \"sea_ice_area_fraction\" ;\n                sea_ice_concentration:description = \"Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\" ;\n\n// global attributes:\n                :title = \"Antarctic sea ice physical properties obtained from CryoSat-2 using the CS2WFA algorithm\" ;\n                :institution = \"NASA GSFC Cryospheric Sciences Laboratory and University of Maryland-College Park\" ;\n                :history = \"File created on November 15, 2022, 12:09:06\" ;\n\ngroup: sea_ice_thickness {\n  ...\ngroup: density {\n  variables:\n        double ice_density(time, y, x) ;\n                ice_density:least_significant_digit = 4LL ;\n ...\n  } // group density\n}\nThere’s a root group with four variables defined in time,y,x and another two groups with further variables on that same grid.\nEach data array is 316x332x1 and we can tell that every lon,lat pair is stored explicitly. This fits a convention in NetCDF where time is “unlimited” and we’re only looking at a subset of an overall time series, that could well be continually generated day to day still now.\nLet’s investigate the coordinates. We’ll use GDAL via a commonly used wrapper in R, its ‘vsicurl’ protocol to stream from the internet, and declare the driver explicitly (lest we get the less sophisticated HDF5 interpretation).\n\n#dsn &lt;- \"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\ndsn &lt;- \"NETCDF:../CS2WFA_25km_201007.nc\"\nlibrary(terra)\n\nterra 1.8.62\n\nlon &lt;- rast(dsn, \"lon\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nlat &lt;- rast(dsn, \"lat\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nplot(c(lon, lat))\n\n\n\n\n\n\n\n\nWe get a message from terra about “cells not equally spaced”, which means that when reading an array the geospatial context in GDAL didn’t find anything specifying a compact representation of coordinates, but lon and lat are the coordinates as data, so we can safely ignore this message. (There’s another message about ncdf4, but ignore that too it’s terra try(ing) stuff. )\nWe won’t pay attention to “time”, because our file only has one time step. We’re interested in these lon,lat coordinates. What does the data itself look like? Again we choose one of the 3D variables (with degenerate 3rd dimension ‘time’) by name.\n\nice &lt;- rast(dsn, \"sea_ice_concentration\")\n\nWarning: [rast] GDAL did not find an extent. installing the ncdf4 package may\nhelp\n\nplot(ice)\n\n\n\n\n\n\n\n\nThat should look familiar, though notice that we don’t have any spatial information it’s just a matrix in its own index coordinates, 0,316 for x and 0,332 for y.\nLet’s plot the coordinates. Now we can see our data in a much more spatial-ish context.\n\nxyz &lt;- values(c(lon, lat, ice))\nplot(xyz[,1:2], pch = \".\", col = palr::d_pal(xyz[,3]), asp = 2)\nmaps::map(\"world2\", add = TRUE)\n\n\n\n\n\n\n\n\nWe now don’t have a nice gridded dataset, it’s points in longitude latitude. What can we do to plot it in that nice polar aspect with proper spatial referencing?\nLet’s have a look at this dataset from a GDAL perspective. Now we use the API package gdalraster which gives a lot more control over GDAL itself. I’m going to skip over interrogating the syntax for a particular array, like we did with terra and ‘sea_ice_concentration’ above, I construct the subdataset syntax to open.\n\n\"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\n\n[1] \"NETCDF:/vsicurl/https://zenodo.org/records/7327711/files/CS2WFA_25km_201007.nc?download=1\"\n\nsds &lt;- sprintf(\"%s:sea_ice_concentration\", dsn)\nlibrary(gdalraster)\n\nGDAL 3.12.0dev-4e2b27da8f (released 2025-08-26), GEOS 3.12.1, PROJ 9.3.1\n\n\n\nAttaching package: 'gdalraster'\n\n\nThe following object is masked from 'package:terra':\n\n    rasterize\n\nds &lt;- new(GDALRaster, sds)\nds$info()\n\nDriver: netCDF/Network Common Data Format\nFiles: ../CS2WFA_25km_201007.nc\nSize is 316, 332\nMetadata:\n  NC_GLOBAL#history=File created on November 15, 2022, 12:09:06\n  NC_GLOBAL#institution=NASA GSFC Cryospheric Sciences Laboratory and University of Maryland-College Park\n  NC_GLOBAL#title=Antarctic sea ice physical properties obtained from CryoSat-2 using the CS2WFA algorithm\n  NETCDF_DIM_EXTRA={time}\n  NETCDF_DIM_time_DEF={1,5}\n  NETCDF_DIM_time_VALUES=126\n  sea_ice_concentration#description=Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\n  sea_ice_concentration#least_significant_digit=4\n  sea_ice_concentration#standard_name=sea_ice_area_fraction\n  sea_ice_concentration#units=1\n  time#calendar=360_day\n  time#long_name=time\n  time#units=months since 2000-01-01\nGeolocation:\n  SRS=GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]\n  X_DATASET=NETCDF:\"../CS2WFA_25km_201007.nc\":lon\n  X_BAND=1\n  Y_DATASET=NETCDF:\"../CS2WFA_25km_201007.nc\":lat\n  Y_BAND=1\n  PIXEL_OFFSET=0\n  PIXEL_STEP=1\n  LINE_OFFSET=0\n  LINE_STEP=1\n  GEOREFERENCING_CONVENTION=PIXEL_CENTER\nCorner Coordinates:\nUpper Left  (    0.0,    0.0)\nLower Left  (    0.0,  332.0)\nUpper Right (  316.0,    0.0)\nLower Right (  316.0,  332.0)\nCenter      (  158.0,  166.0)\nBand 1 Block=316x332 Type=Float64, ColorInterp=Undefined\n  NoData Value=9.969209968386869e+36\n  Unit Type: 1\n  Metadata:\n    NETCDF_VARNAME=sea_ice_concentration\n    least_significant_digit=4\n    units=1\n    standard_name=sea_ice_area_fraction\n    description=Mean sea ice concentration in grid cell, from Bootstrap V3 concentration algorithm (Comiso, 2017 https://doi.org/10.5067/7Q8HCCWS4I0R)\n    NETCDF_DIM_time=126\n\n\nThe interesting part of that output is under “Geolocation:”. We can see that while GDAL doesn’t have spatial referencing for this array, it does seem to know that’s it’s possible because of the X_DATASET and Y_DATASET. These can be used by the GDAL warper API (warp means “reprojection” or “reshaping” for an image) in order to resolve to a new spatial gridded dataset … but also note, the same facilities used to “reproject” one regular grid to another are also the same that can be used to generate a regular grid from irregular geolocation coordinates (we have a point for every data point, but those coordinates might be less dense, or might be GCPs or RCPs, and no matter what coordinate system those latent coordinates are using, we can warp to whatever spatial grid we like). So let’s warp.\nWe don’t have to specifying the geolocation arrays, we saw that GDAL already knows this. We’ll write our new gridded dataset conveniently to a temporary virtual “file” so we can avoid more cleanup.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\")\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 127, 902, 1  (nrow, ncol, nlyr)\nresolution  : 0.398722, 0.3987662  (x, y)\nextent      : 0.164645, 359.8119, -89.80441, -39.1611  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb3943dea1d4.tif \nname        : filedb3943dea1d4 \n\n\nWe indeed now have a spatial grid, a raster. But perhaps we don’t like the 0,360 convention. We can set the target extent, GDAL had to figure one out from the input geolocation arrays, and it will usually do a good job but it’s really our responsibility to specify what we want for reproducibility in later workflows.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\", \n                 cl_arg = c(\"-te\", -180, -90, 180, -39, \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 194, 1371, 1  (nrow, ncol, nlyr)\nresolution  : 0.2625821, 0.2628866  (x, y)\nextent      : -180, 180, -90, -39  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb394444fc021.tif \nname        : filedb394444fc021 \n\n\nAnother thing that is also our responsibility is the dimensions of the grid and the resolution, obviously this and extent (or bbox, bounding box) are all interlinked so we augment our bounding box / extent setting with a nice clean resolution.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:4326\", \n                 cl_arg = c(\"-te\", -180, -90, 180, -39, \"-tr\", 0.25, 0.25,\n                            \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(icegrid &lt;- rast(tf))\nmaps::map(add = TRUE)\n\n\n\n\n\n\n\nicegrid\n\nclass       : SpatRaster \nsize        : 204, 1440, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180, 180, -90, -39  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : filedb39443fd3375.tif \nname        : filedb39443fd3375 \n\n\nThat is all well and nice, and we’ve quietly demonstrated some of the key powers of GDAL:\n\nvirtualization, streaming read, read a particular variable\ninvestigate GDAL logic (coordinates are linked to data via latent “geolocation” arrays)\nwarping with heuristics to a CRS\nwarping with a grid specification, and using geolocation arrays to guide the grid-resolving process\nsetting grid specification with any of crs, extent(bbox), resolution, dimension, and allowing GDAL to internally wrap from 0,360 context to -180,180\n\nBut, we still don’t have that nice polar aspect we saw from the raw array above. So we change CRS. ‘EPSG:3412’ is a Polar Stereographic map projection on the south pole, commonly used for standard sea ice products. As a grid this has nice properties, that don’t require edge-wrap for a matrix model, it’s true scale at approximately where the coastline of Antarctica is, and it preserves shape (this means not all cells are exactly the same size, but they’re close enough for where the sea ice is).\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:3412\", \n                cl_arg = c(\"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot(polargrid &lt;- rast(tf))\n\n\n\n\n\n\n\npolargrid\n\nclass       : SpatRaster \nsize        : 348, 332, 1  (nrow, ncol, nlyr)\nresolution  : 24999.84, 24999.84  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb39427e3e998.tif \nname        : filedb39427e3e998 \n\n\nTo make sure let’s get some spatial data to plot with this.\n\nv &lt;- vect(\"/vsizip//vsicurl/https://github.com/wmgeolab/geoBoundaries/raw/main/releaseData/CGAZ/geoBoundariesCGAZ_ADM0.zip\", \n     query = \"SELECT shapeGroup FROM geoBoundariesCGAZ_ADM0 WHERE shapeGroup IN ('ATA')\")\nv &lt;- crop(v, ext(-180, 180, -84, 0)) ## small hack to remove the pole seam\nv &lt;- project(v, \"EPSG:3412\")\nplot(polargrid)\nplot(v, add = TRUE)\n\n\n\n\n\n\n\n\nLooks good! We’ve plotted the original data in the correct polar aspect.\nBut, the data aren’t exactly the same, we’ve added pixels in the heuristic used by GDAL to determine the grid specification from the extent and resolution of the geolocation array longitude and latitudes.\n\nice\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 316, 0, 332  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource      : CS2WFA_25km_201007.nc:sea_ice_concentration \nvarname     : sea_ice_concentration (sea_ice_area_fraction) \nname        : sea_ice_concentration \nunit        :                     1 \n\npolargrid\n\nclass       : SpatRaster \nsize        : 348, 332, 1  (nrow, ncol, nlyr)\nresolution  : 24999.84, 24999.84  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb39427e3e998.tif \nname        : filedb39427e3e998 \n\n\nSo, let’s try to use the same grid.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:3412\", \n                cl_arg = c(\"-ts\", 316, 332,\n                           \n                  \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\npolargrid &lt;- rast(tf)\n\npolargrid\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 26265.66, 26204.65  (x, y)\nextent      : -3950080, 4349868, -4337584, 4362361  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb394ad7590c.tif \nname        : filedb394ad7590c \n\n\nWe can’t do that without giving them the same extent, but we still don’t know what that is for the original data. So let’s calculate it.\n\nxy &lt;- project(values(c(lon, lat)), to = \"EPSG:3412\", from = \"EPSG:4326\")\nrange(xy[,2])\n\n[1] -3937527  4337533"
  },
  {
    "objectID": "posts/2025-09-04_broken-netcdf/index.html#throw-all-that-away-ignore-everything-above.",
    "href": "posts/2025-09-04_broken-netcdf/index.html#throw-all-that-away-ignore-everything-above.",
    "title": "Coordinates broken in NetCDF",
    "section": "Throw all that away, ignore everything above.",
    "text": "Throw all that away, ignore everything above.\nYou should never ever do the above for a dataset that doesn’t need it. This is a major problem in array computation worlds, there’s a lowest common denominator that will work but why not be smart about it, and keep fidelity with the original scheme for a dataset.\nThe problem here is that we can’t reconstruct the actual grid in the file, we have to treat it as points in geolocation arrays, and infer something about the grid. Here’s the answer:\n\nex &lt;- c(xmin = -3950000,  xmax = 3950000, ymin  = -3950000, ymax = 4350000)\ndiff(ex)[c(1, 3)] / c(316, 332)  ## look familiar?\n\n xmax  ymax \n25000 25000 \n\n\n\nset_ext &lt;- function(x, ex) {terra::ext(x) &lt;- ex; x}\n\nplot(set_ext(ice, ext(ex)))\nplot(v, add = T)\n\n\n\n\n\n\n\nplot(polargrid); plot(v, add = TRUE)\n\n\n\n\n\n\n\n\nNow finally, we get exactly the same answer.\n\ngdalraster::warp(sds,  tf &lt;- tempfile(fileext = \".tif\", tmpdir = \"/vsimem\"), t_srs = \"EPSG:3412\", \n                cl_arg = c(\"-ts\", 316, 332, \"-te\", ex[1], ex[3], ex[2], ex[4],\n                           \n                  \"-to\", \"GEOLOC_NORMALIZE_LONGITUDE_MINUS_180_PLUS_180=YES\"))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\npolargrid &lt;- rast(tf)\n\npolargrid\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 25000, 25000  (x, y)\nextent      : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource      : filedb3946450fee3.tif \nname        : filedb3946450fee3 \n\npolargrid - set_ext(ice, ext(ex))\n\nWarning: [-] CRS do not match\n\n\nclass       : SpatRaster \nsize        : 332, 316, 1  (nrow, ncol, nlyr)\nresolution  : 25000, 25000  (x, y)\nextent      : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\ncoord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \nsource(s)   : memory\nvarname     : filedb3946450fee3 \nname        : filedb3946450fee3 \nmin value   :                 0 \nmax value   :                 0 \n\n# class       : SpatRaster \n# size        : 332, 316, 1  (nrow, ncol, nlyr)\n# resolution  : 25000, 25000  (x, y)\n# extent      : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\n# coord. ref. : NSIDC Sea Ice Polar Stereographic South (EPSG:3412) \n# source(s)   : memory\n# varname     : fileb328ade0cec2 \n# name        : fileb328ade0cec2 \n# min value   :                0 \n# max value   :                0 \n\nBut how did we do that? We plucked some magic numbers out of the air. We can’t obtain those four numbers from the lon lat arrays, we can’t find them in the file, they aren’t in the metadata or the website. We just know, because NSIDC sea ice products are standard on a 3412 grid at 25km resolution.\nThese four numbers: -3950000, 3950000, -3950000, 4350000 are entirely cryptic in the data store itself, and require an expert to notice the problem. We can throw away 332 * 316 * 2 values from every file, replace them with these four values and we have a better and more faithful representation of the original data.\nThis is an entropy problem.\nI refer to this kind of problem as “cryptic curvilinear”. The data present as an irregular grid with curvlinear coordinates, but they are entirely unnecessary, we can replace them with 4 values that anyone can remember and we have the following easy ways to fix, roughly in increasing difficulty and decreasing utility:\n\nset “-a_srs”, and -a_ullr” in gdal raster convert, or via “vrt://{dsn}?a_ullr=,,,” syntax\nOR set “-a_srs”, and “-a_gt” in gdal raster convert, or via “vrt://{dsn}?a_gt=,,,,,” syntax\nset extent with terra in R and set the crs\nopen with rasterio and set geotransform and set spatial ref (same as set extent with R)\nmodify the coordinates in xarray, and set the crs"
  },
  {
    "objectID": "posts/2025-09-04_broken-netcdf/index.html#oh-but-sure-there-are-bad-files-out-there",
    "href": "posts/2025-09-04_broken-netcdf/index.html#oh-but-sure-there-are-bad-files-out-there",
    "title": "Coordinates broken in NetCDF",
    "section": "Oh, but sure there are bad files out there",
    "text": "Oh, but sure there are bad files out there\nThere are so many.\n\nGHRSST noisy, low precision degenerate rectilinear coordinates\nOISST and NSIDC, have degenerate rectilinear coordinates that don’t cause problems"
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html",
    "title": "GDAL warper with R",
    "section": "",
    "text": "There’s several meanings floating around when you say the “GDAL warper”. It can mean\n\ncommand-line gdalwarp\nthe rasterio package in Python, and its WarpedVRT\nthe GDAL C++ API warping library\n\nWe can also mean\n\nthe sf package in R, and its gdal_utils() function\nthe stars package in R, and its st_warp() function\nthe terra package in R, and its project() function\nthe gdalUtils package in R\nthe gdalUtilities package in R\n\nBut, what I mean is the GDAL C++ API warping library."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#an-unfinished-post",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#an-unfinished-post",
    "title": "GDAL warper with R",
    "section": "",
    "text": "There’s several meanings floating around when you say the “GDAL warper”. It can mean\n\ncommand-line gdalwarp\nthe rasterio package in Python, and its WarpedVRT\nthe GDAL C++ API warping library\n\nWe can also mean\n\nthe sf package in R, and its gdal_utils() function\nthe stars package in R, and its st_warp() function\nthe terra package in R, and its project() function\nthe gdalUtils package in R\nthe gdalUtilities package in R\n\nBut, what I mean is the GDAL C++ API warping library."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "title": "GDAL warper with R",
    "section": "The GDAL C++ API warping library",
    "text": "The GDAL C++ API warping library\nGDAL is complex. It deals with very many different formats, and has many tools. In terms of this post, it has very low-level development facilities, written in C++. This means you can usually go as deep as you need into a geospatial problem by writing C++ code against the library directly. Often you don’t need C++ and can use Python, and sometimes you can use R depending on what is exposed there. Sometimes you need to modify GDAL itself, and you can do anything you want then(!), which is how open source is supposed to work.\nKey feature of the gdalwarp_lib.cpp GDALWarp() C++ function, versus the GDALWarpDirect(), GDALWarpIndirect() and the lower level GDALWarpMulti() and GDALWarpImage() functions.\nWhen we use gdalwarp_lib, we get handling of multiple-zoom level sources without intervention on our own.\nMost use cases I see of the warper facilities are for whole-sale conversion of large datasets, multiple input files converted to a another projection in a large output file or series of tiles."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#the-warper-is-also-a-generalized-rasterio",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#the-warper-is-also-a-generalized-rasterio",
    "title": "GDAL warper with R",
    "section": "The warper is also a generalized RasterIO",
    "text": "The warper is also a generalized RasterIO\n(rasterio is a famous python package for using GDAL’s raster facilities, it’s not what we mean here).\nRasterIO is a C++ function in the GDAL library, its job is to take a source data set (i.e. a path to GeoTIFF) and to provide you with a window of pixels from that source. A window can be a subset, the entire raster, or a resampling (i.e. fewer pixels than native) of either the entire or a subset of the raster.\nIt’s really, cool and using it looks like this:\nerr = rasterBand-&gt;RasterIO(GF_Read, \n                           Xoffset, Yoffset, \n                           nXSize, nYSize,\n                           &double_scanline[0], \n                           outXSize, outYSize, \n                           GDT_Float64,\n                           0, 0, &psExtraArg);\nThere is a lot going on in that function call, but in short the key parts are (these are my argument-variable-names):\n\nrasterBand-&gt;RasterIO() is the way we read a pixel values, we have a raster band and we call the member function RasterIO()\nXoffset, Yoffset is the first column and row we should consider for reading (0,0 if we start at the top left corner)\nnXSize, nYSize\noutXSize, outYSize is the dimension of the window we get out\n\nThat last bits, the two kinds of Size is the magic, we can ask to start at a particular row,column and read out a given number of pixels in x and y. But, not only that we can specifiy where to end in the source. If nXSize and outXSize are not equal in value then we have asked for a resampling of the source “only read every nXSize / outXSize values in the x direction. If they are equal, just read every one. Note also that we might ask for an outX/YSize that is larger than the source nXSize/nYSize - and this would give us a resampling to higher resolution. This is really where the resampling algorithm comes in. ‘Nearest neighbour’ would give us copies of pixels, ‘Bilinear’ and interpolation between source pixels for new pixels in between. Other algorithms include ‘Cubic’, ‘Lanczos’, ‘Average’, ‘Sum’.\nThis is the key behind the fast and lazy reads provided by the terra and sf packages in R, this was originally available also in rgdal and raster made heavy use of it.\nBut, what is the offset and the size? We really want to think in geographic coordinates, and this is exactly what crop() does for example. We get a discretized crop, not an exact one because we only get to read by this raster-based mechanism - we are bound to the size and alignment of the source pixels.\nUnder the hood, functions like crop() do the following:\noffsets/scale vs. xlim,ylim\nThe other arguments in RasterIO.\n\nGF_Read controls the mode we are into (read or write or update)\nGDT_Float64 controls the type of data we get out (64 bit doubles here, GDAL will auto-convert if the source is different type)\n0, 0, &psExtraArg are further details we won’t discuss (though, the way resampling is done is controlled in the extra args options).\n\nWhat are its limitations:\n\nonly one source at a time, you can’t resample from multiple rasters at once (we are using offset/size indexing so the rasters must all be the same shape, you can read multiple bands)\n\n\nWhat is interesting about RasterIO vs. Warping is that we would never ever use warping for pure graphics. It doesn’t make sens"
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#enter-the-wutang-warper",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#enter-the-wutang-warper",
    "title": "GDAL warper with R",
    "section": "Enter the WuTang Warper!",
    "text": "Enter the WuTang Warper!\nDoing discretized extent raster math is boring. With the warper we don’t need to do it.\nThis is because the warper is for changing projections, this is an entire re-modelling of raster data. (in the 2000s it was possible to do but still quite slow and problematic, hence you have very slowly changing standards and perceptions of what is possible/normal/appropriate in software comunities etc.).\nThis is a global longlat data set, but we want a local projection so we adopt one by finding its specification.\n\n\nIf we were using RasterIO we’d have to do this kind of math to run the index function.\nBut with the warper, we can use an extent (but we still need to align it, or we won’t get what the RasterIO facility would faithfully give)."
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "",
    "text": "I have been working on a better understanding of the GDAL multidimensional model and to do that really needs a closer look at the GDAL API itself.\nThis post demonstrates loading the GDAL API via its Python bindings into R. We connect to a modern “cloud-ready” ZARR dataset, find out some details about its contents and then convert from its native multidimensional form to a more classic 2D raster model, then use that to create a map from Sentinel 2 imagery.\nWe don’t go very deep into any part, but just want to show a quick tour of some parts of GDAL that don’t get as much attention as deserved (IMO). I’m using a very recent version of GDAL, which might mean some code doesn’t work for you. If that’s the case please let me know and I can explore alternatives and identify when/how the newer features will be more available. There are some echoes here of an older post I made about GDAL in R: https://www.hypertidy.org/posts/2017-09-01_gdal-in-r/"
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#the-gdal-api",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#the-gdal-api",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "",
    "text": "I have been working on a better understanding of the GDAL multidimensional model and to do that really needs a closer look at the GDAL API itself.\nThis post demonstrates loading the GDAL API via its Python bindings into R. We connect to a modern “cloud-ready” ZARR dataset, find out some details about its contents and then convert from its native multidimensional form to a more classic 2D raster model, then use that to create a map from Sentinel 2 imagery.\nWe don’t go very deep into any part, but just want to show a quick tour of some parts of GDAL that don’t get as much attention as deserved (IMO). I’m using a very recent version of GDAL, which might mean some code doesn’t work for you. If that’s the case please let me know and I can explore alternatives and identify when/how the newer features will be more available. There are some echoes here of an older post I made about GDAL in R: https://www.hypertidy.org/posts/2017-09-01_gdal-in-r/"
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#zarr-and-the-european-space-agency-esa",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#zarr-and-the-european-space-agency-esa",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "ZARR and the European Space Agency (ESA)",
    "text": "ZARR and the European Space Agency (ESA)\nThe ESA is moving Copernicus to ZARR, launching its Earth Observation Processing Framework (EOPF) data format (Zarr). ZARR is “a community project to develop specifications and software for storage of large N-dimensional typed arrays”.\nA ZARR is a dataset consisting of trees of array chunks stored (usually) in object storage and indexed by fairly simple JSON metadata that describes how those chunks align together in one potentially very large array. The idea is that all the metadata lives upfront in instantly readable JSON, and changes made to the dataset (extending it each day as new data arrives) affects only the relevant chunks and the small parts of the JSON. This is different to a long list of NetCDFs that grows every day, where the metadata is self-contained for each file and there is no overarching abstraction for the entire file set.\nZARR is usually in object storage, and loaded by datacube software such as xarray. It’s not intended to be zipped into a huge file and downloaded or read, the real power lies in the entire dataset being lazy, and understood by software that needs just one data set description (url, or S3 path, etc)."
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#esa-sample-zarr-datasets",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#esa-sample-zarr-datasets",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "ESA sample Zarr datasets",
    "text": "ESA sample Zarr datasets\nThe ESA provide a set of example ZARRs that are available in zip files:\nhttps://eopf-public.s3.sbg.perf.cloud.ovh.net/product.html\nWe choose one that is described by this URL:\n\nurl &lt;- \"https://eopf-public.s3.sbg.perf.cloud.ovh.net/eoproducts/S02MSIL1C_20230629T063559_0000_A064_T3A5.zarr.zip\""
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#gdal-urls-and-zip-files",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#gdal-urls-and-zip-files",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "GDAL urls and zip files",
    "text": "GDAL urls and zip files\nGDAL doesn’t force us to download data, but we need some syntax to leverage its remote capabilities in the simplest way. The Virtual File System (VSI) allows us to declare special sources like zip files /vsizip/ and urls /vsicurl/, which we can chain together. With ZARR we also need careful quoting of the description, and we declare the ZARR driver upfront.\n\n(dsn &lt;- sprintf('ZARR:\"/vsizip//vsicurl/%s\"', url))\n\n[1] \"ZARR:\\\"/vsizip//vsicurl/https://eopf-public.s3.sbg.perf.cloud.ovh.net/eoproducts/S02MSIL1C_20230629T063559_0000_A064_T3A5.zarr.zip\\\"\""
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#gdal-and-multidimensional-datasets",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#gdal-and-multidimensional-datasets",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "GDAL and multidimensional datasets",
    "text": "GDAL and multidimensional datasets\nGDAL has a multidimensional mode for data sources that aren’t “imagery” in the traditional sense. (If we open one of these datasets in “classic” mode we end up with a lot of bands on a 2D raster, or potentially many bands on many subdatasets within a more general container. Zarr is a container format, much like HDF5 and NetCDF).\nMultidimensional mode is avaible in the API via OpenEx() and declaring type OF_MULTIDIM_RASTER.\nTo actually load this python library we use {reticulate} py_require() which drives the awesome Python uv package manager.\n(For some reason the pypi name of the package is “gdal”, but the actual module is obtained with “osgeo.gdal”).\n\nreticulate::py_require(\"gdal\")\ngdal &lt;- reticulate::import(\"osgeo.gdal\")\ngdal$UseExceptions()\n\nsample(names(gdal), 40)  ## see that we have a huge coverage of the underlying API, 544 elements at time of writing\n\n [1] \"GCI_NIRBand\"                       \"GCI_CyanBand\"                     \n [3] \"GFU_RedMax\"                        \"GRT_COMPOSITE\"                    \n [5] \"GRT_AGGREGATION\"                   \"GRIORA_Gauss\"                     \n [7] \"GPI_RGB\"                           \"GFT_Integer\"                      \n [9] \"GFU_AlphaMin\"                      \"GCI_SAR_C_Band\"                   \n[11] \"GCI_OtherIRBand\"                   \"ApplyVerticalShiftGrid\"           \n[13] \"CPLE_NoWriteAccess\"                \"deprecation_warn\"                 \n[15] \"Rasterize\"                         \"GCI_SAR_K_Band\"                   \n[17] \"TermProgress_nocb\"                 \"GMF_ALL_VALID\"                    \n[19] \"DCAP_CREATECOPY\"                   \"GRA_Q1\"                           \n[21] \"config_option\"                     \"DCAP_FIELD_DOMAINS\"               \n[23] \"GDAL_GCP_Info_set\"                 \"wrapper_GDALVectorTranslateDestDS\"\n[25] \"ColorTable\"                        \"ContourGenerate\"                  \n[27] \"DCAP_COORDINATE_EPOCH\"             \"GDT_UInt16\"                       \n[29] \"GDsCDeleteRelationship\"            \"GCI_YCbCr_CbBand\"                 \n[31] \"IsLineOfSightVisible\"              \"BuildVRTInternalObjects\"          \n[33] \"DEMProcessing\"                     \"GDALDestroyDriverManager\"         \n[35] \"DCAP_RENAME_LAYERS\"                \"MultiDimInfoOptions\"              \n[37] \"CPLES_XML_BUT_QUOTES\"              \"DitherRGB2PCT\"                    \n[39] \"GDT_TypeCount\"                     \"GARIO_ERROR\"                      \n\n\nThe API elements chain in the usual way that works in python with ‘object.element.thing.etc’ syntax uses R’s $ accessor.\n\ngdal$Dimension$GetIndexingVariable\n\n&lt;function Dimension.GetIndexingVariable at 0x7f08c9f61760&gt;\n signature: (self, *args) -&gt; 'GDALMDArrayHS *'"
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#open-the-data",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#open-the-data",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "Open the data",
    "text": "Open the data\nIt’s not very exciting yet.\n\nds &lt;- gdal$OpenEx(dsn, gdal$OF_MULTIDIM_RASTER)\nds\n\n&lt;osgeo.gdal.Dataset; proxy of &lt;Swig Object of type 'GDALDatasetShadow *' at 0x7f08ca0ebcf0&gt; &gt;\n\n\nTo actually find out what’s in there we have to traverse a tree of potentially nested “groups” that organize actual datasets in a hierarchy.\nGet the root group and dive in, it’s very tediuous but shows some of what is there. There might be MDArrays in a group, or there might just be more groups.\n\nrg &lt;- ds$GetRootGroup()\nrg$GetMDArrayNames()\n\nlist()\n\nrg$GetGroupNames()\n\n[1] \"conditions\"   \"measurements\" \"quality\"     \n\ng1 &lt;- rg$OpenGroup(\"quality\")\ng1$GetMDArrayNames()\n\nlist()\n\ng1$GetGroupNames()\n\n[1] \"l1c_quicklook\" \"mask\"         \n\ng2 &lt;- g1$OpenGroup(\"l1c_quicklook\")\ng2$GetMDArrayNames()\n\nlist()\n\ng2$GetGroupNames()\n\n[1] \"r10m\"\n\ng3 &lt;- g2$OpenGroup(\"r10m\")\ng3$GetMDArrayNames()\n\n[1] \"band\" \"x\"    \"y\"    \"tci\" \n\ng3$GetGroupNames()\n\nlist()\n\n\nFinally we got to actual data, we recognize ‘tci’ as being the quicklook RGB of Sentinel 2.\nTo avoid tedium write a quick recursive function to find all the MDArray, at each level use GetFullName() which provides the cumulative path to where we are in the tree.\n\nget_all_mdnames &lt;- function(rootgroup) {\n  groups &lt;- rootgroup$GetGroupNames()\n  groupname &lt;- rootgroup$GetFullName()\n  amd &lt;- rootgroup$GetMDArrayNames()\n  md &lt;- sprintf(\"%s/%s\", groupname, amd)\n  md &lt;- c(md, unlist(lapply(groups, \\(.g) get_all_mdnames(rootgroup$OpenGroup(.g)))))\n  md\n}\n\nget_all_mdnames(ds$GetRootGroup())\n\n  [1] \"/conditions/geometry/angle\"                        \n  [2] \"/conditions/geometry/band\"                         \n  [3] \"/conditions/geometry/detector\"                     \n  [4] \"/conditions/geometry/x\"                            \n  [5] \"/conditions/geometry/y\"                            \n  [6] \"/conditions/geometry/mean_sun_angles\"              \n  [7] \"/conditions/geometry/mean_viewing_incidence_angles\"\n  [8] \"/conditions/geometry/sun_angles\"                   \n  [9] \"/conditions/geometry/viewing_incidence_angles\"     \n [10] \"/conditions/mask/detector_footprint/r10m/x\"        \n [11] \"/conditions/mask/detector_footprint/r10m/y\"        \n [12] \"/conditions/mask/detector_footprint/r10m/b02\"      \n [13] \"/conditions/mask/detector_footprint/r10m/b03\"      \n [14] \"/conditions/mask/detector_footprint/r10m/b04\"      \n [15] \"/conditions/mask/detector_footprint/r10m/b08\"      \n [16] \"/conditions/mask/detector_footprint/r20m/x\"        \n [17] \"/conditions/mask/detector_footprint/r20m/y\"        \n [18] \"/conditions/mask/detector_footprint/r20m/b05\"      \n [19] \"/conditions/mask/detector_footprint/r20m/b06\"      \n [20] \"/conditions/mask/detector_footprint/r20m/b07\"      \n [21] \"/conditions/mask/detector_footprint/r20m/b11\"      \n [22] \"/conditions/mask/detector_footprint/r20m/b12\"      \n [23] \"/conditions/mask/detector_footprint/r20m/b8a\"      \n [24] \"/conditions/mask/detector_footprint/r60m/x\"        \n [25] \"/conditions/mask/detector_footprint/r60m/y\"        \n [26] \"/conditions/mask/detector_footprint/r60m/b01\"      \n [27] \"/conditions/mask/detector_footprint/r60m/b09\"      \n [28] \"/conditions/mask/detector_footprint/r60m/b10\"      \n [29] \"/conditions/mask/l1c_classification/r60m/x\"        \n [30] \"/conditions/mask/l1c_classification/r60m/y\"        \n [31] \"/conditions/mask/l1c_classification/r60m/b00\"      \n [32] \"/conditions/meteorology/cams/latitude\"             \n [33] \"/conditions/meteorology/cams/longitude\"            \n [34] \"/conditions/meteorology/cams/aod1240\"              \n [35] \"/conditions/meteorology/cams/aod469\"               \n [36] \"/conditions/meteorology/cams/aod550\"               \n [37] \"/conditions/meteorology/cams/aod670\"               \n [38] \"/conditions/meteorology/cams/aod865\"               \n [39] \"/conditions/meteorology/cams/bcaod550\"             \n [40] \"/conditions/meteorology/cams/duaod550\"             \n [41] \"/conditions/meteorology/cams/isobaricInhPa\"        \n [42] \"/conditions/meteorology/cams/number\"               \n [43] \"/conditions/meteorology/cams/omaod550\"             \n [44] \"/conditions/meteorology/cams/ssaod550\"             \n [45] \"/conditions/meteorology/cams/step\"                 \n [46] \"/conditions/meteorology/cams/suaod550\"             \n [47] \"/conditions/meteorology/cams/surface\"              \n [48] \"/conditions/meteorology/cams/time\"                 \n [49] \"/conditions/meteorology/cams/valid_time\"           \n [50] \"/conditions/meteorology/cams/z\"                    \n [51] \"/conditions/meteorology/ecmwf/latitude\"            \n [52] \"/conditions/meteorology/ecmwf/longitude\"           \n [53] \"/conditions/meteorology/ecmwf/isobaricInhPa\"       \n [54] \"/conditions/meteorology/ecmwf/msl\"                 \n [55] \"/conditions/meteorology/ecmwf/number\"              \n [56] \"/conditions/meteorology/ecmwf/r\"                   \n [57] \"/conditions/meteorology/ecmwf/step\"                \n [58] \"/conditions/meteorology/ecmwf/surface\"             \n [59] \"/conditions/meteorology/ecmwf/tco3\"                \n [60] \"/conditions/meteorology/ecmwf/tcwv\"                \n [61] \"/conditions/meteorology/ecmwf/time\"                \n [62] \"/conditions/meteorology/ecmwf/u10\"                 \n [63] \"/conditions/meteorology/ecmwf/v10\"                 \n [64] \"/conditions/meteorology/ecmwf/valid_time\"          \n [65] \"/measurements/reflectance/r10m/x\"                  \n [66] \"/measurements/reflectance/r10m/y\"                  \n [67] \"/measurements/reflectance/r10m/b02\"                \n [68] \"/measurements/reflectance/r10m/b03\"                \n [69] \"/measurements/reflectance/r10m/b04\"                \n [70] \"/measurements/reflectance/r10m/b08\"                \n [71] \"/measurements/reflectance/r20m/x\"                  \n [72] \"/measurements/reflectance/r20m/y\"                  \n [73] \"/measurements/reflectance/r20m/b05\"                \n [74] \"/measurements/reflectance/r20m/b06\"                \n [75] \"/measurements/reflectance/r20m/b07\"                \n [76] \"/measurements/reflectance/r20m/b11\"                \n [77] \"/measurements/reflectance/r20m/b12\"                \n [78] \"/measurements/reflectance/r20m/b8a\"                \n [79] \"/measurements/reflectance/r60m/x\"                  \n [80] \"/measurements/reflectance/r60m/y\"                  \n [81] \"/measurements/reflectance/r60m/b01\"                \n [82] \"/measurements/reflectance/r60m/b09\"                \n [83] \"/measurements/reflectance/r60m/b10\"                \n [84] \"/quality/l1c_quicklook/r10m/band\"                  \n [85] \"/quality/l1c_quicklook/r10m/x\"                     \n [86] \"/quality/l1c_quicklook/r10m/y\"                     \n [87] \"/quality/l1c_quicklook/r10m/tci\"                   \n [88] \"/quality/mask/r10m/x\"                              \n [89] \"/quality/mask/r10m/y\"                              \n [90] \"/quality/mask/r10m/b02\"                            \n [91] \"/quality/mask/r10m/b03\"                            \n [92] \"/quality/mask/r10m/b04\"                            \n [93] \"/quality/mask/r10m/b08\"                            \n [94] \"/quality/mask/r20m/x\"                              \n [95] \"/quality/mask/r20m/y\"                              \n [96] \"/quality/mask/r20m/b05\"                            \n [97] \"/quality/mask/r20m/b06\"                            \n [98] \"/quality/mask/r20m/b07\"                            \n [99] \"/quality/mask/r20m/b11\"                            \n[100] \"/quality/mask/r20m/b12\"                            \n[101] \"/quality/mask/r20m/b8a\"                            \n[102] \"/quality/mask/r60m/x\"                              \n[103] \"/quality/mask/r60m/y\"                              \n[104] \"/quality/mask/r60m/b01\"                            \n[105] \"/quality/mask/r60m/b09\"                            \n[106] \"/quality/mask/r60m/b10\"                            \n\n\nHappily, we see our target MDArray name in there /quality/l1c_quicklook/r10m/tci."
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#actual-data",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#actual-data",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "Actual data",
    "text": "Actual data\nFinally let’s get some data out. We can obtain the MDArray now by full name and find out some properties.\n\nreticulate::py_require(\"gdal\")\ngdal &lt;- reticulate::import(\"osgeo.gdal\")\ngdal$UseExceptions()\n\ndsn &lt;- \"ZARR:\\\"/vsizip//vsicurl/https://eopf-public.s3.sbg.perf.cloud.ovh.net/eoproducts/S02MSIL1C_20230629T063559_0000_A064_T3A5.zarr.zip\\\"\"\n\nds &lt;- gdal$OpenEx(dsn, gdal$OF_MULTIDIM_RASTER)\nrg &lt;- ds$GetRootGroup()\nmdname &lt;- \"/quality/l1c_quicklook/r10m/tci\"\nmd &lt;- rg$OpenMDArrayFromFullname(mdname)\nmd$GetDimensionCount()\n\n[1] 3\n\n\nTraverse the dimensions to get their sizes and names.\n\nvapply(md$GetDimensions(), \\(.d) .d$GetSize(), 0L)\n\n[1]     3 10980 10980\n\nvapply(md$GetDimensions(), \\(.d) .d$GetName(), \"\")\n\n[1] \"band\" \"y\"    \"x\"   \n\n\nExplore some metadata attributes.\n\nmnames &lt;- vapply(md$GetAttributes(), \\(.a) .a$GetName(), \"\")\n\n(meta &lt;- setNames(lapply(md$GetAttributes(), \\(.a) unlist(.a$Read())), mnames))\n\n$long_name\n[1] \"TCI: True Color Image\"\n\n$`proj:bbox`\n[1]  300000 4490220  409800 4600020\n\n$`proj:epsg`\n[1] 32636\n\n$`proj:shape`\n[1] 10980 10980\n\n$`proj:transform`\n[1]      10       0  300000       0     -10 4600020       0       0       1\n\n$`proj:wkt2`\n[1] \"PROJCS[\\\"WGS 84 / UTM zone 36N\\\",GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"6326\\\"]],PRIMEM[\\\"Greenwich\\\",0,AUTHORITY[\\\"EPSG\\\",\\\"8901\\\"]],UNIT[\\\"degree\\\",0.0174532925199433,AUTHORITY[\\\"EPSG\\\",\\\"9122\\\"]],AUTHORITY[\\\"EPSG\\\",\\\"4326\\\"]],PROJECTION[\\\"Transverse_Mercator\\\"],PARAMETER[\\\"latitude_of_origin\\\",0],PARAMETER[\\\"central_meridian\\\",33],PARAMETER[\\\"scale_factor\\\",0.9996],PARAMETER[\\\"false_easting\\\",500000],PARAMETER[\\\"false_northing\\\",0],UNIT[\\\"metre\\\",1,AUTHORITY[\\\"EPSG\\\",\\\"9001\\\"]],AXIS[\\\"Easting\\\",EAST],AXIS[\\\"Northing\\\",NORTH],AUTHORITY[\\\"EPSG\\\",\\\"32636\\\"]]\"\n\n\nWe see that the CRS information is in there, but sadly while the geotransform of the data is maintained when we convert to classic raster the CRS does not make the journey (the geotransform is just the image bbox intermingled with its resolution in a mathematical abstraction used in matrix manipulation).\nA multidim raster can be converted to classic form, either in whole or after applying a $GetView() operation that acts like numpy ‘[:]’ array subsetting.\nWhen converting to classic raster we specify the x, then y dimensions of the dataset, which from multdim convention are in reverse order. (Note that dimension 0 is the “3rd” dimension in normal thinking, here that dimension is “band” or a dimension for each of red, green, blue in the quicklook image).\n\ncc &lt;- md$AsClassicDataset(2L, 1L)\ntr &lt;- unlist(cc$GetGeoTransform())\ndm &lt;- c(cc$RasterXSize, cc$RasterYSize)\ncc$GetSpatialRef() ## empty, so we retrieve from the attributes\ncrs &lt;- cc$GetMetadata()[[\"proj:wkt2\"]]\n\nNow, I used reproj_extent to convert the dataset’s bbox to one in longlat, but I won’t share that here, we’ll just use the result so our map is a little more familiar. The source data xy bbox is xmin: 300000 ymin: 4490220  xmax: 409800 ymax:4600020, and we take small section of that which is this in longitude latitude:\n\n# xmin,xmax,ymin,ymax converted below to bbox \nex &lt;- c(31.689, 31.774, 40.665, 40.724)\n\nTo warp the imagery to this region we first need to set the CRS properly on the dataset, so translate to a temporary VRT and use that dataset for the next step.\n\ndsvrt &lt;- gdal$Translate(tempfile(fileext = \".vrt\", tmpdir = \"/vsimem\"), cc, options = c(\"-a_srs\", crs))\ntf &lt;- tf &lt;- \"/vsimem/result.tif\"\nww &lt;- gdal$Warp(tf, dsvrt,  dstSRS = \"EPSG:4326\", outputBounds = ex[c(1, 3, 2, 4)])\nww$Close()\n\n[1] 0\n\n\nFinally we can read the image, plot it, obtain some contextual data and be assured that our map is correct.\n\nlibrary(gdalraster)\ndata &lt;- read_ds(new(GDALRaster, tf))\n\n\nlibrary(osmdata)\nx &lt;- opq(ex[c(1, 3, 2, 4)]) |&gt; add_osm_feature(key = \"highway\") |&gt; osmdata_sf()\n\nlibrary(sf)\nplot_raster(data)\nplot(x$osm_lines[0], add = TRUE, col = \"hotpink\")"
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#summary",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#summary",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "Summary",
    "text": "Summary\nWe had a look at GDAL multidimensional API, converting a 3D dataset to classic raster, augmenting the missing CRS information on a virtual copy and then warping an image to a familiar map context.\nThere is ongoing work to bring the full GDAL API to R in {gdalraster}, and in-development version of gdalraster to add the GDALMultiDimRaster class and helpers: https://github.com/mdsumner/gdalraster/tree/multidimnew.\nPlease get in touch if any of this is of interest!"
  },
  {
    "objectID": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#late-bonus-section-bluelink-ocean-model",
    "href": "posts/2025-03-12_r-py-multidim/r-py-multidim.html#late-bonus-section-bluelink-ocean-model",
    "title": "GDAL multidim and cloud-ready ZARR",
    "section": "Late Bonus Section: Bluelink ocean model",
    "text": "Late Bonus Section: Bluelink ocean model\nThis is something I want to use VRT to encapsulate as one whole dataset, here just a test on one file (there are 174 of these ocean_temp files, up to June 2024 at time of writing).\nNote that we are using ‘/fileServer/’ from Thredds, not ‘/dodsC/’. GDAL uses userfaultd as a trick to read NetCDF remotely.\nfrom osgeo import gdal\ndsn = \"/vsicurl/https://thredds.nci.org.au/thredds/fileServer/gb6/BRAN/BRAN2023/daily/ocean_temp_2010_01.nc\"\nds = gdal.OpenEx(dsn, gdal.OF_MULTIDIM_RASTER)\nrg = ds.GetRootGroup()\nrg.GetMDArrayNames()\n# ['xt_ocean', 'yt_ocean', 'st_ocean', 'Time', 'nv', 'st_edges_ocean', 'average_T1', 'average_T2', 'average_DT', 'Time_bnds', 'temp']\n\ntemp = rg.OpenMDArrayFromFullname(\"//temp\")\n[d.GetName() for d in temp.GetDimensions()]\n#['Time', 'st_ocean', 'yt_ocean', 'xt_ocean']\n[d.GetSize() for d in temp.GetDimensions()]\n#[31, 51, 1500, 3600]\nI don’t know why xarray can seemingly read netcdf from S3, but not from normal url? Maybe just something fsspec I need to understand?\nI also noticed that if the file is local the array names at root level are like this:\n['//xt_ocean', '//yt_ocean', '//st_ocean', '//Time', '//nv', '//st_edges_ocean', '//average_T1', '//average_T2', '//average_DT', '//Time_bnds', '//temp']\n\n\ngdalraster::gdal_version()\n\n[1] \"GDAL 3.11.0dev-dd6009a0eb, released 2018/99/99\"\n[2] \"3110000\"                                       \n[3] \"20189999\"                                      \n[4] \"3.11.0dev-dd6009a0eb\"                          \n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.2 (2024-10-31)\n os       Ubuntu 20.04.6 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_AU.UTF-8\n ctype    en_AU.UTF-8\n tz       Australia/Hobart\n date     2025-03-12\n pandoc   3.2 @ /usr/lib/rstudio-server/bin/quarto/bin/tools/x86_64/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version     date (UTC) lib source\n bit           4.5.0.1     2024-12-03 [2] CRAN (R 4.4.2)\n bit64         4.6.0-1     2025-01-16 [2] CRAN (R 4.4.2)\n cachem        1.1.0       2024-05-16 [2] CRAN (R 4.4.1)\n class         7.3-23      2025-01-01 [4] CRAN (R 4.4.2)\n classInt      0.4-11      2025-01-08 [2] CRAN (R 4.4.2)\n cli           3.6.4       2025-02-13 [2] CRAN (R 4.4.2)\n codetools     0.2-20      2024-03-31 [4] CRAN (R 4.4.1)\n curl          6.2.1       2025-02-19 [2] CRAN (R 4.4.2)\n DBI           1.2.3       2024-06-02 [2] CRAN (R 4.4.1)\n devtools      2.4.5       2022-10-11 [2] CRAN (R 4.4.1)\n digest        0.6.37      2024-08-19 [2] CRAN (R 4.4.1)\n e1071         1.7-16      2024-09-16 [2] CRAN (R 4.4.1)\n ellipsis      0.3.2       2021-04-29 [2] CRAN (R 4.4.1)\n evaluate      1.0.3       2025-01-10 [2] CRAN (R 4.4.2)\n fastmap       1.2.0       2024-05-15 [2] CRAN (R 4.4.1)\n fs            1.6.5.9000  2024-12-18 [2] Github (r-lib/fs@8d40528)\n gdalraster  * 1.12.0.9250 2025-03-10 [1] local\n generics      0.1.3       2022-07-05 [2] CRAN (R 4.4.1)\n glue          1.8.0       2024-09-30 [2] CRAN (R 4.4.1)\n here          1.0.1       2020-12-13 [2] CRAN (R 4.4.1)\n htmltools     0.5.8.1     2024-04-04 [2] CRAN (R 4.4.1)\n htmlwidgets   1.6.4       2023-12-06 [2] CRAN (R 4.4.1)\n httpuv        1.6.15      2024-03-26 [2] CRAN (R 4.4.1)\n httr2         1.1.0       2025-01-18 [2] CRAN (R 4.4.2)\n jsonlite      1.9.0       2025-02-19 [2] CRAN (R 4.4.2)\n KernSmooth    2.23-26     2025-01-01 [4] CRAN (R 4.4.2)\n knitr         1.49        2024-11-08 [2] CRAN (R 4.4.2)\n later         1.3.2       2023-12-06 [2] CRAN (R 4.4.1)\n lattice       0.22-6      2024-03-20 [4] CRAN (R 4.4.1)\n lifecycle     1.0.4       2023-11-07 [2] CRAN (R 4.4.1)\n lubridate     1.9.4       2024-12-08 [2] CRAN (R 4.4.2)\n magrittr      2.0.3       2022-03-30 [2] CRAN (R 4.4.1)\n Matrix        1.7-0       2024-04-26 [2] CRAN (R 4.4.1)\n memoise       2.0.1       2021-11-26 [2] CRAN (R 4.4.1)\n mime          0.12        2021-09-28 [2] CRAN (R 4.4.1)\n miniUI        0.1.1.1     2018-05-18 [2] CRAN (R 4.4.1)\n nanoarrow     0.6.0       2024-10-13 [2] CRAN (R 4.4.2)\n osmdata     * 0.2.5       2023-08-14 [2] CRAN (R 4.4.2)\n pkgbuild      1.4.4       2024-03-17 [2] CRAN (R 4.4.1)\n pkgload       1.4.0       2024-06-28 [2] CRAN (R 4.4.1)\n png           0.1-8       2022-11-29 [2] CRAN (R 4.4.1)\n profvis       0.4.0       2024-09-20 [2] CRAN (R 4.4.1)\n promises      1.3.0       2024-04-05 [2] CRAN (R 4.4.1)\n proxy         0.4-27      2022-06-09 [2] CRAN (R 4.4.1)\n purrr         1.0.4.9000  2025-02-19 [2] Github (tidyverse/purrr@9c8beb4)\n R6            2.6.1       2025-02-15 [2] CRAN (R 4.4.2)\n rappdirs      0.3.3       2021-01-31 [2] CRAN (R 4.4.1)\n Rcpp          1.0.14      2025-01-12 [2] CRAN (R 4.4.2)\n remotes       2.5.0       2024-03-17 [2] CRAN (R 4.4.1)\n reticulate  * 1.41.0      2025-02-24 [2] CRAN (R 4.4.2)\n rlang         1.1.5       2025-01-17 [2] CRAN (R 4.4.2)\n rmarkdown     2.29        2024-11-04 [2] CRAN (R 4.4.2)\n rprojroot     2.0.4       2023-11-05 [2] CRAN (R 4.4.1)\n rstudioapi    0.16.0      2024-03-24 [2] CRAN (R 4.4.1)\n sessioninfo   1.2.2       2021-12-06 [2] CRAN (R 4.4.1)\n sf          * 1.0-19      2024-11-05 [2] CRAN (R 4.4.2)\n shiny         1.9.1       2024-08-01 [2] CRAN (R 4.4.1)\n timechange    0.3.0       2024-01-18 [2] CRAN (R 4.4.1)\n units         0.8-5       2023-11-28 [2] CRAN (R 4.4.1)\n urlchecker    1.0.1       2021-11-30 [2] CRAN (R 4.4.1)\n usethis       3.0.0       2024-07-29 [2] CRAN (R 4.4.1)\n vctrs         0.6.5       2023-12-01 [2] CRAN (R 4.4.1)\n withr         3.0.2       2024-10-28 [2] CRAN (R 4.4.1)\n wk            0.9.4       2024-10-11 [2] CRAN (R 4.4.1)\n xfun          0.50        2025-01-07 [2] CRAN (R 4.4.2)\n xml2          1.3.6       2023-12-04 [2] CRAN (R 4.4.1)\n xtable        1.8-4       2019-04-21 [2] CRAN (R 4.4.1)\n yaml          2.3.10      2024-07-26 [2] CRAN (R 4.4.1)\n\n [1] /perm_storage/home/mdsumner/R/x86_64-pc-linux-gnu-library/4.4\n [2] /usr/local/lib/R/site-library\n [3] /usr/lib/R/site-library\n [4] /usr/lib/R/library\n\n─ Python configuration ───────────────────────────────────────────────────────\n python:         /perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf/bin/python3\n libpython:      /perm_storage/home/mdsumner/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/libpython3.11.so\n pythonhome:     /perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf:/perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf\n virtualenv:     /perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf/bin/activate_this.py\n version:        3.11.11 (main, Feb 12 2025, 14:51:05) [Clang 19.1.6 ]\n numpy:          /perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf/lib/python3.11/site-packages/numpy\n numpy_version:  2.2.3\n osgeo:          /perm_storage/home/mdsumner/.cache/R/reticulate/uv/cache/archive-v0/twFzciVwGCsaiQXG6_cWf/lib/python3.11/site-packages/osgeo\n \n NOTE: Python version was forced by py_require()\n\n──────────────────────────────────────────────────────────────────────────────\n\nreticulate::py_require()\n\n══════════════════════════ Python requirements ══════════════════════════\n\n\n── Current requirements ─────────────────────────────────────────────────\n\n\n Python:   [No Python version specified. Will default to '3.11.11']\n Packages: numpy, gdal\n\n\n── R package requests ───────────────────────────────────────────────────\n\n\nR package  Python packages                           Python version      \nreticulate numpy"
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html",
    "title": "Raster logic without pixels",
    "section": "",
    "text": "Every geospatial package that works with grids has raster logic inside. The old {raster} package in R established powerful abstraction functions and now {terra} includes an improved suite of those, the {stars} package has its own internally, GDAL obviously uses these abstractions deeply, and xarray and many other Python packages also provide this. There is similar logic built into R’s matrix and array functions, its visualization image() function, and the newer rasterImage().\nIs there a problem? This logic is almost always coupled to a data structure. You can’t use raster’s cellFromXY() without a RasterLayer. You can’t use terra’s xyFromCell() or cells() without a SpatRaster. Many packages have embedded this reinvented wheel, and to more or less degree lock the wheel inside a bigger machine.\nThe logic itself is beautifully simple: given a grid with certain dimensions (ncol, nrow) and a spatial extent (xmin, xmax, ymin, ymax, or identically ‘bbox’ xmin,ymin,xmax,ymax), you can compute everything else. Cell indices, row/column positions, coordinate centres and corners, cropping and snapping—all of it flows from six numbers and some basic arithmetic.\nThat’s what {vaster} extracts: the logic alone, without any data. (This logic of course is not especially 2D and extends into n-dimensional concepts as well illustrated by xarray and others, but please let’s park that from our discussion here.)"
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#raster-logic-is-in-our-software",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#raster-logic-is-in-our-software",
    "title": "Raster logic without pixels",
    "section": "",
    "text": "Every geospatial package that works with grids has raster logic inside. The old {raster} package in R established powerful abstraction functions and now {terra} includes an improved suite of those, the {stars} package has its own internally, GDAL obviously uses these abstractions deeply, and xarray and many other Python packages also provide this. There is similar logic built into R’s matrix and array functions, its visualization image() function, and the newer rasterImage().\nIs there a problem? This logic is almost always coupled to a data structure. You can’t use raster’s cellFromXY() without a RasterLayer. You can’t use terra’s xyFromCell() or cells() without a SpatRaster. Many packages have embedded this reinvented wheel, and to more or less degree lock the wheel inside a bigger machine.\nThe logic itself is beautifully simple: given a grid with certain dimensions (ncol, nrow) and a spatial extent (xmin, xmax, ymin, ymax, or identically ‘bbox’ xmin,ymin,xmax,ymax), you can compute everything else. Cell indices, row/column positions, coordinate centres and corners, cropping and snapping—all of it flows from six numbers and some basic arithmetic.\nThat’s what {vaster} extracts: the logic alone, without any data. (This logic of course is not especially 2D and extends into n-dimensional concepts as well illustrated by xarray and others, but please let’s park that from our discussion here.)"
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#r-already-knows-this-sort-of",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#r-already-knows-this-sort-of",
    "title": "Raster logic without pixels",
    "section": "R already knows this (sort of)",
    "text": "R already knows this (sort of)\nR’s base graphics already embody two distinct models for placing gridded data in space.\n\nThe image() model: rectilinear coordinates\n\nm &lt;- volcano[1:10, 1:15]\nx &lt;- seq(0, 1, length.out = nrow(m) + 1)\ny &lt;- seq(0, 2, length.out = ncol(m) + 1)\n\nimage(x, y, m, col = terrain.colors(12))\n\n\n\n\n\n\n\n\nThe image() function takes explicit coordinate vectors for cell boundaries. This is the rectilinear model—you define where every edge falls. It’s flexible (cells don’t need to be square, or even uniform), but it means carrying around those vectors.\nNotice x has nrow(m) + 1 elements, and y has ncol(m) + 1. These are edge coordinates, not centres. The function figures out that a 10×15 matrix needs 11×16 edges. (Centres are also a valid input for image(), it quietly handles either case of ‘n’ or ‘n+1’, note this has implications for interpretation of a grid but that takes us away from the main topic here).\n\n\nThe rasterImage() model: bounding box placement\n\nplot(NA, xlim = c(0, 1), ylim = c(0, 2), asp = 1, xlab = \"x\", ylab = \"y\")\nrasterImage(as.raster(scales::rescale(m)), xleft = 0, ybottom = 0, xright = 1, ytop = 2, interpolate = F)\n\n\n\n\n\n\n\n\nThe rasterImage() function takes a different approach: you hand it an image and four numbers defining the bounding box. The function stretches or compresses the image to fit. This is the affine model—the grid is implicitly regular within the box.\nThese two models are part of a foundation of all raster handling. GeoTIFFs use the affine model (extent + dimension → implicit coordinates). NetCDF often uses the rectilinear model (explicit coordinate arrays). Both are valid; both are useful; both involve the same underlying logic.\n\n\n{ximage}: unifying both models\nThere’s a sort of frustration in base with image() and rasterImage(), each has features the other lacks. image() handles numeric data with colour palettes and is geared to R’s matrix orientation, can create a plot or add to an existing one, but by default will draw into a unit square. rasterImage() handles the orientation more aligned to external raster data and graphics, but only works with unit-scaled data or pre-rendered images—no palette mapping, and no plot creation (only adding to an existing plot) .\n{ximage} merges the features of both into one function that uses the rasterImage orientation:\nlibrary(ximage)\n\n## Plot numeric data in GIS orientation with extent\nximage(volcano, extent = c(0, 61, 0, 87), col = terrain.colors(24))\n\n## Or RGB arrays, or nativeRaster, or hex colours—all work\nximage(rgb_array, extent = c(100, 160, -50, -10))\n\n## Add contours that respect the same extent\nxcontour(volcano, extent = c(0, 61, 0, 87), add = TRUE, levels = c(120, 140, 160))\nOnce we separate the data (a matrix of values) from the placement (extent as four numbers), you can handle any input type with the same logic. The orientation confusion disappears because {ximage} adopts the rasterImage convention—the one that matches how GDAL and every other geospatial tool returns data.\nThis is {vaster}’s philosophy applied to plotting: the spatial meaning comes from the six numbers (dimension + extent), not from the data structure."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#what-vaster-provides",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#what-vaster-provides",
    "title": "Raster logic without pixels",
    "section": "What vaster provides",
    "text": "What vaster provides\n{vaster} gives that underlying logic with no attachment to any data format or structure. All you need are dimension and extent (and extent is optional, a sensible default is [0,nx], [0,ny] rather than unit-square).\n\nlibrary(vaster)\n\ndm &lt;- c(40, 20)  # ncol, nrow\nex &lt;- c(100, 160, -50, -10)  # xmin, xmax, ymin, ymax\n\n# Cell centres - implicit from dimension and extent\nx_centre(dm, ex)\n\n [1] 100.75 102.25 103.75 105.25 106.75 108.25 109.75 111.25 112.75 114.25\n[11] 115.75 117.25 118.75 120.25 121.75 123.25 124.75 126.25 127.75 129.25\n[21] 130.75 132.25 133.75 135.25 136.75 138.25 139.75 141.25 142.75 144.25\n[31] 145.75 147.25 148.75 150.25 151.75 153.25 154.75 156.25 157.75 159.25\n\ny_centre(dm, ex)\n\n [1] -49 -47 -45 -43 -41 -39 -37 -35 -33 -31 -29 -27 -25 -23 -21 -19 -17 -15 -13\n[20] -11\n\n\nWhat cell contains a given point?\n\n# Some arbitrary coordinates\npts &lt;- cbind(x = c(120.5, 145.2, 110.8), \n             y = c(-25.3, -42.1, -15.7))\n\ncell_from_xy(dm, ex, pts)\n\n[1] 294 671  88\n\n\nWhat are the coordinates of cells 1, 100, and 800?\n\nxy_from_cell(dm, ex, c(1, 100, 800))\n\n       [,1] [,2]\n[1,] 100.75  -11\n[2,] 129.25  -15\n[3,] 159.25  -49\n\n\nConvert between row/column and cell index:\n\n# Cell to row, column\ncell &lt;- 42\nrow_from_cell(dm, cell)\n\n[1] 2\n\ncol_from_cell(dm, cell)\n\n[1] 2\n\n# Row, column to cell\ncell_from_row_col(dm, row = 3, col = 15)\n\n[1] 95\n\n\nNone of this requires loading imagery or touching files. It’s pure computation from first principles.\nNote here that indexing is via R’s convention: 1-based, both for cell and row and col."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-snap-problem",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-snap-problem",
    "title": "Raster logic without pixels",
    "section": "The snap problem",
    "text": "The snap problem\nOne of the most common operations in raster work is “snapping”—taking an arbitrary region and aligning it to an existing grid. You want to crop to an area of interest, but you need cell-aligned boundaries.\n\n# A reference grid: 3-degree cells covering the globe\nref_dm &lt;- c(30, 10)\nref_ex &lt;- c(-180, 180, -90, 90)\n\n# Some arbitrary region of interest (not aligned to anything)\nroi &lt;- c(15, 154.7, -44.2, -9.8)\n\n# Snap to the reference grid\nsnapped &lt;- vcrop(roi, ref_dm, extent = ref_ex)\nsnapped\n\n$extent\n[1]  12 156 -54   0\n\n$dimension\n[1] 12  3\n\n\nThe result gives you the exact dimension and extent for the crop window, aligned to the reference grid. No data needed—just the numbers.\n\nplot_extent(ref_ex, border = \"grey\")\nabline(v = x_corner(ref_dm, ref_ex), col = \"grey90\")\nabline(h = y_corner(ref_dm, ref_ex), col = \"grey90\")\n\n# Original ROI (arbitrary)\nplot_extent(roi, border = \"red\", lwd = 2, add = TRUE)\n\n# Snapped ROI (grid-aligned)\nplot_extent(snapped$extent, border = \"blue\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nThe red box is what we asked for; the blue box is what we get when we respect the grid alignment."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-geotransform-connection",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-geotransform-connection",
    "title": "Raster logic without pixels",
    "section": "The geotransform connection",
    "text": "The geotransform connection\nGDAL uses a six-element “geotransform” to encode the affine relationship between pixel coordinates and geographic coordinates. This is the workhorse of georeferenced raster data. {vaster} speaks this language:\n\n# From extent and dimension to geotransform\ngt &lt;- extent_dim_to_gt(ex, dm)\ngt\n\n xmin  xres yskew  ymax xskew  yres \n100.0   1.5   0.0 -10.0   0.0  -2.0 \n\n# And back again\ngt_dim_to_extent(gt, dm)\n\nxmin xmin ymax ymax \n 100  160  -50  -10 \n\n\nThis matters when you’re working directly with GDAL (via {gdalraster}, {vapour}, or Python’s osgeo.gdal) and need to set up or interpret raster metadata without constructing heavyweight objects."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#why-does-this-matter",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#why-does-this-matter",
    "title": "Raster logic without pixels",
    "section": "Why does this matter?",
    "text": "Why does this matter?\n\nLightweight tooling: Sometimes we just need to compute cell indices or snap extents. We shouldn’t need to load terra or GDAL for that.\nCross-package compatibility: The logic is the same whether you’re working with terra, stars, GDAL, or raw arrays. Having it in one place means consistent behaviour everywhere.\nTeaching and understanding: Separating the logic from the data makes it clearer what raster operations actually do. The magic isn’t in the file format or the object class—it’s in the relationship between dimension and extent."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-ecosystem-what-you-can-build-with-pure-logic",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-ecosystem-what-you-can-build-with-pure-logic",
    "title": "Raster logic without pixels",
    "section": "The ecosystem: what you can build with pure logic",
    "text": "The ecosystem: what you can build with pure logic\nOnce you separate grid logic from data, interesting things become possible. Here are three packages that build on this foundation, taken (ahem) shamelessly from the hypertidy suite. (I’m not ignoring other important packages in R and Python that use these ideas, but these are chosen to focus on the core idea of what vaster is and why it exists. )\n\n{fasterize}: fast polygon rasterization\n{fasterize} is a high-performance replacement for raster::rasterize(). It uses the classic scanline algorithm to burn polygons onto a grid. The original fasterize required an actual RasterLayer as a template—not for its data, but for its six numbers (dimension and extent).\n## fasterize needs a \"raster\", but really it just needs dimension + extent\nlibrary(fasterize)\nr &lt;- raster::raster(ncol = 1000, nrow = 800, \n                    xmn = 0, xmx = 100, ymn = 0, ymx = 80)\nresult &lt;- fasterize(polygons_sf, r, field = \"value\")\nThe raster object is just a vessel for grid metadata, but fasterize also will create an actual numeric matrix in memory - so it has these two limitations at input and output. With raster logic available separately, we can drive the same algorithm with nothing but numbers.\n\n\n{controlledburn}: don’t materialize, just rasterize\n{controlledburn} takes the fasterize algorithm and strips away the last step: instead of filling a raster with values, it returns the indices of which cells each polygon covers. The output is run-length encoded scanline segments—start, end, row, polygon_id.\nlibrary(controlledburn)\nlibrary(vaster)\n\n## Define a grid with just numbers\next &lt;- c(100, 160, -50, -10)\ndm &lt;- c(500, 400)\n\n## Get the cell coverage index, not pixel values\nidx &lt;- burn_polygon(polygons_sf, extent = ext, dimension = dm)\nFor a 500,000 × 400,000 grid, materializing a raster would require terabytes. But storing the polygon coverage as run-length scanline indices? Tens of megabytes for typical distributions that real world polygons embody. The grid logic is the same—only the output changes.\nThis is the “cell abstraction” always implicit in raster operations. We can use these indices for extraction, for streaming aggregation, for anything that doesn’t require all pixels to exist at once.\n\n\n{grout}: tiling without tiles\n{grout} applies the same principle to tiling. Given a grid’s dimension and extent, plus a block size, it computes the complete tiling scheme: how many tiles, where each one falls, how much overlap (“dangle”) occurs when dimensions don’t divide evenly.\nlibrary(grout)\n\n## A raster that's 87 × 61 with 8 × 8 tiles\nscheme &lt;- grout(c(87, 61), extent = c(0, 87, 0, 61), blocksize = c(8, 8))\nscheme\n#&gt; tiles: 11, 8 (x * y = 88)\n#&gt; block: 8, 8\n#&gt; dangle: 1, 3\n#&gt; tile extent: 0, 88, -3, 61 (xmin,xmax,ymin,ymax)\n\n## Get offset/size for each tile (for GDAL RasterIO)\ntile_index(scheme)\nNo data loaded. No files opened. Just the arithmetic of how a grid subdivides. This is exactly what you need to drive tiled reading from GDAL, or to generate web map tile pyramids, or to parallelize processing across spatial chunks.\n\n\nThe common thread\nAll three packages share a design principle: the grid specification is just six numbers, and most operations don’t need anything else.\n{fasterize} showed that fast rasterization doesn’t need heavy objects—just geometry and grid metadata. {controlledburn} showed you don’t even need to materialize pixels—indices are enough. {grout} showed that tiling is pure arithmetic on dimension and extent.\n{vaster} is the foundation that makes this all clean. Instead of each package reinventing cellFromXY() and friends, they can share a common vocabulary for grid logic."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#python-has-the-same-mix-of-concepts-and-tools",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#python-has-the-same-mix-of-concepts-and-tools",
    "title": "Raster logic without pixels",
    "section": "Python has the same mix of concepts and tools",
    "text": "Python has the same mix of concepts and tools\nThe Python geospatial ecosystem has grappled with the same issues that motivated {vaster}.\nA recent pangeo discussion compared odc.stac and stackstac—two packages for loading satellite imagery into xarray. Despite ostensibly doing the same thing, they produce different results. Why?\nPixel coordinates: edge or centre? When you say a pixel is at coordinate (100.0, 200.0), do you mean that’s its centre, or its top-left corner? odc.stac defaults to centre, stackstac defaults to edge. Both are valid conventions; GIS tools tend toward corners, while climate/ocean data tends toward centres. But if you don’t know which convention your tool uses, your data shifts by half a pixel.\nCoordinate snapping: When your requested bounding box doesn’t align perfectly with the source grid, what happens? Different tools make different choices about how to snap—and those choices are usually invisible, buried in the library internals.\nThese are exactly the questions {vaster} makes explicit. Dimension and extent. Corner or centre. Snap in or snap out. The logic is universal; only the defaults vary.\n\nxarray’s new RasterIndex\nThe xarray team recently announced flexible indexing, including a RasterIndex that computes coordinates on-the-fly from an affine transform rather than storing explicit coordinate arrays. Sound familiar?\nFrom the post:\n\nFor 2D raster images, this function often takes the form of an Affine Transform. The rasterix library extends Xarray with a RasterIndex which computes coordinates for geospatial images such as GeoTiffs via Affine Transform.\n\nThis is the same insight: coordinates are implicit in dimension and extent. You don’t need to materialize a 7-terabyte coordinate array when six numbers suffice. The xarray team is building infrastructure for raster grid logic itself.\nThe Python ecosystem will align around this conceptual foundation—grid logic as well as R and other language."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-r-matrix-connection",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#the-r-matrix-connection",
    "title": "Raster logic without pixels",
    "section": "The R matrix connection",
    "text": "The R matrix connection\nR’s matrices already have most of this logic, just without the spatial semantics. A matrix has dimension (nrow, ncol). When you use image() with explicit coordinates, you’re adding extent. The [i, j] indexing is cell-from-row-column. which(m &gt; threshold, arr.ind = TRUE) gives you row-column-from-cell.\n{vaster} makes this connection explicit. It treats dimension and extent as first-class inputs, just as R treats matrices as first-class objects. The spatial meaning is in the numbers, not the container."
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#example-raster-logic-is-a-simplifying-principle",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#example-raster-logic-is-a-simplifying-principle",
    "title": "Raster logic without pixels",
    "section": "Example: raster logic is a simplifying principle",
    "text": "Example: raster logic is a simplifying principle\nEnough theory! Let’s make a map. The following string defines a huge world-coverage image server.\n\ndsn &lt;- \"WMTS:https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/WMTS/1.0.0/WMTSCapabilities.xml,layer=World_Imagery\"\n\nSo, obviously we need a huge stack of Python geospatial to make a map image … or maybe we can just use generic tools and do it in R.\nWe’re using {gdalraster} here because it’s the thinnest wrapper over the GDAL API, but the same calls work through {terra}, {vapour}, or Python’s osgeo.gdal. The point isn’t the package — it’s that the interface is just numbers and strings. I’m still using a mix of tools and plotting … but that’s life.\n\nlibrary(gdalraster)\nlibrary(ximage)\nds &lt;- new(GDALRaster, dsn)  ## same as terra::rast(dsn) or osgeo.gdal.Open(dsn) or rioxarray.open_rasterio(dsn)\nds\n\nC++ object of class GDALRaster\n Driver : OGC Web Map Tile Service (WMTS)\n DSN    : WMTS:https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/WMTS/1.0.0/WMTSCapabilities.xml,layer=World_Imagery\n Dim    : 1073741766, 1070224430, 4\n CRS    : WGS 84 / Pseudo-Mercator (EPSG:3857)\n Res    : 0.037323, 0.037323\n Bbox   : -20037507.260427, -19971868.890929, 20037507.248165, 19971868.903191\n\n\nWe don’t want the whole world and certainly not that much detail, how about Mawson Station in Antarctica, Mawson is at approximately 62.87E and -67.6S.\nWe should define a local equal area projection, on Mawson.\n\nprj &lt;- \"+proj=laea +lon_0=62.8742 +lat_0=-67.6033\"\n\nWhat is the right extent or bbox for that projection? It’s just metres around the zero-point, let’s go with a range.\n\nb &lt;- 125000\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\") ## use GDAL to manage IO and cleanup, same in other tools\nchk &lt;- warp(dsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nximage(read_ds(new(GDALRaster, tfile), bands = 1:3), asp = 1)\n\n\n\n\n\n\n\n\nWhat about a different data source, can we see any meaningful data in sea ice at this time of year?\n\nicedsn &lt;- \"/vsicurl/https://data.seaice.uni-bremen.de/amsr2/asi_daygrid_swath/s3125/2025/dec/Antarctic3125/asi-AMSR2-s3125-20251220-v5.4.tif\"\n\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(icedsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\ndat &lt;- read_ds(new(GDALRaster, tfile), bands = 1)\ndat[dat &gt; 100] &lt;- NA\n\nximage(dat, col = grey.colors(100), asp = 1)\n\n\n\n\n\n\n\nsummary(dat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   1.00   94.00   98.00   90.99  100.00  100.00  576851 \n\n\nNot unexpectedly at 3.125km resolution it’s a bit sparse on information, but we didn’t have to use a different approach to get coincident data. The ice is thick here, to see Mawson at about this time from Sentinel 2 imagery, try the estinel catalog, click ‘ESA’ and zoom around for more, you’ll see that dark hole in the sea ice).\nWhat about the bathymetry? Even at this scale we can see the disparity between bathymetric and topographic detail.\n\nbathdsn &lt;- \"/vsicurl/https://projects.pawsey.org.au/idea-gebco-tif/GEBCO_2024.tif\"\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(bathdsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nximage(dat &lt;- read_ds(new(GDALRaster, tfile), bands = 1), col = hcl.colors(128), asp = 1)\n\n\n\n\n\n\n\n\nLet’s zoom right in, and this time we’ll use terra so that adding contours is easy, read the REMA v2 2m topography and add it as contours.\n\nb &lt;- 3000\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(dsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nximage(read_ds(new(GDALRaster, tfile), bands = 1:3), asp = TRUE)\n\nremadsn &lt;- \"/vsicurl/https://raw.githubusercontent.com/mdsumner/rema-ovr/main/REMA-2m_dem_ovr.vrt\"\nlibrary(terra)\nr &lt;- rast(ext(-b, b, -b, b), crs = prj, ncols = 1024, nrows = 1024)\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(remadsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 1024))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nrema &lt;- read_ds(new(GDALRaster, tfile), bands = 1)\ncontour(setValues(r, rema), add = TRUE)"
  },
  {
    "objectID": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#thanks",
    "href": "posts/2025-12-21_vaster-grid-logic/vaster-grid-logic.html#thanks",
    "title": "Raster logic without pixels",
    "section": "Thanks",
    "text": "Thanks\nHopefully this sparks some interest in the simplicity of raster grid logic and more folks using the GDAL warp api to its full potential.\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.5 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Etc/UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] ximage_0.0.0.9012     gdalraster_2.3.0.9007 terra_1.8-80         \n[4] vaster_0.5.0         \n\nloaded via a namespace (and not attached):\n [1] cli_3.6.5          knitr_1.50         rlang_1.1.6        xfun_0.53         \n [5] yyjsonr_0.1.21     jsonlite_2.0.0     glue_1.8.0         bit_4.6.0         \n [9] htmltools_0.5.8.1  scales_1.4.0       rmarkdown_2.29     rappdirs_0.3.3    \n[13] evaluate_1.0.5     fastmap_1.2.0      lifecycle_1.0.4    yaml_2.3.10       \n[17] compiler_4.5.1     nanoarrow_0.7.0-2  codetools_0.2-19   RColorBrewer_1.1-3\n[21] htmlwidgets_1.6.4  Rcpp_1.1.0         rstudioapi_0.15.0  farver_2.1.2      \n[25] digest_0.6.39      wk_0.9.4.9000      R6_2.6.1           dichromat_2.0-0.1 \n[29] parallel_4.5.1     tools_4.5.1        bit64_4.6.0-1      palr_0.4.0        \n[33] xml2_1.5.1"
  },
  {
    "objectID": "posts/2024-12-11_conservative_gdal/index.html",
    "href": "posts/2024-12-11_conservative_gdal/index.html",
    "title": "Conservative regridding with GDAL (?)",
    "section": "",
    "text": "Can GDAL do conservative re-gridding? For cases of regular grid to regular grid, yes I think it can.\nPlease note that I’m using tools I’m comfortable with, because I wrote them. I will reframe in other tools and other languages in time. For some reason I’d been blocked on understanding this issue."
  },
  {
    "objectID": "posts/2024-12-11_conservative_gdal/index.html#simple-grid-with-four-values",
    "href": "posts/2024-12-11_conservative_gdal/index.html#simple-grid-with-four-values",
    "title": "Conservative regridding with GDAL (?)",
    "section": "Simple grid with four values",
    "text": "Simple grid with four values\nTake a grid, 2x2 with values 1,2,3,4 that sums to 10 and warp it to a new size.\n\n## target\ndm &lt;- c(2, 2)\n\ng1 &lt;- matrix(c(1, 2, 3, 4), dm[2], dm[1])\ne1 &lt;- c(-1, 1, -1, 1)\n\nlibrary(vapour)\nlibrary(terra)\n\nterra 1.8.29\n\n## we need this so we can use MEM: via dsn::mem()\nSys.setenv(GDAL_MEM_ENABLE_OPEN = \"YES\")\n\n\n## dsn::mem generates a Memory raster, \n## gdal_raster_data is the warper, here we get identity\nsg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"))\n\nunique(sg[[1]])\n\n[1] 1 2 3 4\n\nsum(sg[[1]])\n\n[1] 10\n\n## spatialize for easy plotting\ntor &lt;- function(x) {\n \n  dm &lt;- attr(x, \"dimension\")[2:1]\n  r &lt;- terra::rast(terra::ext(attr(x, \"extent\")), ncols = dm[1], nrows = dm[2], crs = attr(x, \"projection\"), \n                   vals = x[[1]])\n  r\n}\n\nr &lt;- \"sum\"\n\n##   (always use mem() \"live\" to avoid the garbage collector, and only for Float64 I'm afraid)\n\n\n## now use the same extent but reduce pixel size\ntg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"), target_ext = e1,\n                         target_dim = dm * 8, resample = r)\n\nstr(tg)\n\nList of 1\n $ : num [1:256] 0.0156 0.0156 0.0156 0.0156 0.0156 ...\n - attr(*, \"dimension\")= num [1:2] 16 16\n - attr(*, \"extent\")= num [1:4] -1 1 -1 1\n - attr(*, \"projection\")= chr \"GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AU\"| __truncated__\n\n\nSo we have a lot more values as we resized from 2x2 to 16x16.\nIt still looks the same, but the overall quantity has been distributed.\n\nplot(tor(tg), main = r)\n## draw boundaries on\nabline(v = vaster::x_corner(dm * 8, e1), h = vaster::y_corner(dm * 8, e1))\n\n\n\n\n\n\n\nprint(sum(tg[[1]]))\n\n[1] 10\n\n## our sum of 10 was distributed across 256 pixels\nunique(tg[[1]])\n\n[1] 0.015625 0.031250 0.046875 0.062500\n\n\nNow a different example, an actual map projection change.\nBut, gawd … there’s a bug here. WIP\n\n## now try reprojecting our unit 1x1 longlat grid to LAEA centred a few degrees east and north\ncrs &lt;- \"+proj=laea +lon_0=5 +lat_0=0\"\n## reproject the extent, we use a densified boundary to find the new extent\nprex &lt;- reproj::reproj_extent(c(-1, 1, -1, 1), crs, source = \"EPSG:4326\")\nnewg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"), \n                         target_ext = prex, target_res = c(10000, 10000), target_crs = crs, resample = \"sum\", options = \"-tap\")\nplot(tor(newg))\n\n\n\n\n\n\n\nsum(newg[[1]])\n\n[1] 10\n\n\nAnd what about trying to return, of course this can only be approximate (I want to know how this compares to other tools.)\n\n## what if we go the other way, \n\nnewdm &lt;- attr(newg, \"dimension\")\nnewext &lt;- attr(newg, \"extent\")\nx &lt;- gdal_raster_data(dsn::mem(matrix(newg[[1]],newdm[2], byrow = TRUE),  extent = newext, projection = crs), \n                 target_dim = dm, target_ext = e1, target_crs = \"EPSG:4326\", resample = \"sum\")\nplot(tor(x))\n\n\n\n\n\n\n\nsum(x[[1]])\n\n[1] 9.496864"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "",
    "text": "PLEASE NOTE (April 2022): this post has been migrated from an old site, and some details may have changed. There might an update to this post to reflect the rgl package as it is now. —\nThis post describes the mesh3d format used in the rgl package and particularly how colour properties are stored and used. There are recent changes to this behaviour (see ‘meshColor’), and previously the situation was not clearly documented."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "rgl",
    "text": "rgl\nThe rgl package has long provided interactive 3D graphics for R. The neat thing for me about 3D graphics is the requirement for mesh forms of data, and the fact that meshes are extremely useful for very many tasks. When we plot data in 3D we necessarily have to convert the usual spatial types into mesh forms. You can see me discuss that in more detail in this talk."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "The mesh3d format",
    "text": "The mesh3d format\nHere is an example of a mesh3d object, it stores two polygonal areas in a form ready for 3D graphics.\n\nmesh0 &lt;- structure(list(vb = structure(c(0, 0, 0, 1, 0, 1, 0, 1, 0.75, \n                                1, 0, 1, 1, 0.8, 0, 1, 0.5, 0.7, 0, 1, 0.8, 0.6, 0, 1, 0.69, \n                                0, 0, 1, 0.2, 0.2, 0, 1, 0.5, 0.2, 0, 1, 0.5, 0.4, 0, 1, 0.3, \n                                0.6, 0, 1, 0.2, 0.4, 0, 1, 1.1, 0.63, 0, 1, 1.23, 0.3, 0, 1), .Dim = c(4L, 14L)), \n               it = structure(c(1L, 8L, 12L, 9L, 8L, 1L, 7L, 6L, 5L, \n                                5L, 4L, 3L, 2L, 1L, 12L, 9L, 1L, 7L, 5L, 3L, 2L, 2L, 12L, 11L, \n                                10L, 9L, 7L, 5L, 2L, 11L, 10L, 7L, 5L, 5L, 11L, 10L, 6L, 7L, \n                                14L, 14L, 13L, 6L), .Dim = c(3L, 14L)), \n               primitivetype = \"triangle\", \n               material = list(), \n               normals = NULL, \n               texcoords = NULL), \n               class = c(\"mesh3d\", \"shape3d\"))\n\n\nstr(mesh0)\n\nList of 6\n $ vb           : num [1:4, 1:14] 0 0 0 1 0 1 0 1 0.75 1 ...\n $ it           : int [1:3, 1:14] 1 8 12 9 8 1 7 6 5 5 ...\n $ primitivetype: chr \"triangle\"\n $ material     : list()\n $ normals      : NULL\n $ texcoords    : NULL\n - attr(*, \"class\")= chr [1:2] \"mesh3d\" \"shape3d\"\n\n\n(It’s not obvious about the polygons, please bear with me).\nThe following characterizes the structure.\n\ntwo matrix arrays vb and it\nvb has 4 rows and 14 columns, and contains floating point numbers\nit has 3 rows and 14 columns, and contains integers (starting at 1)\na primitivetype which is “triangle”\nan empty list of material propertes (this is the missing link for the polygons)\na NULL value for normals and texcoords, these won’t be discussed further (but see ?quadmesh::quadmesh for texture coordinates from spatial)\na class, this object is a mesh3d and inherits from shape3d\n\nThe vb array is the vertices, these are the corner coordinates of the elements of the mesh.\n\nplot(t(mesh0$vb), main = \"t(vb) - vertices\", xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\n\nThe elements of this mesh are triangles, and these are specified by the index array it. Elements of a mesh are called primitives, hence the primitivetype here.\n\nplot(t(mesh0$vb), main = \"t(vb[, it]) - primitives\", xlab = \"X\", ylab = \"Y\")\npolygon(t(mesh0$vb[, rbind(mesh0$it, NA)]), col = rgb(0.6, 0.6, 0.6, 0.5))"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Transpose",
    "text": "Transpose\nThese matrix arrays are transpose the way we usually use them in R, for now just remember that you must t()ranspose them for normal plotting, e.g. plot(t(mesh0$vb[1:2, ])) will give the expected scatter plot of the vertices. The reason these arrays are transpose is because each coordinate value is then contiguous in memory, each Y value is right next to its counterpart X, and Z (and W), and vb[it, ] provides a flat vector of XYZW values in a continuous block - this is a very important efficiency, and help explains why computer graphics use elements in a mesh form like this."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Colours",
    "text": "Colours\nUnsurprisingly, if we set the material property to a constant we get a constant colour.\n\nwidgetfun &lt;- function() {\n  view3d(0, phi = 8)\n  rglwidget()\n}\nmesh0$material$color &lt;- \"red\"\nlibrary(rgl)\nclear3d()\nshade3d(mesh0, lit = FALSE); \nwidgetfun()\n\n\n\n\n\nIn the usual R way our singleton colour value is magically recycled across every part of the shape, and it’s all red. But, is it recycled by vertices or by primitive? Until recently it was only possible to tell by trying (or reading the source code).\nHere I think it’s easy to see that the two colours are specified at the vertices, and they bleed across each triangle accordingly. We also get a warning that the behaviour has recently changed.\n\nclear3d()\nmesh0$material$color &lt;- c(\"firebrick\", \"black\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0)\nwidgetfun()\n\n\n\n\n\nThe default is to meshColor = \"vertices\", so let’s specify faces.\n\nclear3d()\nmesh0$material$color &lt;- c(\"firebrick\", \"dodgerblue\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0, meshColor = \"faces\")\nwidgetfun()\n\n\n\n\n\nSometimes we get neighbouring triangles with the same colour, so let’s also add the edges.\n\nmesh0$vb[3, ] &lt;- 0.01  ## vertical bias avoids z-fighting\n## material properties here override the recycling of internal colours\n## onto edges\nwire3d(mesh0, lwd = 5, color = \"black\")\nwidgetfun()\n\n\n\n\n\nIf we go a bit further we can see the original arrangement for this shape, two individual polygons that share a single edge.\nThis only works because I happen to know how this was created, and I know how this control of behaviour occurs in new rgl.\nThere are 12 triangles in the first polygon, and 2 in the second. (The original polygons can be seen here (left panel)).\n\nclear3d()\nmesh0$material$color &lt;- rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2))\nshade3d(mesh0, meshColor = \"faces\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nIf we treat the colours as applying to each vertex, then we needed to propagate it to each vertex around each face (triangle), and this is what rgl now calls legacy behaviour.\n\nclear3d()\nmesh0$material$color &lt;- rep(rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2)), each = 3)\nshade3d(mesh0, meshColor = \"legacy\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nWe cannot recreate this effect with meshColor = \"vertices\", because each of our vertices is actually unique. (It could be done by making the vb array every repeated vertex, and updating the index array but I can’t summon this up atm).\n\nclear3d()\nmesh0$material$color &lt;- rep_len(c(\"firebrick\", \"dodgerblue\"), length.out = ncol(mesh0$vb))\nshade3d(mesh0, meshColor = \"vertices\", lit = FALSE)\nwidgetfun()"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Primitives",
    "text": "Primitives\nThe other kind of element supported by mesh3d is a quad, specified by an ib array with 4 rows (ib versus it, 4 vertices versus 3) and the primitivetype = \"quad\".\nThe it values are an index into, i.e. the column number of the vertex array. The vertices, or coordinates, are stored by column in this structure, whereas normally we would store a coordinate per row.\nWhen I first explored mesh3d I was looking at a quad type mesh - and I was completely confused. Both vb and ib had four rows, and so while I understood that a quad must have 4 vertices (4 index values for every primitive), I did not understand why the vertices also had four rows.\n(There are other kinds of primitives in common use are edge, point, tetrahedron - but rgl has no formal class for these - in practice the edge type is referred to as segment in rgl, and tetrahedra are approximated by enclosing their shape with triangles)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Why does the vertex array have 4 rows?",
    "text": "Why does the vertex array have 4 rows?\nAll mesh3d objects have a vb array, and it always includes 4 rows.\nThe reason there are 4 rows in the vertex array is that these are homogeneous coordinates which …\n\nare ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied\n\n… yeah. For our purposes just think\n\nX, Y, Z in the usual sense and set W = 1.\n\n(Do not set W = 0 because your data will vanish to infinity when plotted with rgl, which is what those math folks are saying more or less)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "QUADS",
    "text": "QUADS\nNow let’s get a quad type mesh from the real world.\n\n## remotes::install_github(\"hypertidy/ceramic\")\nlibrary(ceramic)\ntopo &lt;- cc_elevation(raster::extent(-72, -69, -34, -32), zoom = 6)\n\nPreparing to download: 1 tiles at zoom = 6 from \nhttps://api.mapbox.com/v4/mapbox.terrain-rgb/\n\nqm &lt;- quadmesh::quadmesh(topo)\n\nstr(qm)\n\nList of 8\n $ vb             : num [1:4, 1:60225] -8015493 -3761925 0 1 -8014270 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"x\" \"y\" \"z\" \"1\"\n  .. ..$ : NULL\n $ ib             : int [1:4, 1:59732] 1 2 277 276 2 3 278 277 3 4 ...\n $ primitivetype  : chr \"quad\"\n $ material       : list()\n $ normals        : NULL\n $ texcoords      : NULL\n $ raster_metadata:List of 7\n  ..$ xmn  : num -8015493\n  ..$ xmx  : num -7680393\n  ..$ ymn  : num -4028537\n  ..$ ymx  : num -3761925\n  ..$ ncols: int 274\n  ..$ nrows: int 218\n  ..$ crs  : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n $ crs            : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n - attr(*, \"class\")= chr [1:3] \"quadmesh\" \"mesh3d\" \"shape3d\"\n\n\nThis topographic raster from near Santiago is now a mesh3d subclassed to quadmesh. This adds two properties raster_metadata and crs, which under limited conditions allows reconstruction of the original raster data. To drop back to a generic mesh3d the easiest is to reproject the data.\n\n##remotes::install_github(\"hypertidy/reproj\")\nlibrary(reproj)\nqm_ll &lt;- reproj(qm, \"+proj=longlat +datum=WGS84\")\n\nWarning in reproj.quadmesh(qm, \"+proj=longlat +datum=WGS84\"): quadmesh raster\ninformation cannot be preserved after reprojection, dropping to mesh3d class\n\n\nThis is a lossless reprojection, as it is equivalent to sf::sf_project(t(qm$vb[1:2, ]), from = qm$crs, to = \"+proj=longlat +datum=WGS84\") or with rgdal::project(, qm$crs, inv = TRUE).\nWe can plot this in the usual way with rgl, or see upcoming features in the mapdeck package.\n\nclear3d()\nshade3d(qm_ll, lit = TRUE, col = \"grey\")\naspect3d(1, 1, 0.1); \nview3d(0, phi = -60)\nrglwidget()\n\n\n\n\n\nTo put colours on this, we can do it by faces\n\nclear3d()\nqm_ll$material$color &lt;- colourvalues::color_values(raster::values(topo))\nshade3d(qm_ll, meshColor = \"faces\", lit = TRUE)\nrglwidget()\n\n\n\n\n\n(each face is discretely coloured), or by vertex in the legacy mode.\nNot run, to save the size of the document.\n\nclear3d()\nqm_ll$material$color &lt;-colourvalues::color_values(qm_ll$vb[3, qm_ll$ib])\n                                                   \nshade3d(qm_ll, meshColor = \"legacy\", lit = TRUE)\nrglwidget()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hypertidy-blog",
    "section": "",
    "text": "Raster logic without pixels\n\n\n\n\n\n\ncode\n\n\nnews\n\n\n\n\n\n\n\n\n\nDec 21, 2025\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nCoordinates broken in NetCDF\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 4, 2025\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL multidim and cloud-ready ZARR\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2025\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nConservative regridding with GDAL (?)\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nIDEA - data and software\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2024\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nPlot at native resolution, with R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL and image tiles, the {ceramic} package\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nDegenerate Rectilinear (WIP)\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL raster read/write by blocks\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nMay 4, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL warper with R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nmesh3d - recent changes in rgl workhorse format\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nMay 29, 2019\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 1, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nWeb services for scientific data in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 25, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nR spatial in 2017\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nJan 10, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGIS for 3D in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 28, 2015\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nR matrices and image\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 17, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a quarto website for the hypertidy family of R packages for multi-dimensional and spatial data.\nHypertidy is an approach to spatial or multi-dimensional data in R based on the following principles:\nExamples of these principles are seen in these R packages.\nvapour\nraadtools and angstroms\ntidync, lazyraster,\nsilicate, anglr, rbgm"
  },
  {
    "objectID": "about.html#gridded-data",
    "href": "about.html#gridded-data",
    "title": "About",
    "section": "Gridded data",
    "text": "Gridded data\nHypertidy recognizes that not all gridded data fit into the GIS raster conventions. Gridded data comes in many forms, geographic with longitude-latitude or projected spaces, with time and or depth dimensions, with different orderings of axes (i.e. time-first, the latitude-longitude), and with generally any arbitrary space. A space is simply a set of axes with particular units and projection, and yes we mean “space” and “projection” in the more general mathematical sense. Date-time data is a projection, there is a mapping of a particular set of values to the real line and the position on that line for particular instant is defined by the axis units and epoch.\nMesh and grid share the same meaning in some contexts."
  },
  {
    "objectID": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "href": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "title": "About",
    "section": "Structured vs. unstructured, topology vs. geometry",
    "text": "Structured vs. unstructured, topology vs. geometry\nArray-based data have a straight-forward relationship between a set of axes that have discrete steps. These are “structured grids”. Unstructured grids include triangulations, non-regularly binned histograms, tetrahedral meshes and ragged arrays.\nAn unstructured mesh (grid) is able to represent any data structure, but structured meshes have some advantages because of the regular indexing relationship between dimensions.\nGIS vector constructs “polygons”, “lines”, “points” are special case optimizations of the unstructured grid case. Polygons really are topologically identical to lines, and they are a dead-end in the broader scheme of dimensionality. Points and lines can are topologicaly 0-dimensional and 1-dimensional respectively, and this shape-constraint is the same no matter what geometric dimension they are defined in. A line can twist around a 4D space with x, y, z, t coordinates at its segment nodes or it can be constrained to single dimension with only one of those coordinates specifying its position. The topology of the line is completely independent of the geometry, if we treat the line as composed of topological primitives.\nPolygons are not composed of topological primitives, but they can be treated as being composed of line primitives."
  },
  {
    "objectID": "drafts/2025-06-30-degenerate-zarr/different_polar_netcdfs.html",
    "href": "drafts/2025-06-30-degenerate-zarr/different_polar_netcdfs.html",
    "title": "ncurl <- c(“/vsicurl/https://gws-access.jasmin.ac.uk/public/polarres/MetUM_PolarRES/Antarctic/daily/hfls_ANT-11_ERA5_evaluation_r1i1p1f1_BAS_MetUM_v1-r1_day_20000101_20001231.nc”,",
    "section": "",
    "text": "#— #title: “Degeneracy in array formats” #author: “Michael Sumner” #editor: source #date: “2025-06-30” #categories: [news, code] #draft: true #—\nConsider these netcdf files.\n\nncurl &lt;- c(“/vsicurl/https://gws-access.jasmin.ac.uk/public/polarres/MetUM_PolarRES/Antarctic/daily/hfls_ANT-11_ERA5_evaluation_r1i1p1f1_BAS_MetUM_v1-r1_day_20000101_20001231.nc”,\n\n\n“/vsicurl/http://ftp.climato.be/fettweis/MARv3.13/PolarRES/Antarctic/MAR-MPI-ESM1/MARv3.13-ANT-MPI-1985.nc”)\n\n\n\n\n\nlibrary(terra)\n\n\n\n\n\n## equivalent to “+proj=ob_tran +o_proj=longlat +o_lon_p=0 +o_lat_p=5 +lon_0=200 +datum=WGS84 +no_defs”\n\n\ncrs1 &lt;- ‘GEOGCRS[“Rotated_pole”,BASEGEOGCRS[“WGS 84”,DATUM[“World Geodetic System 1984”,ELLIPSOID[“WGS 84”,6378137,298.257223563,LENGTHUNIT[“metre”,1]]],PRIMEM[“Greenwich”,0,ANGLEUNIT[“degree”,0.0174532925199433]]],DERIVINGCONVERSION[“Pole rotation (netCDF CF convention)”,METHOD[“Pole rotation (netCDF CF convention)”],PARAMETER[“Grid north pole latitude (netCDF CF convention)”,5,ANGLEUNIT[“degree”,0.0174532925199433,ID[“EPSG”,9122]]],PARAMETER[“Grid north pole longitude (netCDF CF convention)”,20,ANGLEUNIT[“degree”,0.0174532925199433,ID[“EPSG”,9122]]],PARAMETER[“North pole grid longitude (netCDF CF convention)”,0,ANGLEUNIT[“degree”,0.0174532925199433,ID[“EPSG”,9122]]]],CS[ellipsoidal,2],AXIS[“geodetic latitude (Lat)”,north,ORDER[1],ANGLEUNIT[“degree”,0.0174532925199433,ID[“EPSG”,9122]]],AXIS[“geodetic longitude (Lon)”,east,ORDER[2],ANGLEUNIT[“degree”,0.0174532925199433,ID[“EPSG”,9122]]]]’\n\n\ncrs2 &lt;- “WGS84”\n\n\nll1 &lt;- c(rast(ncurl[1], “longitude”),\n\n\nrast(ncurl[1], “latitude”)) * 1\n\n\nd1 &lt;- rast(ncurl[1], “surface_upward_latent_heat_flux”)\n\n\n\n\n\nll2 &lt;- c(rast(ncurl[2], “LON”), rast(ncurl[2], “LAT”)) * 1\n\n\npar(mfrow = c(2, 1))\n\n\nplot(values(ll1), xlim = c(-360, 220), pch = “.”)\n\n\nplot(values(ll2), xlim = c(-360, 220), pch = “.”)\n\n\n\n\n\nplot(project(values(ll1), from = “EPSG:4326”, to = crs1), pch = “.”)\n\n\nplot(project(values(ll2), to = “EPSG:3031”, from = “EPSG:4326”), pch = “.”)"
  },
  {
    "objectID": "vaster-blog-draft.html",
    "href": "vaster-blog-draft.html",
    "title": "Raster logic without pixels",
    "section": "",
    "text": "Every geospatial package that works with grids has raster logic inside. The old {raster} package in R established powerful abstraction functions and now {terra} includes an improved suite of those, the {stars} package has its own internally, GDAL obviously uses these abstractions deeply, and xarray and many other Python packages also provide this. There is similar logic built into R’s matrix and array functions, its visualization image() function, and the newer rasterImage().\nIs there a problem? This logic is almost always coupled to a data structure. You can’t use raster’s cellFromXY() without a RasterLayer. You can’t use terra’s xyFromCell() or cells() without a SpatRaster. Many packages have embedded this reinvented wheel, and to more or less degree lock the wheel inside a bigger machine.\nThe logic itself is beautifully simple: given a grid with certain dimensions (ncol, nrow) and a spatial extent (xmin, xmax, ymin, ymax, or identically ‘bbox’ xmin,ymin,xmax,ymax), you can compute everything else. Cell indices, row/column positions, coordinate centres and corners, cropping and snapping—all of it flows from six numbers and some basic arithmetic.\nThat’s what {vaster} extracts: the logic alone, without any data. (This logic of course is not especially 2D and extends into n-dimensional concepts as well illustrated by xarray and others, but please let’s park that from our discussion here.)"
  },
  {
    "objectID": "vaster-blog-draft.html#raster-logic-is-in-our-software",
    "href": "vaster-blog-draft.html#raster-logic-is-in-our-software",
    "title": "Raster logic without pixels",
    "section": "",
    "text": "Every geospatial package that works with grids has raster logic inside. The old {raster} package in R established powerful abstraction functions and now {terra} includes an improved suite of those, the {stars} package has its own internally, GDAL obviously uses these abstractions deeply, and xarray and many other Python packages also provide this. There is similar logic built into R’s matrix and array functions, its visualization image() function, and the newer rasterImage().\nIs there a problem? This logic is almost always coupled to a data structure. You can’t use raster’s cellFromXY() without a RasterLayer. You can’t use terra’s xyFromCell() or cells() without a SpatRaster. Many packages have embedded this reinvented wheel, and to more or less degree lock the wheel inside a bigger machine.\nThe logic itself is beautifully simple: given a grid with certain dimensions (ncol, nrow) and a spatial extent (xmin, xmax, ymin, ymax, or identically ‘bbox’ xmin,ymin,xmax,ymax), you can compute everything else. Cell indices, row/column positions, coordinate centres and corners, cropping and snapping—all of it flows from six numbers and some basic arithmetic.\nThat’s what {vaster} extracts: the logic alone, without any data. (This logic of course is not especially 2D and extends into n-dimensional concepts as well illustrated by xarray and others, but please let’s park that from our discussion here.)"
  },
  {
    "objectID": "vaster-blog-draft.html#r-already-knows-this-sort-of",
    "href": "vaster-blog-draft.html#r-already-knows-this-sort-of",
    "title": "Raster logic without pixels",
    "section": "R already knows this (sort of)",
    "text": "R already knows this (sort of)\nR’s base graphics already embody two distinct models for placing gridded data in space.\n\nThe image() model: rectilinear coordinates\n\nm &lt;- volcano[1:10, 1:15]\nx &lt;- seq(0, 1, length.out = nrow(m) + 1)\ny &lt;- seq(0, 2, length.out = ncol(m) + 1)\n\nimage(x, y, m, col = terrain.colors(12))\n\n\n\n\n\n\n\n\nThe image() function takes explicit coordinate vectors for cell boundaries. This is the rectilinear model—you define where every edge falls. It’s flexible (cells don’t need to be square, or even uniform), but it means carrying around those vectors.\nNotice x has nrow(m) + 1 elements, and y has ncol(m) + 1. These are edge coordinates, not centres. The function figures out that a 10×15 matrix needs 11×16 edges. (Centres are also a valid input for image(), it quietly handles either case of ‘n’ or ‘n+1’, note this has implications for interpretation of a grid but that takes us away from the main topic here).\n\n\nThe rasterImage() model: bounding box placement\n\nplot(NA, xlim = c(0, 1), ylim = c(0, 2), asp = 1, xlab = \"x\", ylab = \"y\")\nrasterImage(as.raster(m/max(m)), xleft = 0, ybottom = 0, xright = 1, ytop = 2, interpolate = F)\n\n\n\n\n\n\n\n\nThe rasterImage() function takes a different approach: you hand it an image and four numbers defining the bounding box. The function stretches or compresses the image to fit. This is the affine model—the grid is implicitly regular within the box.\nThese two models are part of a foundation of all raster handling. GeoTIFFs use the affine model (extent + dimension → implicit coordinates). NetCDF often uses the rectilinear model (explicit coordinate arrays). Both are valid; both are useful; both involve the same underlying logic.\n\n\n{ximage}: unifying both models\nThere’s a sort of frustration in base are with image() and rasterImage(), each have features the other lacks. image() handles numeric data with colour palettes and is geared to R’s matrix orientation, can create a plot or add to an existing one, but by default will draw into a unit square. rasterImage() handles the orientation more aligned to external raster data and graphics, but only works with unit-scaled data or pre-rendered images—no palette mapping, and no plot creation (only adding to an existing plot) .\n{ximage} merges the features of both into one function that uses the rasterImage orientation:\nlibrary(ximage)\n\n## Plot numeric data in GIS orientation with extent\nximage(volcano, extent = c(0, 61, 0, 87), col = terrain.colors(24))\n\n## Or RGB arrays, or nativeRaster, or hex colours—all work\nximage(rgb_array, extent = c(100, 160, -50, -10))\n\n## Add contours that respect the same extent\nxcontour(volcano, extent = c(0, 61, 0, 87), add = TRUE, levels = c(120, 140, 160))\nOnce we separate the data (a matrix of values) from the placement (extent as four numbers), you can handle any input type with the same logic. The orientation confusion disappears because {ximage} adopts the rasterImage convention—the one that matches how GDAL and every other geospatial tool returns data.\nThis is {vaster}’s philosophy applied to plotting: the spatial meaning comes from the six numbers (dimension + extent), not from the data structure."
  },
  {
    "objectID": "vaster-blog-draft.html#what-vaster-provides",
    "href": "vaster-blog-draft.html#what-vaster-provides",
    "title": "Raster logic without pixels",
    "section": "What vaster provides",
    "text": "What vaster provides\n{vaster} gives that underlying logic with no attachment to any data format or structure. All you need are dimension and extent (and extent is optional, a sensible default is [0,nx], [0,ny] rather than unit-square).\n\nlibrary(vaster)\n\ndm &lt;- c(40, 20)  # ncol, nrow\nex &lt;- c(100, 160, -50, -10)  # xmin, xmax, ymin, ymax\n\n# Cell centres - implicit from dimension and extent\nx_centre(dm, ex)\n\n [1] 100.75 102.25 103.75 105.25 106.75 108.25 109.75 111.25 112.75 114.25\n[11] 115.75 117.25 118.75 120.25 121.75 123.25 124.75 126.25 127.75 129.25\n[21] 130.75 132.25 133.75 135.25 136.75 138.25 139.75 141.25 142.75 144.25\n[31] 145.75 147.25 148.75 150.25 151.75 153.25 154.75 156.25 157.75 159.25\n\ny_centre(dm, ex)\n\n [1] -49 -47 -45 -43 -41 -39 -37 -35 -33 -31 -29 -27 -25 -23 -21 -19 -17 -15 -13\n[20] -11\n\n\nWhat cell contains a given point?\n\n# Some arbitrary coordinates\npts &lt;- cbind(x = c(120.5, 145.2, 110.8), \n             y = c(-25.3, -42.1, -15.7))\n\ncell_from_xy(dm, ex, pts)\n\n[1] 294 671  88\n\n\nWhat are the coordinates of cells 1, 100, and 800?\n\nxy_from_cell(dm, ex, c(1, 100, 800))\n\n       [,1] [,2]\n[1,] 100.75  -11\n[2,] 129.25  -15\n[3,] 159.25  -49\n\n\nConvert between row/column and cell index:\n\n# Cell to row, column\ncell &lt;- 42\nrow_from_cell(dm, cell)\n\n[1] 2\n\ncol_from_cell(dm, cell)\n\n[1] 2\n\n# Row, column to cell\ncell_from_row_col(dm, row = 3, col = 15)\n\n[1] 95\n\n\nNone of this requires loading imagery or touching files. It’s pure computation from first principles.\nNote here that indexing is via R’s convention: 1-based, both for cell and row and col."
  },
  {
    "objectID": "vaster-blog-draft.html#the-snap-problem",
    "href": "vaster-blog-draft.html#the-snap-problem",
    "title": "Raster logic without pixels",
    "section": "The snap problem",
    "text": "The snap problem\nOne of the most common operations in raster work is “snapping”—taking an arbitrary region and aligning it to an existing grid. You want to crop to an area of interest, but you need cell-aligned boundaries.\n\n# A reference grid: 3-degree cells covering the globe\nref_dm &lt;- c(120, 60)\nref_ex &lt;- c(-180, 180, -90, 90)\n\n# Some arbitrary region of interest (not aligned to anything)\nroi &lt;- c(112.3, 154.7, -44.2, -9.8)\n\n# Snap to the reference grid\nsnapped &lt;- vcrop(roi, ref_dm, extent = ref_ex)\nsnapped\n\n$extent\n[1] 111 156 -45  -9\n\n$dimension\n[1] 15 12\n\n\nThe result gives you the exact dimension and extent for the crop window, aligned to the reference grid. No data needed—just the numbers.\n\nplot_extent(ref_ex, border = \"grey\")\nabline(v = x_corner(ref_dm, ref_ex), col = \"grey90\")\nabline(h = y_corner(ref_dm, ref_ex), col = \"grey90\")\n\n# Original ROI (arbitrary)\nplot_extent(roi, border = \"red\", lwd = 2, add = TRUE)\n\n# Snapped ROI (grid-aligned)\nplot_extent(snapped$extent, border = \"blue\", lwd = 2, add = TRUE)\n\n\n\n\n\n\n\n\nThe red box is what we asked for; the blue box is what we get when we respect the grid alignment."
  },
  {
    "objectID": "vaster-blog-draft.html#the-geotransform-connection",
    "href": "vaster-blog-draft.html#the-geotransform-connection",
    "title": "Raster logic without pixels",
    "section": "The geotransform connection",
    "text": "The geotransform connection\nGDAL uses a six-element “geotransform” to encode the affine relationship between pixel coordinates and geographic coordinates. This is the workhorse of georeferenced raster data. {vaster} speaks this language:\n\n# From extent and dimension to geotransform\ngt &lt;- extent_dim_to_gt(ex, dm)\ngt\n\n xmin  xres yskew  ymax xskew  yres \n100.0   1.5   0.0 -10.0   0.0  -2.0 \n\n# And back again\ngt_dim_to_extent(gt, dm)\n\nxmin xmin ymax ymax \n 100  160  -50  -10 \n\n\nThis matters when you’re working directly with GDAL (via {gdalraster}, {vapour}, or Python’s osgeo.gdal) and need to set up or interpret raster metadata without constructing heavyweight objects."
  },
  {
    "objectID": "vaster-blog-draft.html#why-does-this-matter",
    "href": "vaster-blog-draft.html#why-does-this-matter",
    "title": "Raster logic without pixels",
    "section": "Why does this matter?",
    "text": "Why does this matter?\n\nLightweight tooling: Sometimes we just need to compute cell indices or snap extents. We shouldn’t need to load terra or GDAL for that.\nCross-package compatibility: The logic is the same whether you’re working with terra, stars, GDAL, or raw arrays. Having it in one place means consistent behaviour everywhere.\nTeaching and understanding: Separating the logic from the data makes it clearer what raster operations actually do. The magic isn’t in the file format or the object class—it’s in the relationship between dimension and extent."
  },
  {
    "objectID": "vaster-blog-draft.html#the-ecosystem-what-you-can-build-with-pure-logic",
    "href": "vaster-blog-draft.html#the-ecosystem-what-you-can-build-with-pure-logic",
    "title": "Raster logic without pixels",
    "section": "The ecosystem: what you can build with pure logic",
    "text": "The ecosystem: what you can build with pure logic\nOnce you separate grid logic from data, interesting things become possible. Here are three packages that build on this foundation, taken (ahem) shamelessly from the hypertidy suite. (I’m not ignoring other important packages in R and Python that use these ideas, but these are chosen to focus on the core idea of what vaster is and why it exists. )\n\n{fasterize}: fast polygon rasterization\n{fasterize} is a high-performance replacement for raster::rasterize(). It uses the classic scanline algorithm to burn polygons onto a grid. The original fasterize required an actual RasterLayer as a template—not for its data, but for its six numbers (dimension and extent).\n## fasterize needs a \"raster\", but really it just needs dimension + extent\nlibrary(fasterize)\nr &lt;- raster::raster(ncol = 1000, nrow = 800, \n                    xmn = 0, xmx = 100, ymn = 0, ymx = 80)\nresult &lt;- fasterize(polygons_sf, r, field = \"value\")\nThe raster object is just a vessel for grid metadata, but fasterize also will create an actual numeric matrix in memory - so it has these two limitations at input and output. With raster logic available separately, we can drive the same algorithm with nothing but numbers.\n\n\n{controlledburn}: don’t materialize, just rasterize\n{controlledburn} takes the fasterize algorithm and strips away the last step: instead of filling a raster with values, it returns the indices of which cells each polygon covers. The output is run-length encoded scanline segments—start, end, row, polygon_id.\nlibrary(controlledburn)\nlibrary(vaster)\n\n## Define a grid with just numbers\next &lt;- c(100, 160, -50, -10)\ndm &lt;- c(500, 400)\n\n## Get the cell coverage index, not pixel values\nidx &lt;- burn_polygon(polygons_sf, extent = ext, dimension = dm)\nFor a 500,000 × 400,000 grid, materializing a raster would require terabytes. But storing the polygon coverage as run-length scanline indices? Tens of megabytes for typical distributions that real world polygons embody. The grid logic is the same—only the output changes.\nThis is the “cell abstraction” always implicit in raster operations. We can use these indices for extraction, for streaming aggregation, for anything that doesn’t require all pixels to exist at once.\n\n\n{grout}: tiling without tiles\n{grout} applies the same principle to tiling. Given a grid’s dimension and extent, plus a block size, it computes the complete tiling scheme: how many tiles, where each one falls, how much overlap (“dangle”) occurs when dimensions don’t divide evenly.\nlibrary(grout)\n\n## A raster that's 87 × 61 with 8 × 8 tiles\nscheme &lt;- grout(c(87, 61), extent = c(0, 87, 0, 61), blocksize = c(8, 8))\nscheme\n#&gt; tiles: 11, 8 (x * y = 88)\n#&gt; block: 8, 8\n#&gt; dangle: 1, 3\n#&gt; tile extent: 0, 88, -3, 61 (xmin,xmax,ymin,ymax)\n\n## Get offset/size for each tile (for GDAL RasterIO)\ntile_index(scheme)\nNo data loaded. No files opened. Just the arithmetic of how a grid subdivides. This is exactly what you need to drive tiled reading from GDAL, or to generate web map tile pyramids, or to parallelize processing across spatial chunks.\n\n\nThe common thread\nAll three packages share a design principle: the grid specification is just six numbers, and most operations don’t need anything else.\n{fasterize} showed that fast rasterization doesn’t need heavy objects—just geometry and grid metadata. {controlledburn} showed you don’t even need to materialize pixels—indices are enough. {grout} showed that tiling is pure arithmetic on dimension and extent.\n{vaster} is the foundation that makes this all clean. Instead of each package reinventing cellFromXY() and friends, they can share a common vocabulary for grid logic."
  },
  {
    "objectID": "vaster-blog-draft.html#the-python-world-is-catching-up",
    "href": "vaster-blog-draft.html#the-python-world-is-catching-up",
    "title": "Raster logic without pixels",
    "section": "The Python world is catching up",
    "text": "The Python world is catching up\nThe Python geospatial ecosystem has grappled with the same issues that motivated {vaster}.\nA recent pangeo discussion compared odc.stac and stackstac—two packages for loading satellite imagery into xarray. Despite ostensibly doing the same thing, they produce different results. Why?\nPixel coordinates: edge or centre? When you say a pixel is at coordinate (100.0, 200.0), do you mean that’s its centre, or its top-left corner? odc.stac defaults to centre, stackstac defaults to edge. Both are valid conventions; GIS tools tend toward corners, while climate/ocean data tends toward centres. But if you don’t know which convention your tool uses, your data shifts by half a pixel.\nCoordinate snapping: When your requested bounding box doesn’t align perfectly with the source grid, what happens? Different tools make different choices about how to snap—and those choices are usually invisible, buried in the library internals.\nThese are exactly the questions {vaster} makes explicit. Dimension and extent. Corner or centre. Snap in or snap out. The logic is universal; only the defaults vary.\n\nxarray’s new RasterIndex\nThe xarray team recently announced flexible indexing, including a RasterIndex that computes coordinates on-the-fly from an affine transform rather than storing explicit coordinate arrays. Sound familiar?\nFrom the post:\n\nFor 2D raster images, this function often takes the form of an Affine Transform. The rasterix library extends Xarray with a RasterIndex which computes coordinates for geospatial images such as GeoTiffs via Affine Transform.\n\nThis is the same insight: coordinates are implicit in dimension and extent. You don’t need to materialize a 7-terabyte coordinate array when six numbers suffice. The xarray team is building infrastructure for what {vaster} has provided all along.\nThe Python ecosystem is large and well-funded, so these tools will eventually mature. But it’s worth noting that the conceptual foundation—grid logic as pure computation on dimension and extent—has been available in R for years, just waiting to be extracted from the packages that imprisoned it."
  },
  {
    "objectID": "vaster-blog-draft.html#the-r-matrix-connection",
    "href": "vaster-blog-draft.html#the-r-matrix-connection",
    "title": "Raster logic without pixels",
    "section": "The R matrix connection",
    "text": "The R matrix connection\nR’s matrices already have most of this logic, just without the spatial semantics. A matrix has dimension (nrow, ncol). When you use image() with explicit coordinates, you’re adding extent. The [i, j] indexing is cell-from-row-column. which(m &gt; threshold, arr.ind = TRUE) gives you row-column-from-cell.\n{vaster} makes this connection explicit. It treats dimension and extent as first-class inputs, just as R treats matrices as first-class objects. The spatial meaning is in the numbers, not the container."
  },
  {
    "objectID": "vaster-blog-draft.html#example-raster-logic-is-a-simplifying-principle",
    "href": "vaster-blog-draft.html#example-raster-logic-is-a-simplifying-principle",
    "title": "Raster logic without pixels",
    "section": "Example: raster logic is a simplifying principle",
    "text": "Example: raster logic is a simplifying principle\nEnough theory! Let’s make a map. The following string defines a huge world-coverage image server.\n\ndsn &lt;- \"WMTS:https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/WMTS/1.0.0/WMTSCapabilities.xml,layer=World_Imagery\"\n\nSo, obviously we need a huge stack of Python geospatial to make a map image … or maybe we can just use generic tools and do it in R.\nWe’re using {gdalraster} here because it’s the thinnest wrapper over the GDAL API, but the same calls work through {terra}, {vapour}, or Python’s osgeo.gdal. The point isn’t the package — it’s that the interface is just numbers and strings.\n\nlibrary(gdalraster)\n\nGDAL 3.12.0beta1 (released 2025-10-26), GEOS 3.12.1, PROJ 9.7.0\n\nds &lt;- new(GDALRaster, dsn)  ## same as terra::rast(dsn) or osgeo.gdal.Open(dsn) or rioxarray.open_rasterio(dsn)\nds\n\nC++ object of class GDALRaster\n Driver : OGC Web Map Tile Service (WMTS)\n DSN    : WMTS:https://services.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/WMTS/1.0.0/WMTSCapabilities.xml,layer=World_Imagery\n Dim    : 1073741766, 1070224430, 4\n CRS    : WGS 84 / Pseudo-Mercator (EPSG:3857)\n Res    : 0.037323, 0.037323\n Bbox   : -20037507.260427, -19971868.890929, 20037507.248165, 19971868.903191\n\n\nWe don’t want the whole world and certainly not that much detail, how about Mawson Station in Antarctica, Mawson is at approximately 62.87E and -67.6S.\nWe should define a local equal area projection, on Mawson.\n\nprj &lt;- \"+proj=laea +lon_0=62.8742 +lat_0=-67.6033\"\n\nWhat is the right extent or bbox for that projection? It’s just metres around the zero-point, let’s go with a range.\n\nb &lt;- 75000\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\") ## use GDAL to manage IO and cleanup, same in other tools\nchk &lt;- warp(dsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot_raster(read_ds(new(GDALRaster, tfile), bands = 1:3))\n\n\n\n\n\n\n\n\nWhat about a different data source, can we see any meaningful data in sea ice at this time of year?\n\nicedsn &lt;- \"/vsicurl/https://data.seaice.uni-bremen.de/amsr2/asi_daygrid_swath/s3125/2025/dec/Antarctic3125/asi-AMSR2-s3125-20251220-v5.4.tif\"\n\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(icedsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\ndat &lt;- read_ds(new(GDALRaster, tfile), bands = 1)\ndat[dat &gt; 100] &lt;- NA\nop &lt;- par(bg = \"darkgrey\")\nplot_raster(dat, legend = T)\n\n\n\n\n\n\n\npar(op)\n\nNot unexpectedly at 3.125km resolution it’s a bit sparse on information, but we didn’t have to use a different approach to get coincident data.\nWhat about the bathymetry? Even at this scale we can see the disparity between bathymetric and topographic detail.\n\nbathdsn &lt;- \"/vsicurl/https://projects.pawsey.org.au/idea-gebco-tif/GEBCO_2024.tif\"\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(bathdsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot_raster(read_ds(new(GDALRaster, tfile), bands = 1), col_map_fn = hcl.colors(128), legend = TRUE)\n\n\n\n\n\n\n\n\nLet’s zoom right in, and this time we’ll use terra so that adding contours is easy, read the REMA v2 2m topography and add it as contours.\n\nb &lt;- 3000\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(dsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 0))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nplot_raster(read_ds(new(GDALRaster, tfile), bands = 1:3))\n\nremadsn &lt;- \"/vsicurl/https://raw.githubusercontent.com/mdsumner/rema-ovr/main/REMA-2m_dem_ovr.vrt\"\nlibrary(terra)\n\nterra 1.8.80\n\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:gdalraster':\n\n    rasterize\n\n\nThe following object is masked from 'package:vaster':\n\n    origin\n\nr &lt;- rast(ext(-b, b, -b, b), crs = prj, ncols = 1024, nrows = 1024)\ntfile &lt;- tempfile(tmpdir = \"/vsimem\", fileext = \".vrt\")\nchk &lt;- warp(remadsn, tfile, t_srs = prj, cl_arg = c(\"-te\", -b, -b, b, b,  \"-ts\", 1024, 1024))\n\n0...10...20...30...40...50...60...70...80...90...100 - done.\n\nrema &lt;- read_ds(new(GDALRaster, tfile), bands = 1)\ncontour(setValues(r, rema), add = TRUE)\n\n\n\n\n\n\n\n\n\nsessionInfo()\n\nR version 4.5.1 (2025-06-13)\nPlatform: x86_64-pc-linux-gnu\nRunning under: Ubuntu 22.04.5 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=en_AU.UTF-8       LC_NUMERIC=C              \n [3] LC_TIME=en_AU.UTF-8        LC_COLLATE=en_AU.UTF-8    \n [5] LC_MONETARY=en_AU.UTF-8    LC_MESSAGES=en_AU.UTF-8   \n [7] LC_PAPER=en_AU.UTF-8       LC_NAME=C                 \n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \n[11] LC_MEASUREMENT=en_AU.UTF-8 LC_IDENTIFICATION=C       \n\ntime zone: Etc/UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n[1] terra_1.8-80          gdalraster_2.3.0.9007 vaster_0.5.0         \n\nloaded via a namespace (and not attached):\n [1] cli_3.6.5          knitr_1.50         rlang_1.1.6        xfun_0.53         \n [5] yyjsonr_0.1.21     jsonlite_2.0.0     glue_1.8.0         bit_4.6.0         \n [9] htmltools_0.5.8.1  scales_1.4.0       rmarkdown_2.29     rappdirs_0.3.3    \n[13] evaluate_1.0.5     fastmap_1.2.0      lifecycle_1.0.4    yaml_2.3.10       \n[17] compiler_4.5.1     nanoarrow_0.7.0-2  codetools_0.2-19   RColorBrewer_1.1-3\n[21] htmlwidgets_1.6.4  Rcpp_1.1.0         rstudioapi_0.15.0  farver_2.1.2      \n[25] digest_0.6.39      wk_0.9.4.9000      R6_2.6.1           dichromat_2.0-0.1 \n[29] parallel_4.5.1     tools_4.5.1        bit64_4.6.0-1      xml2_1.5.1"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "title": "Web services for scientific data in R",
    "section": "",
    "text": "NOTE: this post has been resurrected from a 2017 post (April 2022).\nThis post is in-progress …\nTwo very important packages in R are rerddap and plotdap providing straightforward access to and visualization of time-varying gridded data sets. Traditionally this is handled by individuals who either have or quickly gain expert knowledge about the minute details regarding obtaining data sources, exploring and extracting data from them, manipulating the data for the required purpose and reporting results. Often this task is not the primary goal, it’s simply a requirement for comparison to environmental data or validation of direct measurements or modelled scenarios against independent data.\nThis is a complex area, it touches on big data, web services, complex and sophisticated multi-dimensional scientific data in array formats (primarily NetCDF), map projections, and data aesthetics or scaling methods by which data values are converted into visualizations.\nR is not known to be strong in this area for handling large and complex array-based data, although it has had good support for every piece in the chain many of them were either not designed to work together or or languished without modernization for some time. There are many many approaches to bring it all together but unfortunately no concerted effort to sort it all out. What there is however is a very exciting and productive wave of experimentation and new packages to try, there’s a lot of exploration occurring and a lot of powerful new approaches.\nROpenSci is producing a variety of valuable new R packages for scientific exploration and analysis. It and the RConsortium are both contributing directly into this ecosystem, with the latter helping to foster developments in simple features (polygons, lines and points), interactive map editing and an integration of large raster data handling."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "title": "Web services for scientific data in R",
    "section": "Cool right!?",
    "text": "Cool right!?\nThis is extremely cool, there is a lot of exciting new support for these sometimes challenging sources of data. However, there is unfortunately no concerted vision for integration of multi-dimensional data into the tidyverse and many of the projects created for data getting and data extraction must include their own internal handlers for downloading and caching data sources and converting data into required forms. This is a complex area, but in some places it is harder and more complex than it really needs to be.\nTo put some guides in this discussion, the rest of this post is informed by the following themes.\n\nthe tidyverse is not just cool, it’s totally awesome (also provides a long-term foundation for the future)\n“good software design” facilitates powerful APIs and strong useability: composable, orthogonal components and effortless exploration and workflow development\nnew abstractions are still to be found\n\nThe tidyverse is loudly loved and hated. Critics complain that they already understood long-form data frames and that constant effusive praise on twitter is really annoying. Supporters just sit agog at a constant stream of pointless shiny fashion parading before their eyes … I mean, they fall into a pit of success and never climb out. Developers who choose the tidyverse as framework for their work appreciate its seamless integration of database principles and actual databases, the consistent and systematic syntax of composable single-purpose functions with predictable return types, the modularization and abstractability of magrittr piping, and the exciting and disruptive impact of tidy evaluation seen clearly already in dplyr and family, but which will clearly make it very easy to traverse the boundary between being a user and being a developer. What could be a better environment for the future of science and research?"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "title": "Web services for scientific data in R",
    "section": "What about the rasters!",
    "text": "What about the rasters!\nI’ve been using gridded data in R since 2002. I remember clearly learning about the use of the graphics::image function for visualizing 2D kernel density maps created using sm. I have a life-long shudder reflex at the heat.colors palette, and at KDE maps generally. I also remember my first encounter with the NetCDF format which would have looked exactly like this (after waiting half and hour to download this file).\n\nprint(sst.file)\n\n[1] \"ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc\"\n\nlibrary(RNetCDF)\ncon &lt;- open.nc(file.path(dp, sst.file))\nlon &lt;- var.get.nc(con, \"lon\")\nlat &lt;- var.get.nc(con, \"lat\")\nxlim &lt;- c(140, 155)\nylim &lt;- c(-50, -35)\nxsub &lt;- lon &gt;= xlim[1] & lon &lt;= xlim[2]\nysub &lt;- lat &gt;= ylim[1] & lat &lt;= ylim[2]\ntlim  &lt;- \"oh just give up, it's too painful ...\"\ntime &lt;- var.get.nc(con, \"time\")\n## you get the idea, who can be bothered indexing time as well these days\nv &lt;- var.get.nc(con, \"sst\", start = c(which(xsub)[1], length(ysub) - max(which(ysub)), length(time)), count = c(sum(xsub), sum(ysub), 1))\nimage(lon[xsub], rev(lat[ysub]), v[nrow(v):1, ], asp = 1/cos(42 * pi/180))\n\n\n\n\n\n\n\n\nWhat a hassle! Let’s just use raster. (These aren’t the same but I really don’t care about making sure the old way works, the new way is much better - when it works, which is mostly …).\n\nlibrary(raster)\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nb &lt;- brick(file.path(dp, sst.file))\n\nLoading required namespace: ncdf4\n\nnlayers(b)\n\n[1] 1685\n\nraster(b)\n\nclass      : RasterLayer \ndimensions : 180, 360, 64800  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n\n## what time did you want?\n\nplot(crop(subset(b, nlayers(b) - c(1, 0)), extent(xlim, ylim), snap = \"out\"), col = heat.colors(12))\n\n\n\n\n\n\n\n\nThese are pretty cruddy data anyway, 1 degree resolution, weekly time steps? Come on man!\nWhy is this data set relevant? For a very long time the Optimally Interpolated Sea Surface Temperature data set, known fondly as Reynolds SST in some circles, was a very important touchstone for those working in marine animal tracking. From the earliest days of tuna tracking by (PDF): Northwest Pacific by light-level geo-locators, a regional or global data set of surface ocean temperatures was a critical comparison for tag-measured water temperatures. The strong and primarily zonal-gradients (i.e. varying by latitude, it gets cold as you move towards the poles) in the oceans provided an informative corrective to “light level geo-location” latitude estimates, especially when plagued by total zonal ambiguity (see Figure 12.3) around the equinoxes.\nToday we can use much finer resoution blended products for the entire globe. Blended means it’s a combination of measured (remote-sensing, bucket off a ship) and modelled observations, that’s been interpolated to “fill gaps”. This is not a simple topic of course, remotely sensed temperatures must consider whether it is day or night, how windy it is, the presence of sea ice, and many other factors - but as a global science community we have developed to the point of delivering a single agreed data set for this property. And now that it’s 2017, you have the chance of downloading all 5000 or so daily files, the total is only 2000 Gb.\nSo nothing’s free right? You want high-resolution, you get a big download bill."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "title": "Web services for scientific data in R",
    "section": "Web services for scientific array data",
    "text": "Web services for scientific array data\nAh, no - we don’t have to download every large data set. That’s where ERDDAP comes in!\nThis makes it easy, but I’m still not happy. In this code a raw NetCDF file is downloaded but is not readily useable by other processes, it’s not obvious how to connect directly to the source with NetCDF API, the raster data itself is turned into both a data frame, and turned into a grid ignoring irregularity in the coordinates, the raster is then resized and possibly reprojected, then turned into a polygon layer (eek) and finally delivered to the user as a very simple high level function that accepts standard grammar expressions of the tidyverse.\nWhat follows is some raw but real examples of using an in-development package tidync in the hypertidy family. It’s very much work-in-progress, as is this blog post …\nPlease reach out to discuss any of this if you are interested!\n\n#install.packages(\"rerddap\")\n#devtools::install_github(\"ropensci/plotdap\")\nlibrary(rerddap)\nlibrary(plotdap)\nlibrary(ggplot2)\nsstInfo &lt;- info('jplMURSST41')\n#system.time({  ## 26 seconds\nmurSST &lt;- griddap(sstInfo, latitude = c(22., 51.), longitude = c(-140., -105),\n                  time = c('last','last'), fields = 'analysed_sst')\n\ninfo() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap\n\n#})\nf &lt;- attr(murSST, \"path\")\n#unlink(f)\n\n## the murSST (it's a GHRSST L4  foundational SST product ) is an extremely detailed raster source, it's really the only\n## daily blended (remote sensing + model) and interpolated (no-missing values)\n## Sea Surface Temperature for global general usage that is high resolution.\n## The other daily blended product Optimally Interpolated (OISST) is only 0.25 degree resolution\n## The GHRSST product is available since 2002, whereas OISST is available since\n## 1981 (the start of the AVHRR sensor era)\nmaxpixels &lt;- 50000\ndres &lt;- c(mean(diff(sort(unique(murSST$data$lon)))), mean(diff(sort(unique(murSST$data$lat)))))\nlibrary(raster)\nr &lt;- raster(extent(range(murSST$data$lon) + c(-1, 1) * dres[1]/2, range(murSST$data$lat) + c(-1, 1) * dres[2]/2),\n     res = dres, crs = \"+init=epsg:4326\")\n\ndim(r) &lt;- dim(r)[1:2] %/% sqrt(ceiling(ncell(r) / maxpixels))\n\ndat &lt;- murSST$data %&gt;%\n mutate(bigcell = cellFromXY(r, cbind(lon, lat))) %&gt;%\n    group_by(time, bigcell) %&gt;%\n  summarize(analysed_sst = mean(analysed_sst, na.rm = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(lon = xFromCell(r, bigcell), lat = yFromCell(r, bigcell))\n\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n\nr[] &lt;- NA\nr[dat$bigcell] &lt;- dat$analysed_sst\nnames(r) &lt;- \"analysed_sst\"\ndat$bigcell &lt;- NA\n\n#m &lt;- sf::st_as_sf(maps::map(\"world\", region = \"USA\"))\nbgMap &lt;- sf::st_as_sf( maps::map('world', plot = FALSE, fill = TRUE))\n\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_raster(data = dat, aes(x = lon, y = lat, fill = analysed_sst))\n\n\n\n\n\n\n\n## now, what happened before?\n\n#system.time({p &lt;- sf::st_as_sf(raster::rasterToPolygons(r))})\n## should be a bit faster due to use of implicit coordinate mesh\nsystem.time({p &lt;- sf::st_as_sf(spex::polygonize(r, na.rm = TRUE))})\n\n   user  system elapsed \n  0.800   0.004   0.804 \n\n## plot(p, border = NA)\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_sf(data = p, aes(fill = analysed_sst), colour = \"transparent\")\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\n\n\nu &lt;- \"http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41\"\n\nlibrary(tidync)\nlibrary(dplyr)\ntnc &lt;- tidync::tidync(u)\n\nnot a file: \n' http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41 '\n\n... attempting remote connection\n\n\nConnection succeeded.\n\ntnc  ## notice there are four variables in this active space\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) &lt;dimension family&gt; : &lt;associated variables&gt; \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name    length     min    max start count    dmin   dmax unlim coord_dim \n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;     \n1 D0    latitu…  17999 -9.00e1 9.00e1     1 17999 -9.00e1 9.00e1 FALSE TRUE      \n2 D1    longit…  36000 -1.80e2 1.8 e2     1 36000 -1.80e2 1.8 e2 FALSE TRUE      \n3 D2    time      7266  1.02e9 1.65e9     1  7266  1.02e9 1.65e9 FALSE TRUE      \n\nhf &lt;- tnc %&gt;% hyper_filter(longitude = longitude &gt;= -140 & longitude &lt;= -105, latitude = latitude &gt;= 22 & latitude &lt;= 51,\n                       time = index == max(index))\nhf\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) &lt;dimension family&gt; : &lt;associated variables&gt; \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name   length     min    max start count    dmin    dmax unlim coord_dim \n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;     \n1 D0    latit…  17999 -9.00e1 9.00e1 11200  2901  2.2 e1  5.1 e1 FALSE TRUE      \n2 D1    longi…  36000 -1.80e2 1.8 e2  4000  3501 -1.4 e2 -1.05e2 FALSE TRUE      \n3 D2    time     7266  1.02e9 1.65e9  7266     1  1.65e9  1.65e9 FALSE TRUE      \n\n## looking ok, so let's go for gold!\n## specify just sst, otherwise we will get all four\n## hyper_tibble gets the raw arrays with ncvar_get(conn, start = , count = ) calls\n## then expands out the axes based on the values from the filtered axis tables\nsystem.time({\n tab &lt;- hf %&gt;% hyper_tibble(select_var = \"analysed_sst\")\n})\n\n   user  system elapsed \n  3.452   2.750  23.779 \n\n# system.time({  ## 210 seconds\n#   hs &lt;- hyper_slice(hf, select_var = \"analysed_sst\")\n# })\n# hyper_index(hf)\n# nc &lt;- ncdf4::nc_open(u)\n# system.time({  ## 144 seconds\n#   l &lt;- ncdf4::ncvar_get(nc, \"analysed_sst\", start = c(4000, 2901, 5531), count = c(3501, 2901, 1))\n# })"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html",
    "href": "posts/2015-12-28_gis3d/index.html",
    "title": "GIS for 3D in R",
    "section": "",
    "text": "GIS data structures are not well suited for generalization, and visualizations and models in 3D require pretty forceful and ad hoc approaches.\nHere I describe a simple example, showing several ways of visualizing a simple polygon data set. I use the programming environment R for the data manipulation and the creation of this document via several extensions (packages) to base R."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "href": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "title": "GIS for 3D in R",
    "section": "Polygon “layer”",
    "text": "Polygon “layer”\nThe R package maptools contains an in-built data set called wrld_simpl, which is a basic (and out of date) set of polygons describing the land masses of the world by country. This code loads the data set and plots it with a basic grey-scale scheme for individual countries.\n\nlibrary(maptools)\ndata(wrld_simpl)\nprint(wrld_simpl)\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 246 \nextent      : -180, 180, -90, 83.57027  (xmin, xmax, ymin, ymax)\nvariables   : 11\n# A tibble: 246 × 11\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows\n\nplot(wrld_simpl, col = grey(sample(seq(0, 1, length = nrow(wrld_simpl)))))\n\n\n\n\n\n\n\n\nWe also include a print statement to get a description of the data set, this is a SpatialPolygonsDataFrame which is basically a table of attributes with one row for each country, linked to a recursive data structure holding sets of arrays of coordinates for each individual piece of these complex polygons.\nThese structures are quite complicated, involving nested lists of matrices with X-Y coordinates. I can use class coercion from polygons, to lines, then to points as the most straightforward way of obtaining every XY coordinate by dropping the recursive hierarchy structure to get at every single vertex in one matrix.\n\nallcoords &lt;- coordinates(as(as(wrld_simpl, \"SpatialLines\"), \"SpatialPoints\"))\ndim(allcoords)\n\n[1] 26264     2\n\nhead(allcoords)  ## print top few rows\n\n     coords.x1 coords.x2\n[1,] -61.68667  17.02444\n[2,] -61.88722  17.10527\n[3,] -61.79445  17.16333\n[4,] -61.68667  17.02444\n[5,] -61.72917  17.60861\n[6,] -61.85306  17.58305\n\n\n(There are other methods to obtain all coordinates while retaining information about the country objects and their component “pieces”, but I’m ignoring that for now.)\nWe need to put these “X/Y” coordinates in 3D so I simply add another column filled with zeroes.\n\nallcoords &lt;- cbind(allcoords, 0)\nhead(allcoords)\n\n     coords.x1 coords.x2  \n[1,] -61.68667  17.02444 0\n[2,] -61.88722  17.10527 0\n[3,] -61.79445  17.16333 0\n[4,] -61.68667  17.02444 0\n[5,] -61.72917  17.60861 0\n[6,] -61.85306  17.58305 0\n\n\n(Note for non-R users: in R expressions that don’t include assignment to an object with &lt;- are generally just a side-effect, here the side effect of the head(allcoords) here is to print the top few rows of allcoords, just for illustration, there’s no other consequence of this code)."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "href": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "title": "GIS for 3D in R",
    "section": "OpenGL in R",
    "text": "OpenGL in R\nIn R we have access to 3D visualizations in OpenGL via the rgl package, but the model for data representation is very different so I first plot the vertices of the wrld_simpl layer as points only.\n\nlibrary(rgl)\nplot3d(allcoords, xlab = \"\", ylab = \"\") ## smart enough to treat 3-columns as X,Y,Z\nrglwidget()\n\n\n\n\n\nPlotting in the plane is one thing, but more striking is to convert the vertices from planar longitude-latitude to Cartesizan XYZ. Define an R function to take “longitude-latitude-height” and return spherical coordinates (we can leave WGS84 for another day).\n\nllh2xyz &lt;- \nfunction (lonlatheight, rad = 6378137, exag = 1) \n{\n    cosLat = cos(lonlatheight[, 2] * pi/180)\n    sinLat = sin(lonlatheight[, 2] * pi/180)\n    cosLon = cos(lonlatheight[, 1] * pi/180)\n    sinLon = sin(lonlatheight[, 1] * pi/180)\n    rad &lt;- (exag * lonlatheight[, 3] + rad)\n    x = rad * cosLat * cosLon\n    y = rad * cosLat * sinLon\n    z = rad * sinLat\n    cbind(x, y, z)\n}\n\n## deploy our custom function on the longitude-latitude values\nxyzcoords &lt;- llh2xyz(allcoords)\n\nNow we can visualize these XYZ coordinates in a more natural setting, and even add a blue sphere for visual effect.\n\nplot3d(xyzcoords, xlab = \"\", ylab = \"\")\nspheres3d(0, 0, 0, radius = 6370000, col = \"lightblue\")\nrglwidget()\n\n\n\n\n\nThis is still not very exciting, since our plot knows nothing about the connectivity between vertices."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "href": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "title": "GIS for 3D in R",
    "section": "Organization of polygons",
    "text": "Organization of polygons\nThe in-development R package gris provides a way to represent spatial objects as a set of relational tables. I’m leaving out the details because it’s not the point I want to make, but in short a gris object has tables “o” (objects), “b” (for branches), “bXv” (links between branches and vertices) and “v” the vertices.\nIf we ingest the wrld_simpl layer we get a list with several tables.\nEDITOR NOTE (April 2022): see anglr function DEL0() for an updated way to create what gris was doing in 2015.\n\nlibrary(gris)  ## devtools::install_github(\"r-gris/gris\")\nlibrary(dplyr)\ngobject &lt;- gris(wrld_simpl)\n\nThe objects, these are individual countries with several attributes including the NAME.\n\ngobject$o\n\n# A tibble: 246 × 12\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows, and 1 more variable: object_ &lt;int&gt;\n\n\nThe branches, these are individual simple, one-piece “ring polygons”. Every object may have one or more branches (branches may be an “island” or a “hole” but this is not currently recorded). Note how branch 1 and 2 (branch_) both belong to object 1, but branch 3 is the only piece of object 2.\n\ngobject$b\n\n# A tibble: 3,768 × 3\n   object_ branch_ island_\n     &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;  \n 1       1       1 TRUE   \n 2       1       2 TRUE   \n 3       2       3 TRUE   \n 4       3       4 TRUE   \n 5       3       5 TRUE   \n 6       3       6 TRUE   \n 7       3       7 TRUE   \n 8       3       8 TRUE   \n 9       4       9 TRUE   \n10       5      10 TRUE   \n# … with 3,758 more rows\n\nplot(gobject[1, ], col = \"#333333\")\ntitle(gobject$o$NAME[1])\n\n\n\n\n\n\n\nplot(gobject[2, ], col = \"#909090\")\ntitle(gobject$o$NAME[2])\n\n\n\n\n\n\n\n\n(Antigua and Barbuda sadly don’t get a particularly good picture here, but this is not the point of the story.)\nThe links between branches and vertices.\n\ngobject$bXv\n\n# A tibble: 26,264 × 3\n   branch_ order_ vertex_\n     &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n 1       1      1    5589\n 2       1      2    5620\n 3       1      3    5605\n 4       1      4    5589\n 5       2      1    5596\n 6       2      2    5611\n 7       2      3    5613\n 8       2      4    5596\n 9       3      1   16101\n10       3      2   17581\n# … with 26,254 more rows\n\n\nThis table is required so that we can normalize the vertices by removing any duplicates based on X/Y pairs. This is required for the triangulation engine used below, although not by the visualization strictly. (Note that we could also normalize branches for objects, since multiple objects might use the same branch - but again off-topic).\nFinally, the vertices themselves. Here we only have X and Y, but these table structures can hold any number of attributes and of many types.\n\ngobject$v\n\n# A tibble: 21,165 × 3\n       x_    y_ vertex_\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 -61.7   17.0    5589\n 2 -61.9   17.1    5620\n 3 -61.8   17.2    5605\n 4 -61.7   17.6    5596\n 5 -61.9   17.6    5611\n 6 -61.9   17.7    5613\n 7   2.96  36.8   16101\n 8   4.79  36.9   17581\n 9   5.33  36.6   18103\n10   6.40  37.1   18790\n# … with 21,155 more rows\n\n\nThe normalization is only relevant for particular choices of vertices, so if we had X/Y/Z in use there might be a different version of “unique”. I think this is a key point for flexibility, some of these tasks must be done on-demand and some ahead of time.\nIndices here are numeric, but there’s actually no reason that they couldn’t be character or other identifier. Under the hood the dplyr package is in use for doing straightforward (and fast!) table manipulations including joins between tables and filtering on values."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "href": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "title": "GIS for 3D in R",
    "section": "More 3D already!",
    "text": "More 3D already!\nWhy go to all this effort just for a few polygons? The structure of the gris objects gives us much more flexibility, so I can for example store the XYZ Cartesian coordinates right on the same data set. I don’t need to recursively visit nested objects, it’s just a straightforward calculation and update - although we’re only making a simple point, this could be generalized a lot more for user code.\n\ngobject$v$zlonlat &lt;- 0\ndo_xyz &lt;- function(table) {\n  xyz &lt;- llh2xyz(dplyr::select(table, x_, y_, zlonlat))\n  table$X &lt;- xyz[,1]\n  table$Y &lt;- xyz[,2]\n  table$Z &lt;- xyz[,3]\n  table\n}\n\ngobject$v &lt;- do_xyz(gobject$v)\n\ngobject$v\n\n# A tibble: 21,165 × 7\n       x_    y_ vertex_ zlonlat        X         Y        Z\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 -61.7   17.0    5589       0 2892546. -5369047. 1867388.\n 2 -61.9   17.1    5620       0 2872491. -5376810. 1875991.\n 3 -61.8   17.2    5605       0 2880293. -5370474. 1882167.\n 4 -61.7   17.6    5596       0 2879394. -5354145. 1929470.\n 5 -61.9   17.6    5611       0 2868217. -5361116. 1926758.\n 6 -61.9   17.7    5613       0 2864423. -5358522. 1939577.\n 7   2.96  36.8   16101       0 5100196.   264042. 3820852.\n 8   4.79  36.9   17581       0 5083067.   425571. 3829093.\n 9   5.33  36.6   18103       0 5095693.   475230. 3806402.\n10   6.40  37.1   18790       0 5056321.   567008. 3846135.\n# … with 21,155 more rows\n\n\nI now have XYZ coordinates for my data set, and so for example I will extract out a few nearby countries and plot them.\n\nlocalarea &lt;- gobject[gobject$o$NAME %in% c(\"Australia\", \"New Zealand\"), ]\n## plot in traditional 2d\nplot(localarea, col = c(\"dodgerblue\", \"firebrick\"))\n\n\n\n\n\n\n\n\nThe plot is a bit crazy since parts of NZ that are over the 180 meridian skews everything, and we could fix that easily by modifiying the vertex values for longitude, but it’s more sensible in 3D.\n\nrgl::plot3d(as.matrix(localarea$v[c(\"X\", \"Y\", \"Z\")]), xlab = \"\", ylab = \"\")\nrglwidget()\n\n\n\n\n\nFinally, to get to the entire point of this discussion let’s triangulate the polygons and make a nice plot of the world.\nThe R package RTriangle wraps Jonathan Shewchuk’s Triangle library, allowing constrained Delaunay triangulations. To run this we need to make a Planar Straight Line Graph from the polygons, but this is fairly straightforward by tracing through paired vertices in the data set. The key parts of the PSLG are the vertices P and the segment indexes S defining paired vertices for each line segment. This is a “structural” index where the index values are bound to the actual size and shape of the vertices, as opposed to a more general but perhaps less efficient relational index.\n\npslgraph &lt;- gris:::mkpslg(gobject)\ndim(pslgraph$P)\n\n[1] 21165     2\n\nrange(pslgraph$S)\n\n[1]     1 21165\n\nhead(pslgraph$P)\n\n            x_       y_\n[1,] -61.68667 17.02444\n[2,] -61.88722 17.10527\n[3,] -61.79445 17.16333\n[4,] -61.72917 17.60861\n[5,] -61.85306 17.58305\n[6,] -61.87306 17.70389\n\nhead(pslgraph$S)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    3\n[3,]    3    1\n[4,]    1    1\n[5,]    4    5\n[6,]    5    6\n\n\nThe PSLG is what we need for the triangulation.\n\ntri &lt;- RTriangle::triangulate(pslgraph)\n\nThe triangulation vertices (long-lat) can be converted to XYZ, and plotted.\n\nxyz &lt;- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   5 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\naspect3d(\"iso\"); bg3d(\"grey12\")\nrglwidget()\n\n\n\n\n\nThese are very ugly polygons since there’s no internal vertices to carry the curvature of this sphere. This is the same problem we’d face if we tried to drape these polygons over topography: at some point we need internal structure.\nLuckily Triangle can set a minimum triangle size. We set a constant minimum area, which means no individual triangle can be larger in area than so many “square degrees”. This gives a lot more internal structure so the polygons are more elegantly draped around the surface of the sphere. (There’s not really enough internal structure added with this minimum area, but I’ve kept it simpler to make the size of this document more manageable).\n\ntri &lt;- RTriangle::triangulate(pslgraph, a = 9)  ## a (area) is in degrees, same as our vertices\nxyz &lt;- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   6 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\nbg3d(\"gray12\")\nrglwidget()\n\n\n\n\n\nWe still can’t identify individual polygons as we smashed that information after putting the polygon boundary segments through the triangulator. With more careful work we could build a set of tables to store particular triangles between our vertices and objects, but to finish this story I just loop over each object adding them to the scene.\n\n## loop over objects\ncols &lt;- sample(grey(seq(0, 1, length = nrow(gobject$o))))\nopen3d()\n\nnull \n   7 \n\nfor (iobj in seq(nrow(gobject$o))) {\n  pslgraph &lt;- gris:::mkpslg(gobject[iobj, ])\n  tri &lt;- RTriangle::triangulate(pslgraph, a = 9)  ## a is in units of degrees, same as our vertices\n  xyz &lt;- llh2xyz(cbind(tri$P, 0))\n  triangles3d(xyz[t(tri$T), ], col = cols[iobj], specular = \"black\")\n}\nbg3d(\"gray12\")\nrglwidget()"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "href": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "title": "GIS for 3D in R",
    "section": "Real world topography, Image textures",
    "text": "Real world topography, Image textures\nfuture work …\nSee anglr package"
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html",
    "href": "posts/2017-09-01_gdal-in-r/index.html",
    "title": "GDAL in R",
    "section": "",
    "text": "For some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\nI already knew how immensely powerful R was, with its epic flexibity and useability and I could just sense there was a brighter way once I understood many more details. As my experience grew I was able to do amazing tasks like, merge a few shapefiles together into one, or plot a window of data from a georeferenced grid. Previously the best I’d done in this space was VBScript in Manifold GIS, which I could use to automate some data tasks - but the prospects of full automation from raw data files to output, end-to-end with a software tool that anyone could use was absolutely mind-blowing. I was super-powered, I remember earning a carton of beer from a colleague of my father, for munging some SHP or TAB files between AGD66 and GDA94 … or something, and I knew I had a bright future ahead.\nSo what’s the abstraction? GDAL does not care what format the data is in, it could be points, lines, areas, a raster DEM, a time series of remote sensing, or an actual image. It just doesn’t mind, there’s an interpretation in its model for what’s in the file (or data base, or web server) and it will deliver that interpretation to you, very efficiently. If you understand that intepretation you can do a whole lot of amazing stuff. When this works well it’s because the tasks are modular, you have a series of basic tools designed to work together, and it’s up to you as a developer or a user to chain together the pieces to solve your particular problem."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "href": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "title": "GDAL in R",
    "section": "Where does this get difficult?",
    "text": "Where does this get difficult?\nGDAL is a C++ library, and that’s not accessible to most users or developers. The other key user-interface is the set of command line utilities, these are called gdal_translate, gdalinfo, ogr2ogr, ogrinfo, and many others. The command line is also not that accessible to many users, though it’s more so than C++ - this is why command line is a key topic for Software Carpentry. These interfaces give very high fidelity to the native interpretation provided by the GDAL model.\nGDAL is used from many languages, there’s Python, R, Perl, C#, Fortran, and it is bundled into many, many softwares - a very long list. The original author wrote code for some of the most influential geo-spatial software the world has, and some of that is in GDAL, some is locked up forever in propietary forms. He saw this as a problem and very early on engineered the work to be able to be open, in the do anything with me, including privatize me-license called MIT. Have a look in the source code for gdalwarp, you’ll see the company who was the best at raster reprojection in the late 1990s and early 2000s.\nPython is surely the closest other language to the native interpretation, but then it’s not that simple, and this is not that story …\nR has a very particular interpretation of the GDAL interpretation, it’s called rgdal and if you are familiar with the GDAL model and with R you can see a very clear extra layer there. This extra level is there partly because of when it was done, the goals of the authors, the community response to the amazingly powerful facilities it provided, but also and perhaps mainly because it was very hard. R’s rather peculiar API meant that in the early 2000s the authors had to write in another language, a language between the native GDAL C++ and the R user language - this is the R API, it’s full of SEXP and UNPROTECTs and if you search this issue you’ll see clear signals not to bother - now you should just get on with learning Rcpp.\nThese extra levels are there in the R API, the hard stuff down in the rgdal/src/ folder but also in the R code itself. There’s a bunch of rigorous rules applied there, things to help protect us from that lower level, and to save from making serious analytical mistakes. All of this was very hard work and very well-intended, but it’s clear that it takes us away from the magic of the GDAL abstraction, we have a contract with rgdal, the R code has a contract with the R API, and the rgdal/src has a contract with GDAL. All of these things divorce R users and developers from the original schemes that GDAL provides, because at the R level rgdal itself has to provide certain guarantees and contracts with both R and with R users. I think that is too much for one package.\nAdd to this the complex zoo of formats, the other libraries that GDAL requires for full use. The Windows rgdal on CRAN doesn’t include HDF4, or HDF5, or NetCDF, or DODS - there are many missing things in this list, and it’s not clear if it’s because it’s hard, it’s against the license (ECW might be tricky, MrSID most definitely would be), or because no one has asked or maybe no one knows how, or maybe CRAN doesn’t want to. (Would you know how to find out?) All of these things add up to being way too much for one package. It’s kind of impossible, though now there are many more eyes on the problem and progress is being made. Who should decide these things? How would anyone know it’s even an issue?\nI wonder if many of us see rgdal as the definition of the GDAL abstraction. I see pretty clearly the difference, and while the package has been extremely useful for me I’ve long wanted lower level control and access to the GDAL core itself. (I had rather influential guidance from extremely expert programmers I’ve worked with, and I’ve discussed GDAL with many others, including employees of various companies, and across various projects and with many users. I assume most R users don’t know much about the details, and why would they want to?).\nThere is active work to modernize rgdal, and you should be aware of the immensely successful sf and the soon to be stars project. sf is an R interpretation of the Simple Features Standard (GDAL has an interpretation of that standard too, sf starts there when it reads with GDAL). stars will start with GDAL as a model for gridded data, and it’s not yet fleshed out what the details of that will be. None of these interpretations are permanent, though while the simple features standard is unlikely to change, there is no doubt that GDAL will evolve and include more features that don’t involve that standard. These things do change and very few people, relatively, are engaged in the decisions that are made. (GDAL could definitely benefit from more input, also something I’ve long wanted to do more of).\nIt’s been a long time, but I’ve recently found a way over the key obstacle I had - building a package to compile for use in R with bindings to GDAL itself. I have a lot to thank Roger Bivand and Edzer Pebesma for many years of instruction and guidance in that, in many different ways. I also am extremely grateful to R-Core for the overall environment, and the tireless work done by the CRAN maintainers. I have to mention Jeroen Ooms and Mark Padgham who’ve been extremely helpful very recently. This is something I’ve wanted to be able to do for a really long time, I hope this post helps provide context to why, and I hope it encourages some more interest in the general topic."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "href": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "title": "GDAL in R",
    "section": "vapour",
    "text": "vapour\nMy response to the interpretation layers is vapour, this is my version of a very minimal interpretation of the core GDAL facilities. There’s a function to read the attribute data from geometric features, a function to read the geometry data as raw binary, or various text formats, a function to read only the bounding box of each feature, and there’s a function to read the raw pixel values from a local window within a gridded data set.\nNone of this is new, we have R packages that do these things and the vapour functions will have bugs, and will need maintenance, and maybe no one but me will ever use them. I’ve needed them and I’ve started to learn a whole lot more about what I’m going to do next with them. I recommend that any R user with an interest in geo-spatial or GDAL facilities have a closer look at how they work - and if you know there’s a lower level below the interpretations provide in R you should explore them. Rcpp and the modern tools for R really do make this immensely more easy than in the past (RStudio has intellisense for C++ …).\nI also believe strongly that R is well-placed to write the future of multi-dimensional and hierarchical and complex structured and geo-spatial data. Do you know what that future should look like?"
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html",
    "title": "GDAL raster read/write by blocks",
    "section": "",
    "text": "A block is another word for a tile, a tile in a small-ish raster window within a larger raster. Tiles can be very clever, such as 256x256, they make a nice way to organize large data- keeping pieces of data that are nearby spatial nearby to each other in memory. There is a pair of functions internal to vapour that will 1) read data from a raster block 2) write data to a raster block. At the moment only Float64 data is handled."
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html#read-and-write-raster-by-blocks.",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html#read-and-write-raster-by-blocks.",
    "title": "GDAL raster read/write by blocks",
    "section": "",
    "text": "A block is another word for a tile, a tile in a small-ish raster window within a larger raster. Tiles can be very clever, such as 256x256, they make a nice way to organize large data- keeping pieces of data that are nearby spatial nearby to each other in memory. There is a pair of functions internal to vapour that will 1) read data from a raster block 2) write data to a raster block. At the moment only Float64 data is handled."
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html#tiles",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html#tiles",
    "title": "GDAL raster read/write by blocks",
    "section": "Tiles",
    "text": "Tiles\nTiles are a way of organizing rasters, a tile may be 256x256, 512x512 (typically powers of 2 for sensible reasons), or they may be of higher dimensions 256x256x8 - this is really a private detail for a storage format, such as a file. The data values stored in a tile are a nother matter, these are like variables - we might have 1 variable (say elevation) stored in a double floating point value, or we might have 3 variables of byte values for storing an RGB image. These are independent concepts to the size of the tile, and strictly there may be no variables at all, just the abstract idea of the tiling.\nAnother kind of block, or tile is a the scanline of a raster. Imagine reading each row of the raster, from the top row to the bottom. Each line is its own kind of degenerate tile. We can read from a raster this way no matter what its internal tiling is, just software might be opening several tiles and reading just one line from each. So, best if we match our “tile attack” to the native structure of the data.\nHere we find a large, tiled raster, obtain its internal tiling, and use that to update a copy of the same file.\nPlease note how we copy the source file, then write to that. There’s probably other software better suited to doing this atm, this post simply aims to air the topic a little in an R context.\nHere we use only temporary files that we download, and copy as needed. In practice, you should set this up to work across different physical disks, and for better workflow we need the ability to open an empty file to write to (WIP).\n\nraster_url &lt;- \"ftp://ftp.data.pgc.umn.edu/elev/dem/setsm/REMA/mosaic/v1.1/200m/REMA_200m_dem.tif\"\n\nreadfrom_file &lt;- tempfile(pattern = \"readfrom\", fileext = \".tif\")\nwriteto_file &lt;- tempfile(pattern = \"writeto\", fileext = \".tif\")\n## file size is 1.3Gb\ncurl::curl_download(raster_url, readfrom_file)\n\n## this now makes a copy of the 1.3Gb file, so we have two of them\nfs::file_copy(readfrom_file, writeto_file)\n\nNow we want the tiling\n\ninfo &lt;- vapour::vapour_raster_info(readfrom_file)\ntiling &lt;- list(dimension = info$dimXY, tiles = info$tilesXY)\nfac &lt;- 100\nfac * tiling$tiles\nif (tiling$tiles[2] == 1) {\n  ## let's take fac scanlines at a time\n  tiling$tiles[2] &lt;- fac\n}\ncalc_steps &lt;- function(dimension, tiles) {\n  bounds_x &lt;- seq(0, dimension, by = tiles)\n  steps_x &lt;-  rep(tiles, length.out = length(bounds_x)-1)\n  dangle_x &lt;- sum(steps_x) -dimension\n  if (dangle_x &gt; 0) steps_x[length(steps_x)] &lt;- steps_x[length(steps_x)] - dangle_x\n  \n  list(head(bounds_x, -1), steps_x)\n}\nx_step &lt;- calc_steps(tiling$dimension[1], tiling$tiles[1])\ny_step &lt;- calc_steps(tiling$dimension[2], tiling$tiles[2])\ny_step\nsystem.time({\nfor (i in seq_along(x_step[[1]])) {\n  startx &lt;- x_step[[1]][i]\n  countx &lt;- x_step[[2]][i]\n  for (j in seq_along(y_step[[1]])) {\n  starty &lt;- y_step[[1]][j]\n  county &lt;- y_step[[2]][j]\n    \n  ## now read\n  offset &lt;- c(startx, starty)\n  dimension &lt;- c(countx, county)\n  vals &lt;- vapour:::vapour_read_raster_block(readfrom_file, offset = offset, dimension = dimension, band_output_type = info$datatype, band = 1)[[1L]]\n  \n  ## do something to the values\n  if (any(na.omit(vals) &gt; info$nodata_value)) {\n    vals &lt;- vals * -1\n    vapour:::vapour_write_raster_block(writeto_file, vals, offset, dimension, band = 1, overwrite = TRUE)\n  }\n  }\n}"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m &lt;- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] &lt;- 0\nm[2, 1] &lt;- -10\nm[3, 2] &lt;- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#matrix-and-image",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#matrix-and-image",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m &lt;- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] &lt;- 0\nm[2, 1] &lt;- -10\nm[3, 2] &lt;- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "title": "R matrices and image",
    "section": "Data placement with image()",
    "text": "Data placement with image()\nThis is fairly obvious, each cell is painted as a discrete block with cell centres evenly spaced between 0 and 1.\n\nm &lt;- matrix(1:12, 3)\nimage(m)\n\n\n\n\n\n\n\n\nWe didn’t give it any coordinates to position the image, so it made some up.\n\nimage(m, main = \"input coordinates are cell centres\")\nxx &lt;- seq.int(0, 1, length.out = nrow(m))\nyy &lt;- seq.int(0, 1, length.out = ncol(m))\nabline(h = yy, v = xx, lty = 2)\n\n\n\n\n\n\n\n\nThis lends itself to a convenient data structure.\n\ndat &lt;- list(x = xx, y = yy, z = m)\nimage(dat)\ntext(expand.grid(xx, yy), lab = as.vector(m))\n\n\n\n\n\n\n\n\n\n## points(expand.grid(xx, yy))\n\nThe function image() has some hidden tricks.\n\nxcorner &lt;- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner &lt;- seq.int(0, 1, length.out = ncol(m) + 1L)\nprint(xcorner)\n\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\nprint(ycorner)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\n## [1] 0.00 0.25 0.50 0.75 1.00\n\nimage(xcorner, ycorner, m, main = \"input coordinates are cell corners\")\nabline(h = ycorner, v = xcorner)\n\n\n\n\n\n\n\n\nWe can even use non-regular coordinates.\n\nycorner &lt;- 1.5^seq_along(ycorner)\nimage(xcorner, ycorner, m)\nabline(h = ycorner, v = xcorner)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "title": "R matrices and image",
    "section": "Under the hood",
    "text": "Under the hood\n\nprint(image.default)\n\nfunction (x = seq(0, 1, length.out = nrow(z)), y = seq(0, 1, \n    length.out = ncol(z)), z, zlim = range(z[is.finite(z)]), \n    xlim = range(x), ylim = range(y), col = hcl.colors(12, \"YlOrRd\", \n        rev = TRUE), add = FALSE, xaxs = \"i\", yaxs = \"i\", xlab, \n    ylab, breaks, oldstyle = FALSE, useRaster, ...) \n{\n    if (missing(z)) {\n        if (!missing(x)) {\n            if (is.list(x)) {\n                z &lt;- x$z\n                y &lt;- x$y\n                x &lt;- x$x\n            }\n            else {\n                if (is.null(dim(x))) \n                  stop(\"argument must be matrix-like\")\n                z &lt;- x\n                x &lt;- seq.int(0, 1, length.out = nrow(z))\n            }\n            if (missing(xlab)) \n                xlab &lt;- \"\"\n            if (missing(ylab)) \n                ylab &lt;- \"\"\n        }\n        else stop(\"no 'z' matrix specified\")\n    }\n    else if (is.list(x)) {\n        xn &lt;- deparse1(substitute(x))\n        if (missing(xlab)) \n            xlab &lt;- paste0(xn, \"$x\")\n        if (missing(ylab)) \n            ylab &lt;- paste0(xn, \"$y\")\n        y &lt;- x$y\n        x &lt;- x$x\n    }\n    else {\n        if (missing(xlab)) \n            xlab &lt;- if (missing(x)) \n                \"\"\n            else deparse1(substitute(x))\n        if (missing(ylab)) \n            ylab &lt;- if (missing(y)) \n                \"\"\n            else deparse1(substitute(y))\n    }\n    if (any(!is.finite(x)) || any(!is.finite(y))) \n        stop(\"'x' and 'y' values must be finite and non-missing\")\n    if (any(diff(x) &lt;= 0) || any(diff(y) &lt;= 0)) \n        stop(\"increasing 'x' and 'y' values expected\")\n    if (!is.matrix(z)) \n        stop(\"'z' must be a matrix\")\n    if (!typeof(z) %in% c(\"logical\", \"integer\", \"double\")) \n        stop(\"'z' must be numeric or logical\")\n    if (length(x) &gt; 1 && length(x) == nrow(z)) {\n        dx &lt;- 0.5 * diff(x)\n        x &lt;- c(x[1L] - dx[1L], x[-length(x)] + dx, x[length(x)] + \n            dx[length(x) - 1])\n    }\n    if (length(y) &gt; 1 && length(y) == ncol(z)) {\n        dy &lt;- 0.5 * diff(y)\n        y &lt;- c(y[1L] - dy[1L], y[-length(y)] + dy, y[length(y)] + \n            dy[length(y) - 1L])\n    }\n    if (missing(breaks)) {\n        nc &lt;- length(col)\n        if (!missing(zlim) && (any(!is.finite(zlim)) || diff(zlim) &lt; \n            0)) \n            stop(\"invalid z limits\")\n        if (diff(zlim) == 0) \n            zlim &lt;- if (zlim[1L] == 0) \n                c(-1, 1)\n            else zlim[1L] + c(-0.4, 0.4) * abs(zlim[1L])\n        z &lt;- (z - zlim[1L])/diff(zlim)\n        zi &lt;- if (oldstyle) \n            floor((nc - 1) * z + 0.5)\n        else floor((nc - 1e-05) * z + 1e-07)\n        zi[zi &lt; 0 | zi &gt;= nc] &lt;- NA\n    }\n    else {\n        if (length(breaks) != length(col) + 1) \n            stop(\"must have one more break than colour\")\n        if (any(!is.finite(breaks))) \n            stop(\"'breaks' must all be finite\")\n        if (is.unsorted(breaks)) {\n            warning(\"unsorted 'breaks' will be sorted before use\")\n            breaks &lt;- sort(breaks)\n        }\n        zi &lt;- .bincode(z, breaks, TRUE, TRUE) - 1L\n    }\n    if (!add) \n        plot(xlim, ylim, xlim = xlim, ylim = ylim, type = \"n\", \n            xaxs = xaxs, yaxs = yaxs, xlab = xlab, ylab = ylab, \n            ...)\n    if (length(x) &lt;= 1) \n        x &lt;- par(\"usr\")[1L:2]\n    if (length(y) &lt;= 1) \n        y &lt;- par(\"usr\")[3:4]\n    if (length(x) != nrow(z) + 1 || length(y) != ncol(z) + 1) \n        stop(\"dimensions of z are not length(x)(-1) times length(y)(-1)\")\n    check_irregular &lt;- function(x, y) {\n        dx &lt;- diff(x)\n        dy &lt;- diff(y)\n        (length(dx) && !isTRUE(all.equal(dx, rep(dx[1], length(dx))))) || \n            (length(dy) && !isTRUE(all.equal(dy, rep(dy[1], length(dy)))))\n    }\n    if (missing(useRaster)) {\n        useRaster &lt;- getOption(\"preferRaster\", FALSE)\n        if (useRaster && check_irregular(x, y)) \n            useRaster &lt;- FALSE\n        if (useRaster) {\n            useRaster &lt;- FALSE\n            ras &lt;- dev.capabilities(\"rasterImage\")$rasterImage\n            if (identical(ras, \"yes\")) \n                useRaster &lt;- TRUE\n            if (identical(ras, \"non-missing\")) \n                useRaster &lt;- all(!is.na(zi))\n        }\n    }\n    if (useRaster) {\n        if (check_irregular(x, y)) \n            stop(gettextf(\"%s can only be used with a regular grid\", \n                sQuote(\"useRaster = TRUE\")), domain = NA)\n        if (!is.character(col)) {\n            col &lt;- as.integer(col)\n            if (any(!is.na(col) & col &lt; 0L)) \n                stop(\"integer colors must be non-negative\")\n            col[col &lt; 1L] &lt;- NA_integer_\n            p &lt;- palette()\n            col &lt;- p[((col - 1L)%%length(p)) + 1L]\n        }\n        zc &lt;- col[zi + 1L]\n        dim(zc) &lt;- dim(z)\n        zc &lt;- t(zc)[ncol(zc):1L, , drop = FALSE]\n        rasterImage(as.raster(zc), min(x), min(y), max(x), max(y), \n            interpolate = FALSE)\n    }\n    else .External.graphics(C_image, x, y, zi, col)\n    invisible()\n}\n&lt;bytecode: 0x55b5b27527a8&gt;\n&lt;environment: namespace:graphics&gt;\n\n\nThis is like looping with rect()\n\nop &lt;- par(mfrow = c(1, 2))\n## life is hard\ncols &lt;- topo.colors(25)\nscale &lt;- round((m - min(m))/diff(range(m)) * (length(cols) - 1) + 1)\nplot(NA, type = \"n\", xlim = range(xcorner), ylim = range(ycorner), asp = 1)\nfor (i in seq_along(xcorner[-1L])) {\n    for (j in seq_along(ycorner[-1L])) {\n        rect(xleft = xcorner[i], ybottom = ycorner[j], xright = xcorner[i + \n            1L], ytop = ycorner[j + 1L], col = cols[scale[i, j]], angle = 45 * \n            (i + j)%%2, density = 20, lwd = 2)\n    }\n    \n}\n\n## life is good\nimage(list(x = xcorner, y = ycorner, z = m), col = topo.colors(25), asp = 1)\n\n\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "title": "R matrices and image",
    "section": "“Raster graphics” (not the raster package)",
    "text": "“Raster graphics” (not the raster package)\nRelatively recently native image-graphics support was added to R.\nOld style\n\nm &lt;- matrix(1:12, nrow = 3)\nxcorner &lt;- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner &lt;- seq.int(0, 1, length.out = ncol(m) + 1L)\nimage(xcorner, ycorner, m, col = topo.colors(25))"
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "",
    "text": "There are a lot of problems when it comes to array representation in data formats and software. There is a whole family of complex issues with sometimes subtle or even cryptic causes, and here we focus on one particular detail."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#degenerate-rectilinear-coordinates",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#degenerate-rectilinear-coordinates",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "Degenerate Rectilinear Coordinates",
    "text": "Degenerate Rectilinear Coordinates\nI have taken to saying this phrase to describe a particular situation in array handling. I get confused responses, or normally just no response or feedback on the term itself. I think it’s extremely important though. Python communities raced up to where R was a few years back and just sped on by without any nod to what R folks had learned, I think that’s a huge failure and I take it personally.\nHere’s what it is. Let’s say we have a pretty low resolution grid of the world, we break up the entire range of longitude and latitude into intervals, 360 for meridians and 180 for parallels.\n(This is neat because in whole number terms, this is exactly the right number).\nSo, define a grid, we make it a farily typical 1-degree-per-pixel grid of the entire globe flattened unceremoniously from an\nangular coordinate system “longitude and latitude” to one where we just plot those numbers in x,y on a flat plane.\n\n## define a grid in -180,180 -90,90 (360x180)\ndm &lt;- c(360, 180)\nxlim &lt;- c(-180, 180)\nylim &lt;- c(-90, 90)\n\n## create a grid\ndat &lt;- matrix(c(0, 1, -1, 0), 2L, 2L)[rep(1:2, each = 180), rep(1:2, each = 90)]\n## we don't get much from this because 0,1 0,1\n## and the world isn't 0,360 0,180 anyway (that would have been handy!)\n#image(dat)\n\n\nxs &lt;- seq(xlim[1], xlim[2], length.out = dm[1] + 1)\nys &lt;- seq(ylim[1], ylim[2], length.out = dm[2] + 1)\nimage(xs, ys, dat, asp = 1)\nmaps::map(add = TRUE)\n\n\n\n\n\n\n\n\nWhat happened there? Well, dat is a matrix, a 2D array in R. It has 360 rows and 180 columns (but yes, we are treating the world map as if it had 360 columns, 180 rows - but that is not this blogpost). Just don’t question it right now, but be assured we are treating the world as having 360 unique whole number longitudes, and 180 unique whole number latitudes.\nWe can image() that, by which I mean draw a pixel map on the screen of all the values in the matrix as if they were a field of little rectangles.\n\nimage(dat)\n\n\n\n\n\n\n\n\nBut, that’s very boring because we don’t have any idea where anything is in 0,1 0,1 space.\nWhat if we use xlim and ylim, these are plot() arguments in R (and exist in image() too).\n\nimage(dat, xlim = xlim, ylim = ylim)\n\n\n\n\n\n\n\n\nOk that was a trick, our matrix is drawn as a tiny dot right at 0,0, because xlim/ylim is about the plot not the data.\nNo more tricks, image() has 3 arguments x,y,z for normal usage.\n\nimage(x = xs, y = ys, z = dat, asp = 1)\n\n\n\n\n\n\n\n\nWhy does that work? We essentially have an x coordinate and a y coordinate for every pixel edge. We have one for the left side of the leftmost pixel column, and the second one is the right side of the left most pixel, and so on. We used dm[1] + 1 and dm[2] + 1 up there exactly because we wanted xlim and ylim to be the very extremes of the data.\nCool, explained - a matrix has a coordinate array for each side, we plot it like a map with those. Done."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#we-are-so-so-so-not-done.",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#we-are-so-so-so-not-done.",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "We are so so so not done.",
    "text": "We are so so so not done.\nI’m not here to explain how image() works in R, but that’s kind of necessary because I want people who don’t use R to also read this."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-is-rectilinear",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-is-rectilinear",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "What is Rectilinear?",
    "text": "What is Rectilinear?\nRectilinear means varying in one dimension. When applied to axis coordinates it just means you have a position along the axis for each step. These can vary in how big each step is, and that is exactly when we say it’s rectilinear, because otherwise it’s regular.\nOur ys above are a regular list of coordinates, each one is exactly the same distance apart (distance in the frame we are using, forget about the actual Earth).\n\nop &lt;- par(mfrow = c(2, 1))\nplot(ys)\nrange(diff(ys))\n\n[1] 1 1\n\nplot(diff(ys))\n\n\n\n\n\n\n\n\nSee? They just plod along with exactly the same delta between each, in this case the step size is 1 but it could be anything."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-do-you-mean-degenerate",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-do-you-mean-degenerate",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "What do you mean Degenerate?",
    "text": "What do you mean Degenerate?\nIsn’t this a bad word, pejorative? Well no, it means something very specific.\n\nIt means even though there is a quite a lot of data, the information is confused or confounded. The required information could be expressed very compactly, but it’s actually obscured by the details."
  },
  {
    "objectID": "posts/2024-12-04_plot_native/index.html",
    "href": "posts/2024-12-04_plot_native/index.html",
    "title": "Plot at native resolution, with R",
    "section": "",
    "text": "Plot native, something I should have done long ago.\n\n#' Plot raster at native resolution\n#'\n#' Determines the current device size and plots the raster centred on its own\n#' middle to plot at native resolution. \n#'\n#' @param x as SpatRaster\n#' @param ... passed to terra::plot\n#'\n#' @return the input raster, cropped corresponding to the plot made\n#' @export\n#'\n#' @examples\n#' plot_native(terra::rast(volcano))\n#' plot_native(terra::disagg(terra::rast(volcano), 64))\nplot_native &lt;- function(x, ...) {\n  ex &lt;- as.vector(terra::ext(x))\n  at &lt;- NULL\n  ## take the centre\n  if (is.null(at)) {\n    at &lt;- apply(matrix(ex, 2), 2, mean)\n  }\n  dv &lt;- dev.size(\"px\")\n  scl &lt;- terra::res(x)\n  halfx &lt;- dv[1]/2 * scl[1]\n  halfy &lt;- dv[2]/2 * scl[2]\n  cropex &lt;- c(at[1] - halfx, at[1] + halfx, at[2] - halfy, at[2] + halfy)\n  x &lt;- terra::crop(x, terra::ext(cropex), extend = TRUE)\n  add &lt;- FALSE\n  if (terra::nlyr(x) &gt;= 3) terra::plotRGB(x, add = add) else plot(x, ..., add = add)\n  x\n}\n\nSo as an example read this world imagery.\n\ndsn &lt;- \"&lt;GDAL_WMS&gt;&lt;Service name=\\\"TMS\\\"&gt;&lt;ServerUrl&gt;http://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/${z}/${y}/${x}&lt;/ServerUrl&gt;&lt;/Service&gt;&lt;DataWindow&gt;&lt;UpperLeftX&gt;-20037508.34&lt;/UpperLeftX&gt;&lt;UpperLeftY&gt;20037508.34&lt;/UpperLeftY&gt;&lt;LowerRightX&gt;20037508.34&lt;/LowerRightX&gt;&lt;LowerRightY&gt;-20037508.34&lt;/LowerRightY&gt;&lt;TileLevel&gt;17&lt;/TileLevel&gt;&lt;TileCountX&gt;1&lt;/TileCountX&gt;&lt;TileCountY&gt;1&lt;/TileCountY&gt;&lt;YOrigin&gt;top&lt;/YOrigin&gt;&lt;/DataWindow&gt;&lt;Projection&gt;EPSG:900913&lt;/Projection&gt;&lt;BlockSizeX&gt;256&lt;/BlockSizeX&gt;&lt;BlockSizeY&gt;256&lt;/BlockSizeY&gt;&lt;BandsCount&gt;3&lt;/BandsCount&gt;&lt;MaxConnections&gt;10&lt;/MaxConnections&gt;&lt;Cache /&gt;&lt;ZeroBlockHttpCodes&gt;204,404,403&lt;/ZeroBlockHttpCodes&gt;&lt;/GDAL_WMS&gt;\"\n\nlibrary(terra)\n\nterra 1.7.83\n\nim &lt;- project(rast(dsn), rast(ext(-1, 1, -1, 1) * 1e6, res = 1000, crs = \"+proj=laea +lon_0=147 +lat_0=-42\"), by_util = TRUE)\nplotRGB(im)\n\n\n\n\n\n\n\n\nDo we have more, or less resolution than our device can handle? This next plot shows that we have more, and the cropped raster is returned to match the device.\n\nprint(dim(im))\n\n[1] 2000 2000    3\n\nprint(dev.size(\"px\"))\n\n[1] 1344  960\n\nplot_native(im)\n\n\n\n\n\n\n\n\nclass       : SpatRaster \ndimensions  : 960, 1344, 3  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : -672000, 672000, -480000, 480000  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=laea +lat_0=-42 +lon_0=147 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : GDAL_WMS&gt;_1, GDAL_WMS&gt;_2, GDAL_WMS&gt;_3 \nmin values  :           0,           0,           0 \nmax values  :         255,         237,         213"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#section",
    "href": "posts/2024-12-05_idea_update/idea-update.html#section",
    "title": "IDEA - data and software",
    "section": "",
    "text": "raadtools, software to extract maps of ocean properties and values at points-in-time"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#raadtools-10-years-old-r-package",
    "href": "posts/2024-12-05_idea_update/idea-update.html#raadtools-10-years-old-r-package",
    "title": "IDEA - data and software",
    "section": "raadtools > 10 years old R package",
    "text": "raadtools &gt; 10 years old R package\n\n\n\n\n\n\n\nR function\nPurpose\n\n\n\n\nreadsst()\nglobal sea surface temperature\n\n\nreadice()\npolar sea ice concentrations\n\n\nreadghrsst()\nhigh resolution sea surface temperature\n\n\nread_adt/ugos/vgos_daily()\nglobal altimetry, sea height, surface currents\n\n\nread_chla_daily()\nglobal ocean colour\n\n\nreadtopo()\nglobal or local bathymetry\n\n\n…\n\n\n\n\nother functions we don’t have, yet …"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#software-we-want",
    "href": "posts/2024-12-05_idea_update/idea-update.html#software-we-want",
    "title": "IDEA - data and software",
    "section": "software we want",
    "text": "software we want\n\n\n\nfeatures we want\nraadtools 🤔\n&lt;new tool&gt;\n\n\n\n\nusers don’t download files\n✅\n✅\n\n\ndata is up to date\n✅\n✅\n\n\nwe can add new data\n✅\n✅\n\n\nyou can add new data\n❌\n✅\n\n\nuse outside AAD without Mike or Ben\n❌\n✅\n\n\nuse outside of R\n❌\n✅\n\n\nscale up on super computing\n❌\n✅\n\n\nrobust to research/local outage\n❌\n✅\n\n\navailable offline on Nuyina\n❌\n??"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#python-support",
    "href": "posts/2024-12-05_idea_update/idea-update.html#python-support",
    "title": "IDEA - data and software",
    "section": "Python support",
    "text": "Python support\nAddress entire data cubes, with one line of code e.g. daily data 1993 to November 2024\nimport xarray; &lt;some settings&gt;\n\nds = xarray.open_dataset('s3://vzarr/SEALEVEL_GLO_PHY_L4.parquet', &lt;more settings&gt;)\n&lt;xarray.Dataset&gt; Size: 574GB\nDimensions:    (time: 11538, latitude: 720, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * longitude  (longitude) float32 6kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time       (time) datetime64[ns] 92kB 1993-01-01 1993-01-02 ... 2024-11-25\nData variables:\n    adt        (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    sla        (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    ugos       (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    ugosa      (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    vgos       (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    vgosa      (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\nAttributes: (12/44) ..."
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#how-are-we-doing-this",
    "href": "posts/2024-12-05_idea_update/idea-update.html#how-are-we-doing-this",
    "title": "IDEA - data and software",
    "section": "How are we doing this",
    "text": "How are we doing this\n\nThe old and new tools reflect user-demand, tell us your ideas!\nModern tech: cloud-native and efficient public-available files\nData curation and cataloguing tools: {bowerbird}, STAC, VirtualiZarr\nExploring best-practice usage in Python xarray, odc, and in R terra, gdalraster, rsi\nContributing to software libraries GDAL.org, and community with AADC, SCAR, rOpenSci, Pangeo, Radiant Earth, Opendatacube, Digital Earth Antarctica"
  }
]