[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a quarto website for the hypertidy family of R packages for multi-dimensional and spatial data.\nHypertidy is an approach to spatial or multi-dimensional data in R based on the following principles:\nExamples of these principles are seen in these R packages.\nvapour\nraadtools and angstroms\ntidync, lazyraster,\nsilicate, anglr, rbgm"
  },
  {
    "objectID": "about.html#gridded-data",
    "href": "about.html#gridded-data",
    "title": "About",
    "section": "Gridded data",
    "text": "Gridded data\nHypertidy recognizes that not all gridded data fit into the GIS raster conventions. Gridded data comes in many forms, geographic with longitude-latitude or projected spaces, with time and or depth dimensions, with different orderings of axes (i.e. time-first, the latitude-longitude), and with generally any arbitrary space. A space is simply a set of axes with particular units and projection, and yes we mean “space” and “projection” in the more general mathematical sense. Date-time data is a projection, there is a mapping of a particular set of values to the real line and the position on that line for particular instant is defined by the axis units and epoch.\nMesh and grid share the same meaning in some contexts."
  },
  {
    "objectID": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "href": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "title": "About",
    "section": "Structured vs. unstructured, topology vs. geometry",
    "text": "Structured vs. unstructured, topology vs. geometry\nArray-based data have a straight-forward relationship between a set of axes that have discrete steps. These are “structured grids”. Unstructured grids include triangulations, non-regularly binned histograms, tetrahedral meshes and ragged arrays.\nAn unstructured mesh (grid) is able to represent any data structure, but structured meshes have some advantages because of the regular indexing relationship between dimensions.\nGIS vector constructs “polygons”, “lines”, “points” are special case optimizations of the unstructured grid case. Polygons really are topologically identical to lines, and they are a dead-end in the broader scheme of dimensionality. Points and lines can are topologicaly 0-dimensional and 1-dimensional respectively, and this shape-constraint is the same no matter what geometric dimension they are defined in. A line can twist around a 4D space with x, y, z, t coordinates at its segment nodes or it can be constrained to single dimension with only one of those coordinates specifying its position. The topology of the line is completely independent of the geometry, if we treat the line as composed of topological primitives.\nPolygons are not composed of topological primitives, but they can be treated as being composed of line primitives."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "",
    "text": "PLEASE NOTE (April 2022): this post has been migrated from an old site, and some details may have changed. There might an update to this post to reflect the rgl package as it is now. —\nThis post describes the mesh3d format used in the rgl package and particularly how colour properties are stored and used. There are recent changes to this behaviour (see ‘meshColor’), and previously the situation was not clearly documented."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "rgl",
    "text": "rgl\nThe rgl package has long provided interactive 3D graphics for R. The neat thing for me about 3D graphics is the requirement for mesh forms of data, and the fact that meshes are extremely useful for very many tasks. When we plot data in 3D we necessarily have to convert the usual spatial types into mesh forms. You can see me discuss that in more detail in this talk."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "The mesh3d format",
    "text": "The mesh3d format\nHere is an example of a mesh3d object, it stores two polygonal areas in a form ready for 3D graphics.\n\nmesh0 &lt;- structure(list(vb = structure(c(0, 0, 0, 1, 0, 1, 0, 1, 0.75, \n                                1, 0, 1, 1, 0.8, 0, 1, 0.5, 0.7, 0, 1, 0.8, 0.6, 0, 1, 0.69, \n                                0, 0, 1, 0.2, 0.2, 0, 1, 0.5, 0.2, 0, 1, 0.5, 0.4, 0, 1, 0.3, \n                                0.6, 0, 1, 0.2, 0.4, 0, 1, 1.1, 0.63, 0, 1, 1.23, 0.3, 0, 1), .Dim = c(4L, 14L)), \n               it = structure(c(1L, 8L, 12L, 9L, 8L, 1L, 7L, 6L, 5L, \n                                5L, 4L, 3L, 2L, 1L, 12L, 9L, 1L, 7L, 5L, 3L, 2L, 2L, 12L, 11L, \n                                10L, 9L, 7L, 5L, 2L, 11L, 10L, 7L, 5L, 5L, 11L, 10L, 6L, 7L, \n                                14L, 14L, 13L, 6L), .Dim = c(3L, 14L)), \n               primitivetype = \"triangle\", \n               material = list(), \n               normals = NULL, \n               texcoords = NULL), \n               class = c(\"mesh3d\", \"shape3d\"))\n\n\nstr(mesh0)\n\nList of 6\n $ vb           : num [1:4, 1:14] 0 0 0 1 0 1 0 1 0.75 1 ...\n $ it           : int [1:3, 1:14] 1 8 12 9 8 1 7 6 5 5 ...\n $ primitivetype: chr \"triangle\"\n $ material     : list()\n $ normals      : NULL\n $ texcoords    : NULL\n - attr(*, \"class\")= chr [1:2] \"mesh3d\" \"shape3d\"\n\n\n(It’s not obvious about the polygons, please bear with me).\nThe following characterizes the structure.\n\ntwo matrix arrays vb and it\nvb has 4 rows and 14 columns, and contains floating point numbers\nit has 3 rows and 14 columns, and contains integers (starting at 1)\na primitivetype which is “triangle”\nan empty list of material propertes (this is the missing link for the polygons)\na NULL value for normals and texcoords, these won’t be discussed further (but see ?quadmesh::quadmesh for texture coordinates from spatial)\na class, this object is a mesh3d and inherits from shape3d\n\nThe vb array is the vertices, these are the corner coordinates of the elements of the mesh.\n\nplot(t(mesh0$vb), main = \"t(vb) - vertices\", xlab = \"X\", ylab = \"Y\")\n\n\n\n\n\n\n\n\nThe elements of this mesh are triangles, and these are specified by the index array it. Elements of a mesh are called primitives, hence the primitivetype here.\n\nplot(t(mesh0$vb), main = \"t(vb[, it]) - primitives\", xlab = \"X\", ylab = \"Y\")\npolygon(t(mesh0$vb[, rbind(mesh0$it, NA)]), col = rgb(0.6, 0.6, 0.6, 0.5))"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Transpose",
    "text": "Transpose\nThese matrix arrays are transpose the way we usually use them in R, for now just remember that you must t()ranspose them for normal plotting, e.g. plot(t(mesh0$vb[1:2, ])) will give the expected scatter plot of the vertices. The reason these arrays are transpose is because each coordinate value is then contiguous in memory, each Y value is right next to its counterpart X, and Z (and W), and vb[it, ] provides a flat vector of XYZW values in a continuous block - this is a very important efficiency, and help explains why computer graphics use elements in a mesh form like this."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Colours",
    "text": "Colours\nUnsurprisingly, if we set the material property to a constant we get a constant colour.\n\nwidgetfun &lt;- function() {\n  view3d(0, phi = 8)\n  rglwidget()\n}\nmesh0$material$color &lt;- \"red\"\nlibrary(rgl)\nclear3d()\nshade3d(mesh0, lit = FALSE); \nwidgetfun()\n\n\n\n\n\nIn the usual R way our singleton colour value is magically recycled across every part of the shape, and it’s all red. But, is it recycled by vertices or by primitive? Until recently it was only possible to tell by trying (or reading the source code).\nHere I think it’s easy to see that the two colours are specified at the vertices, and they bleed across each triangle accordingly. We also get a warning that the behaviour has recently changed.\n\nclear3d()\nmesh0$material$color &lt;- c(\"firebrick\", \"black\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0)\nwidgetfun()\n\n\n\n\n\nThe default is to meshColor = \"vertices\", so let’s specify faces.\n\nclear3d()\nmesh0$material$color &lt;- c(\"firebrick\", \"dodgerblue\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0, meshColor = \"faces\")\nwidgetfun()\n\n\n\n\n\nSometimes we get neighbouring triangles with the same colour, so let’s also add the edges.\n\nmesh0$vb[3, ] &lt;- 0.01  ## vertical bias avoids z-fighting\n## material properties here override the recycling of internal colours\n## onto edges\nwire3d(mesh0, lwd = 5, color = \"black\")\nwidgetfun()\n\n\n\n\n\nIf we go a bit further we can see the original arrangement for this shape, two individual polygons that share a single edge.\nThis only works because I happen to know how this was created, and I know how this control of behaviour occurs in new rgl.\nThere are 12 triangles in the first polygon, and 2 in the second. (The original polygons can be seen here (left panel)).\n\nclear3d()\nmesh0$material$color &lt;- rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2))\nshade3d(mesh0, meshColor = \"faces\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nIf we treat the colours as applying to each vertex, then we needed to propagate it to each vertex around each face (triangle), and this is what rgl now calls legacy behaviour.\n\nclear3d()\nmesh0$material$color &lt;- rep(rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2)), each = 3)\nshade3d(mesh0, meshColor = \"legacy\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nWe cannot recreate this effect with meshColor = \"vertices\", because each of our vertices is actually unique. (It could be done by making the vb array every repeated vertex, and updating the index array but I can’t summon this up atm).\n\nclear3d()\nmesh0$material$color &lt;- rep_len(c(\"firebrick\", \"dodgerblue\"), length.out = ncol(mesh0$vb))\nshade3d(mesh0, meshColor = \"vertices\", lit = FALSE)\nwidgetfun()"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Primitives",
    "text": "Primitives\nThe other kind of element supported by mesh3d is a quad, specified by an ib array with 4 rows (ib versus it, 4 vertices versus 3) and the primitivetype = \"quad\".\nThe it values are an index into, i.e. the column number of the vertex array. The vertices, or coordinates, are stored by column in this structure, whereas normally we would store a coordinate per row.\nWhen I first explored mesh3d I was looking at a quad type mesh - and I was completely confused. Both vb and ib had four rows, and so while I understood that a quad must have 4 vertices (4 index values for every primitive), I did not understand why the vertices also had four rows.\n(There are other kinds of primitives in common use are edge, point, tetrahedron - but rgl has no formal class for these - in practice the edge type is referred to as segment in rgl, and tetrahedra are approximated by enclosing their shape with triangles)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Why does the vertex array have 4 rows?",
    "text": "Why does the vertex array have 4 rows?\nAll mesh3d objects have a vb array, and it always includes 4 rows.\nThe reason there are 4 rows in the vertex array is that these are homogeneous coordinates which …\n\nare ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied\n\n… yeah. For our purposes just think\n\nX, Y, Z in the usual sense and set W = 1.\n\n(Do not set W = 0 because your data will vanish to infinity when plotted with rgl, which is what those math folks are saying more or less)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "QUADS",
    "text": "QUADS\nNow let’s get a quad type mesh from the real world.\n\n## remotes::install_github(\"hypertidy/ceramic\")\nlibrary(ceramic)\ntopo &lt;- cc_elevation(raster::extent(-72, -69, -34, -32), zoom = 6)\n\nPreparing to download: 1 tiles at zoom = 6 from \nhttps://api.mapbox.com/v4/mapbox.terrain-rgb/\n\nqm &lt;- quadmesh::quadmesh(topo)\n\nstr(qm)\n\nList of 8\n $ vb             : num [1:4, 1:60225] -8015493 -3761925 0 1 -8014270 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"x\" \"y\" \"z\" \"1\"\n  .. ..$ : NULL\n $ ib             : int [1:4, 1:59732] 1 2 277 276 2 3 278 277 3 4 ...\n $ primitivetype  : chr \"quad\"\n $ material       : list()\n $ normals        : NULL\n $ texcoords      : NULL\n $ raster_metadata:List of 7\n  ..$ xmn  : num -8015493\n  ..$ xmx  : num -7680393\n  ..$ ymn  : num -4028537\n  ..$ ymx  : num -3761925\n  ..$ ncols: int 274\n  ..$ nrows: int 218\n  ..$ crs  : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n $ crs            : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n - attr(*, \"class\")= chr [1:3] \"quadmesh\" \"mesh3d\" \"shape3d\"\n\n\nThis topographic raster from near Santiago is now a mesh3d subclassed to quadmesh. This adds two properties raster_metadata and crs, which under limited conditions allows reconstruction of the original raster data. To drop back to a generic mesh3d the easiest is to reproject the data.\n\n##remotes::install_github(\"hypertidy/reproj\")\nlibrary(reproj)\nqm_ll &lt;- reproj(qm, \"+proj=longlat +datum=WGS84\")\n\nWarning in reproj.quadmesh(qm, \"+proj=longlat +datum=WGS84\"): quadmesh raster\ninformation cannot be preserved after reprojection, dropping to mesh3d class\n\n\nThis is a lossless reprojection, as it is equivalent to sf::sf_project(t(qm$vb[1:2, ]), from = qm$crs, to = \"+proj=longlat +datum=WGS84\") or with rgdal::project(, qm$crs, inv = TRUE).\nWe can plot this in the usual way with rgl, or see upcoming features in the mapdeck package.\n\nclear3d()\nshade3d(qm_ll, lit = TRUE, col = \"grey\")\naspect3d(1, 1, 0.1); \nview3d(0, phi = -60)\nrglwidget()\n\n\n\n\n\nTo put colours on this, we can do it by faces\n\nclear3d()\nqm_ll$material$color &lt;- colourvalues::color_values(raster::values(topo))\nshade3d(qm_ll, meshColor = \"faces\", lit = TRUE)\nrglwidget()\n\n\n\n\n\n(each face is discretely coloured), or by vertex in the legacy mode.\nNot run, to save the size of the document.\n\nclear3d()\nqm_ll$material$color &lt;-colourvalues::color_values(qm_ll$vb[3, qm_ll$ib])\n                                                   \nshade3d(qm_ll, meshColor = \"legacy\", lit = TRUE)\nrglwidget()"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html",
    "href": "posts/2015-12-28_gis3d/index.html",
    "title": "GIS for 3D in R",
    "section": "",
    "text": "GIS data structures are not well suited for generalization, and visualizations and models in 3D require pretty forceful and ad hoc approaches.\nHere I describe a simple example, showing several ways of visualizing a simple polygon data set. I use the programming environment R for the data manipulation and the creation of this document via several extensions (packages) to base R."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "href": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "title": "GIS for 3D in R",
    "section": "Polygon “layer”",
    "text": "Polygon “layer”\nThe R package maptools contains an in-built data set called wrld_simpl, which is a basic (and out of date) set of polygons describing the land masses of the world by country. This code loads the data set and plots it with a basic grey-scale scheme for individual countries.\n\nlibrary(maptools)\ndata(wrld_simpl)\nprint(wrld_simpl)\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 246 \nextent      : -180, 180, -90, 83.57027  (xmin, xmax, ymin, ymax)\nvariables   : 11\n# A tibble: 246 × 11\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows\n\nplot(wrld_simpl, col = grey(sample(seq(0, 1, length = nrow(wrld_simpl)))))\n\n\n\n\n\n\n\n\nWe also include a print statement to get a description of the data set, this is a SpatialPolygonsDataFrame which is basically a table of attributes with one row for each country, linked to a recursive data structure holding sets of arrays of coordinates for each individual piece of these complex polygons.\nThese structures are quite complicated, involving nested lists of matrices with X-Y coordinates. I can use class coercion from polygons, to lines, then to points as the most straightforward way of obtaining every XY coordinate by dropping the recursive hierarchy structure to get at every single vertex in one matrix.\n\nallcoords &lt;- coordinates(as(as(wrld_simpl, \"SpatialLines\"), \"SpatialPoints\"))\ndim(allcoords)\n\n[1] 26264     2\n\nhead(allcoords)  ## print top few rows\n\n     coords.x1 coords.x2\n[1,] -61.68667  17.02444\n[2,] -61.88722  17.10527\n[3,] -61.79445  17.16333\n[4,] -61.68667  17.02444\n[5,] -61.72917  17.60861\n[6,] -61.85306  17.58305\n\n\n(There are other methods to obtain all coordinates while retaining information about the country objects and their component “pieces”, but I’m ignoring that for now.)\nWe need to put these “X/Y” coordinates in 3D so I simply add another column filled with zeroes.\n\nallcoords &lt;- cbind(allcoords, 0)\nhead(allcoords)\n\n     coords.x1 coords.x2  \n[1,] -61.68667  17.02444 0\n[2,] -61.88722  17.10527 0\n[3,] -61.79445  17.16333 0\n[4,] -61.68667  17.02444 0\n[5,] -61.72917  17.60861 0\n[6,] -61.85306  17.58305 0\n\n\n(Note for non-R users: in R expressions that don’t include assignment to an object with &lt;- are generally just a side-effect, here the side effect of the head(allcoords) here is to print the top few rows of allcoords, just for illustration, there’s no other consequence of this code)."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "href": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "title": "GIS for 3D in R",
    "section": "OpenGL in R",
    "text": "OpenGL in R\nIn R we have access to 3D visualizations in OpenGL via the rgl package, but the model for data representation is very different so I first plot the vertices of the wrld_simpl layer as points only.\n\nlibrary(rgl)\nplot3d(allcoords, xlab = \"\", ylab = \"\") ## smart enough to treat 3-columns as X,Y,Z\nrglwidget()\n\n\n\n\n\nPlotting in the plane is one thing, but more striking is to convert the vertices from planar longitude-latitude to Cartesizan XYZ. Define an R function to take “longitude-latitude-height” and return spherical coordinates (we can leave WGS84 for another day).\n\nllh2xyz &lt;- \nfunction (lonlatheight, rad = 6378137, exag = 1) \n{\n    cosLat = cos(lonlatheight[, 2] * pi/180)\n    sinLat = sin(lonlatheight[, 2] * pi/180)\n    cosLon = cos(lonlatheight[, 1] * pi/180)\n    sinLon = sin(lonlatheight[, 1] * pi/180)\n    rad &lt;- (exag * lonlatheight[, 3] + rad)\n    x = rad * cosLat * cosLon\n    y = rad * cosLat * sinLon\n    z = rad * sinLat\n    cbind(x, y, z)\n}\n\n## deploy our custom function on the longitude-latitude values\nxyzcoords &lt;- llh2xyz(allcoords)\n\nNow we can visualize these XYZ coordinates in a more natural setting, and even add a blue sphere for visual effect.\n\nplot3d(xyzcoords, xlab = \"\", ylab = \"\")\nspheres3d(0, 0, 0, radius = 6370000, col = \"lightblue\")\nrglwidget()\n\n\n\n\n\nThis is still not very exciting, since our plot knows nothing about the connectivity between vertices."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "href": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "title": "GIS for 3D in R",
    "section": "Organization of polygons",
    "text": "Organization of polygons\nThe in-development R package gris provides a way to represent spatial objects as a set of relational tables. I’m leaving out the details because it’s not the point I want to make, but in short a gris object has tables “o” (objects), “b” (for branches), “bXv” (links between branches and vertices) and “v” the vertices.\nIf we ingest the wrld_simpl layer we get a list with several tables.\nEDITOR NOTE (April 2022): see anglr function DEL0() for an updated way to create what gris was doing in 2015.\n\nlibrary(gris)  ## devtools::install_github(\"r-gris/gris\")\nlibrary(dplyr)\ngobject &lt;- gris(wrld_simpl)\n\nThe objects, these are individual countries with several attributes including the NAME.\n\ngobject$o\n\n# A tibble: 246 × 12\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt;    &lt;int&gt;   &lt;dbl&gt;  &lt;int&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows, and 1 more variable: object_ &lt;int&gt;\n\n\nThe branches, these are individual simple, one-piece “ring polygons”. Every object may have one or more branches (branches may be an “island” or a “hole” but this is not currently recorded). Note how branch 1 and 2 (branch_) both belong to object 1, but branch 3 is the only piece of object 2.\n\ngobject$b\n\n# A tibble: 3,768 × 3\n   object_ branch_ island_\n     &lt;int&gt;   &lt;int&gt; &lt;lgl&gt;  \n 1       1       1 TRUE   \n 2       1       2 TRUE   \n 3       2       3 TRUE   \n 4       3       4 TRUE   \n 5       3       5 TRUE   \n 6       3       6 TRUE   \n 7       3       7 TRUE   \n 8       3       8 TRUE   \n 9       4       9 TRUE   \n10       5      10 TRUE   \n# … with 3,758 more rows\n\nplot(gobject[1, ], col = \"#333333\")\ntitle(gobject$o$NAME[1])\n\n\n\n\n\n\n\nplot(gobject[2, ], col = \"#909090\")\ntitle(gobject$o$NAME[2])\n\n\n\n\n\n\n\n\n(Antigua and Barbuda sadly don’t get a particularly good picture here, but this is not the point of the story.)\nThe links between branches and vertices.\n\ngobject$bXv\n\n# A tibble: 26,264 × 3\n   branch_ order_ vertex_\n     &lt;int&gt;  &lt;int&gt;   &lt;int&gt;\n 1       1      1    5589\n 2       1      2    5620\n 3       1      3    5605\n 4       1      4    5589\n 5       2      1    5596\n 6       2      2    5611\n 7       2      3    5613\n 8       2      4    5596\n 9       3      1   16101\n10       3      2   17581\n# … with 26,254 more rows\n\n\nThis table is required so that we can normalize the vertices by removing any duplicates based on X/Y pairs. This is required for the triangulation engine used below, although not by the visualization strictly. (Note that we could also normalize branches for objects, since multiple objects might use the same branch - but again off-topic).\nFinally, the vertices themselves. Here we only have X and Y, but these table structures can hold any number of attributes and of many types.\n\ngobject$v\n\n# A tibble: 21,165 × 3\n       x_    y_ vertex_\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;\n 1 -61.7   17.0    5589\n 2 -61.9   17.1    5620\n 3 -61.8   17.2    5605\n 4 -61.7   17.6    5596\n 5 -61.9   17.6    5611\n 6 -61.9   17.7    5613\n 7   2.96  36.8   16101\n 8   4.79  36.9   17581\n 9   5.33  36.6   18103\n10   6.40  37.1   18790\n# … with 21,155 more rows\n\n\nThe normalization is only relevant for particular choices of vertices, so if we had X/Y/Z in use there might be a different version of “unique”. I think this is a key point for flexibility, some of these tasks must be done on-demand and some ahead of time.\nIndices here are numeric, but there’s actually no reason that they couldn’t be character or other identifier. Under the hood the dplyr package is in use for doing straightforward (and fast!) table manipulations including joins between tables and filtering on values."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "href": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "title": "GIS for 3D in R",
    "section": "More 3D already!",
    "text": "More 3D already!\nWhy go to all this effort just for a few polygons? The structure of the gris objects gives us much more flexibility, so I can for example store the XYZ Cartesian coordinates right on the same data set. I don’t need to recursively visit nested objects, it’s just a straightforward calculation and update - although we’re only making a simple point, this could be generalized a lot more for user code.\n\ngobject$v$zlonlat &lt;- 0\ndo_xyz &lt;- function(table) {\n  xyz &lt;- llh2xyz(dplyr::select(table, x_, y_, zlonlat))\n  table$X &lt;- xyz[,1]\n  table$Y &lt;- xyz[,2]\n  table$Z &lt;- xyz[,3]\n  table\n}\n\ngobject$v &lt;- do_xyz(gobject$v)\n\ngobject$v\n\n# A tibble: 21,165 × 7\n       x_    y_ vertex_ zlonlat        X         Y        Z\n    &lt;dbl&gt; &lt;dbl&gt;   &lt;int&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 -61.7   17.0    5589       0 2892546. -5369047. 1867388.\n 2 -61.9   17.1    5620       0 2872491. -5376810. 1875991.\n 3 -61.8   17.2    5605       0 2880293. -5370474. 1882167.\n 4 -61.7   17.6    5596       0 2879394. -5354145. 1929470.\n 5 -61.9   17.6    5611       0 2868217. -5361116. 1926758.\n 6 -61.9   17.7    5613       0 2864423. -5358522. 1939577.\n 7   2.96  36.8   16101       0 5100196.   264042. 3820852.\n 8   4.79  36.9   17581       0 5083067.   425571. 3829093.\n 9   5.33  36.6   18103       0 5095693.   475230. 3806402.\n10   6.40  37.1   18790       0 5056321.   567008. 3846135.\n# … with 21,155 more rows\n\n\nI now have XYZ coordinates for my data set, and so for example I will extract out a few nearby countries and plot them.\n\nlocalarea &lt;- gobject[gobject$o$NAME %in% c(\"Australia\", \"New Zealand\"), ]\n## plot in traditional 2d\nplot(localarea, col = c(\"dodgerblue\", \"firebrick\"))\n\n\n\n\n\n\n\n\nThe plot is a bit crazy since parts of NZ that are over the 180 meridian skews everything, and we could fix that easily by modifiying the vertex values for longitude, but it’s more sensible in 3D.\n\nrgl::plot3d(as.matrix(localarea$v[c(\"X\", \"Y\", \"Z\")]), xlab = \"\", ylab = \"\")\nrglwidget()\n\n\n\n\n\nFinally, to get to the entire point of this discussion let’s triangulate the polygons and make a nice plot of the world.\nThe R package RTriangle wraps Jonathan Shewchuk’s Triangle library, allowing constrained Delaunay triangulations. To run this we need to make a Planar Straight Line Graph from the polygons, but this is fairly straightforward by tracing through paired vertices in the data set. The key parts of the PSLG are the vertices P and the segment indexes S defining paired vertices for each line segment. This is a “structural” index where the index values are bound to the actual size and shape of the vertices, as opposed to a more general but perhaps less efficient relational index.\n\npslgraph &lt;- gris:::mkpslg(gobject)\ndim(pslgraph$P)\n\n[1] 21165     2\n\nrange(pslgraph$S)\n\n[1]     1 21165\n\nhead(pslgraph$P)\n\n            x_       y_\n[1,] -61.68667 17.02444\n[2,] -61.88722 17.10527\n[3,] -61.79445 17.16333\n[4,] -61.72917 17.60861\n[5,] -61.85306 17.58305\n[6,] -61.87306 17.70389\n\nhead(pslgraph$S)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    3\n[3,]    3    1\n[4,]    1    1\n[5,]    4    5\n[6,]    5    6\n\n\nThe PSLG is what we need for the triangulation.\n\ntri &lt;- RTriangle::triangulate(pslgraph)\n\nThe triangulation vertices (long-lat) can be converted to XYZ, and plotted.\n\nxyz &lt;- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   5 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\naspect3d(\"iso\"); bg3d(\"grey12\")\nrglwidget()\n\n\n\n\n\nThese are very ugly polygons since there’s no internal vertices to carry the curvature of this sphere. This is the same problem we’d face if we tried to drape these polygons over topography: at some point we need internal structure.\nLuckily Triangle can set a minimum triangle size. We set a constant minimum area, which means no individual triangle can be larger in area than so many “square degrees”. This gives a lot more internal structure so the polygons are more elegantly draped around the surface of the sphere. (There’s not really enough internal structure added with this minimum area, but I’ve kept it simpler to make the size of this document more manageable).\n\ntri &lt;- RTriangle::triangulate(pslgraph, a = 9)  ## a (area) is in degrees, same as our vertices\nxyz &lt;- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   6 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\nbg3d(\"gray12\")\nrglwidget()\n\n\n\n\n\nWe still can’t identify individual polygons as we smashed that information after putting the polygon boundary segments through the triangulator. With more careful work we could build a set of tables to store particular triangles between our vertices and objects, but to finish this story I just loop over each object adding them to the scene.\n\n## loop over objects\ncols &lt;- sample(grey(seq(0, 1, length = nrow(gobject$o))))\nopen3d()\n\nnull \n   7 \n\nfor (iobj in seq(nrow(gobject$o))) {\n  pslgraph &lt;- gris:::mkpslg(gobject[iobj, ])\n  tri &lt;- RTriangle::triangulate(pslgraph, a = 9)  ## a is in units of degrees, same as our vertices\n  xyz &lt;- llh2xyz(cbind(tri$P, 0))\n  triangles3d(xyz[t(tri$T), ], col = cols[iobj], specular = \"black\")\n}\nbg3d(\"gray12\")\nrglwidget()"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "href": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "title": "GIS for 3D in R",
    "section": "Real world topography, Image textures",
    "text": "Real world topography, Image textures\nfuture work …\nSee anglr package"
  },
  {
    "objectID": "posts/2024-12-04_plot_native/index.html",
    "href": "posts/2024-12-04_plot_native/index.html",
    "title": "Plot at native resolution, with R",
    "section": "",
    "text": "Plot native, something I should have done long ago.\n\n#' Plot raster at native resolution\n#'\n#' Determines the current device size and plots the raster centred on its own\n#' middle to plot at native resolution. \n#'\n#' @param x as SpatRaster\n#' @param ... passed to terra::plot\n#'\n#' @return the input raster, cropped corresponding to the plot made\n#' @export\n#'\n#' @examples\n#' plot_native(terra::rast(volcano))\n#' plot_native(terra::disagg(terra::rast(volcano), 64))\nplot_native &lt;- function(x, ...) {\n  ex &lt;- as.vector(terra::ext(x))\n  at &lt;- NULL\n  ## take the centre\n  if (is.null(at)) {\n    at &lt;- apply(matrix(ex, 2), 2, mean)\n  }\n  dv &lt;- dev.size(\"px\")\n  scl &lt;- terra::res(x)\n  halfx &lt;- dv[1]/2 * scl[1]\n  halfy &lt;- dv[2]/2 * scl[2]\n  cropex &lt;- c(at[1] - halfx, at[1] + halfx, at[2] - halfy, at[2] + halfy)\n  x &lt;- terra::crop(x, terra::ext(cropex), extend = TRUE)\n  add &lt;- FALSE\n  if (terra::nlyr(x) &gt;= 3) terra::plotRGB(x, add = add) else plot(x, ..., add = add)\n  x\n}\n\nSo as an example read this world imagery.\n\ndsn &lt;- \"&lt;GDAL_WMS&gt;&lt;Service name=\\\"TMS\\\"&gt;&lt;ServerUrl&gt;http://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/${z}/${y}/${x}&lt;/ServerUrl&gt;&lt;/Service&gt;&lt;DataWindow&gt;&lt;UpperLeftX&gt;-20037508.34&lt;/UpperLeftX&gt;&lt;UpperLeftY&gt;20037508.34&lt;/UpperLeftY&gt;&lt;LowerRightX&gt;20037508.34&lt;/LowerRightX&gt;&lt;LowerRightY&gt;-20037508.34&lt;/LowerRightY&gt;&lt;TileLevel&gt;17&lt;/TileLevel&gt;&lt;TileCountX&gt;1&lt;/TileCountX&gt;&lt;TileCountY&gt;1&lt;/TileCountY&gt;&lt;YOrigin&gt;top&lt;/YOrigin&gt;&lt;/DataWindow&gt;&lt;Projection&gt;EPSG:900913&lt;/Projection&gt;&lt;BlockSizeX&gt;256&lt;/BlockSizeX&gt;&lt;BlockSizeY&gt;256&lt;/BlockSizeY&gt;&lt;BandsCount&gt;3&lt;/BandsCount&gt;&lt;MaxConnections&gt;10&lt;/MaxConnections&gt;&lt;Cache /&gt;&lt;ZeroBlockHttpCodes&gt;204,404,403&lt;/ZeroBlockHttpCodes&gt;&lt;/GDAL_WMS&gt;\"\n\nlibrary(terra)\n\nterra 1.7.83\n\nim &lt;- project(rast(dsn), rast(ext(-1, 1, -1, 1) * 1e6, res = 1000, crs = \"+proj=laea +lon_0=147 +lat_0=-42\"), by_util = TRUE)\nplotRGB(im)\n\n\n\n\n\n\n\n\nDo we have more, or less resolution than our device can handle? This next plot shows that we have more, and the cropped raster is returned to match the device.\n\nprint(dim(im))\n\n[1] 2000 2000    3\n\nprint(dev.size(\"px\"))\n\n[1] 1344  960\n\nplot_native(im)\n\n\n\n\n\n\n\n\nclass       : SpatRaster \ndimensions  : 960, 1344, 3  (nrow, ncol, nlyr)\nresolution  : 1000, 1000  (x, y)\nextent      : -672000, 672000, -480000, 480000  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=laea +lat_0=-42 +lon_0=147 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : GDAL_WMS&gt;_1, GDAL_WMS&gt;_2, GDAL_WMS&gt;_3 \nmin values  :           0,           0,           0 \nmax values  :         255,         237,         213"
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html",
    "href": "posts/2017-09-01_gdal-in-r/index.html",
    "title": "GDAL in R",
    "section": "",
    "text": "For some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\nI already knew how immensely powerful R was, with its epic flexibity and useability and I could just sense there was a brighter way once I understood many more details. As my experience grew I was able to do amazing tasks like, merge a few shapefiles together into one, or plot a window of data from a georeferenced grid. Previously the best I’d done in this space was VBScript in Manifold GIS, which I could use to automate some data tasks - but the prospects of full automation from raw data files to output, end-to-end with a software tool that anyone could use was absolutely mind-blowing. I was super-powered, I remember earning a carton of beer from a colleague of my father, for munging some SHP or TAB files between AGD66 and GDA94 … or something, and I knew I had a bright future ahead.\nSo what’s the abstraction? GDAL does not care what format the data is in, it could be points, lines, areas, a raster DEM, a time series of remote sensing, or an actual image. It just doesn’t mind, there’s an interpretation in its model for what’s in the file (or data base, or web server) and it will deliver that interpretation to you, very efficiently. If you understand that intepretation you can do a whole lot of amazing stuff. When this works well it’s because the tasks are modular, you have a series of basic tools designed to work together, and it’s up to you as a developer or a user to chain together the pieces to solve your particular problem."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "href": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "title": "GDAL in R",
    "section": "Where does this get difficult?",
    "text": "Where does this get difficult?\nGDAL is a C++ library, and that’s not accessible to most users or developers. The other key user-interface is the set of command line utilities, these are called gdal_translate, gdalinfo, ogr2ogr, ogrinfo, and many others. The command line is also not that accessible to many users, though it’s more so than C++ - this is why command line is a key topic for Software Carpentry. These interfaces give very high fidelity to the native interpretation provided by the GDAL model.\nGDAL is used from many languages, there’s Python, R, Perl, C#, Fortran, and it is bundled into many, many softwares - a very long list. The original author wrote code for some of the most influential geo-spatial software the world has, and some of that is in GDAL, some is locked up forever in propietary forms. He saw this as a problem and very early on engineered the work to be able to be open, in the do anything with me, including privatize me-license called MIT. Have a look in the source code for gdalwarp, you’ll see the company who was the best at raster reprojection in the late 1990s and early 2000s.\nPython is surely the closest other language to the native interpretation, but then it’s not that simple, and this is not that story …\nR has a very particular interpretation of the GDAL interpretation, it’s called rgdal and if you are familiar with the GDAL model and with R you can see a very clear extra layer there. This extra level is there partly because of when it was done, the goals of the authors, the community response to the amazingly powerful facilities it provided, but also and perhaps mainly because it was very hard. R’s rather peculiar API meant that in the early 2000s the authors had to write in another language, a language between the native GDAL C++ and the R user language - this is the R API, it’s full of SEXP and UNPROTECTs and if you search this issue you’ll see clear signals not to bother - now you should just get on with learning Rcpp.\nThese extra levels are there in the R API, the hard stuff down in the rgdal/src/ folder but also in the R code itself. There’s a bunch of rigorous rules applied there, things to help protect us from that lower level, and to save from making serious analytical mistakes. All of this was very hard work and very well-intended, but it’s clear that it takes us away from the magic of the GDAL abstraction, we have a contract with rgdal, the R code has a contract with the R API, and the rgdal/src has a contract with GDAL. All of these things divorce R users and developers from the original schemes that GDAL provides, because at the R level rgdal itself has to provide certain guarantees and contracts with both R and with R users. I think that is too much for one package.\nAdd to this the complex zoo of formats, the other libraries that GDAL requires for full use. The Windows rgdal on CRAN doesn’t include HDF4, or HDF5, or NetCDF, or DODS - there are many missing things in this list, and it’s not clear if it’s because it’s hard, it’s against the license (ECW might be tricky, MrSID most definitely would be), or because no one has asked or maybe no one knows how, or maybe CRAN doesn’t want to. (Would you know how to find out?) All of these things add up to being way too much for one package. It’s kind of impossible, though now there are many more eyes on the problem and progress is being made. Who should decide these things? How would anyone know it’s even an issue?\nI wonder if many of us see rgdal as the definition of the GDAL abstraction. I see pretty clearly the difference, and while the package has been extremely useful for me I’ve long wanted lower level control and access to the GDAL core itself. (I had rather influential guidance from extremely expert programmers I’ve worked with, and I’ve discussed GDAL with many others, including employees of various companies, and across various projects and with many users. I assume most R users don’t know much about the details, and why would they want to?).\nThere is active work to modernize rgdal, and you should be aware of the immensely successful sf and the soon to be stars project. sf is an R interpretation of the Simple Features Standard (GDAL has an interpretation of that standard too, sf starts there when it reads with GDAL). stars will start with GDAL as a model for gridded data, and it’s not yet fleshed out what the details of that will be. None of these interpretations are permanent, though while the simple features standard is unlikely to change, there is no doubt that GDAL will evolve and include more features that don’t involve that standard. These things do change and very few people, relatively, are engaged in the decisions that are made. (GDAL could definitely benefit from more input, also something I’ve long wanted to do more of).\nIt’s been a long time, but I’ve recently found a way over the key obstacle I had - building a package to compile for use in R with bindings to GDAL itself. I have a lot to thank Roger Bivand and Edzer Pebesma for many years of instruction and guidance in that, in many different ways. I also am extremely grateful to R-Core for the overall environment, and the tireless work done by the CRAN maintainers. I have to mention Jeroen Ooms and Mark Padgham who’ve been extremely helpful very recently. This is something I’ve wanted to be able to do for a really long time, I hope this post helps provide context to why, and I hope it encourages some more interest in the general topic."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "href": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "title": "GDAL in R",
    "section": "vapour",
    "text": "vapour\nMy response to the interpretation layers is vapour, this is my version of a very minimal interpretation of the core GDAL facilities. There’s a function to read the attribute data from geometric features, a function to read the geometry data as raw binary, or various text formats, a function to read only the bounding box of each feature, and there’s a function to read the raw pixel values from a local window within a gridded data set.\nNone of this is new, we have R packages that do these things and the vapour functions will have bugs, and will need maintenance, and maybe no one but me will ever use them. I’ve needed them and I’ve started to learn a whole lot more about what I’m going to do next with them. I recommend that any R user with an interest in geo-spatial or GDAL facilities have a closer look at how they work - and if you know there’s a lower level below the interpretations provide in R you should explore them. Rcpp and the modern tools for R really do make this immensely more easy than in the past (RStudio has intellisense for C++ …).\nI also believe strongly that R is well-placed to write the future of multi-dimensional and hierarchical and complex structured and geo-spatial data. Do you know what that future should look like?"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html",
    "href": "posts/2017-01-10_r-spatial-2017/index.html",
    "title": "R spatial in 2017",
    "section": "",
    "text": "This document is a broad overview of what I see as most relevant to future spatial, for 2017 and beyond. I’ve tried to be as broad as possible, without going into too much detail, but it’s also quite personal and opinionated."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "title": "R spatial in 2017",
    "section": "The state of things",
    "text": "The state of things\nAn enormous amount of activity has been going on in R spatial. The keystone activity is the new simple features for R package sf, and the many responses to “supporting sf” in various packages but there are many other non-obvious linkages.\nPersonally, I have learnt quite a lot recently about the broader context within R and I’m keen to help consolidate some of the “non-central” tools that we use. There are also some surprisingly helpful implications of the new simple features support both for within the central package, and for the ecosystem around it.\n\nsimple features for R via the sf package\nupcoming replacement for raster\npoint clouds\nmapview and leaflet\nsimple features in other packages\n“exotic types” in sf\nspatial data that simple features cannot support\nhtmlwidgets and plotly"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "title": "R spatial in 2017",
    "section": "sf",
    "text": "sf\nThe simple features package sf was one of the first supported projects of the RConsortium and has been created by Edzer Pebesma. This package replaces sp vector data completely, includes a replacement for rgdal and rgeos and there is a long list of important improvements. These are described in full in the package vignettes and blog posts.\nThe key changes relevant here are\n\nno S4 classes, everything is S3 and so is more immediately manipulable/accessible\nmuch better methods support overall, for printing/summary etc.\nthe key objects are list-vectors, and data frames\nsupport for simple features standard, no more ambiguity for multi-polygons/holes, mixed types, NULL geometry, XYZ-M support\nnative support for dplyr verbs, tibbles (some is WIP but group_by/summarize reprojection and geometric union are good examples that already work)\nreprojection and geometry manipulation and file format support is now GDAL 2.0 and GEOS(?), modernized, more reliable, easier\n\nI strongly recommend getting familiar with the sf data structures, it’s really important to understand the hierarchy levels and the ways the vectors (POINT) and matrices (everything else) are stored. POINT is a vector, MULTIPOINT is a matrix, POLYGON is a list of matrices (one island, zero or more holes), LINESTRING is a matrix (same as m-point), MULTIPOLYGON is a list of lists of matrices (a list of POLYGONs, effectively), and MULTILINESTRING is a list of matrices (a list of LINESTRINGs, effectively and structurally the same as a POLYGON).\nIf you want to see how to convert between sf forms and extract the raw coordinates I would look at st_cast in, and the leaflet approach here:\nhttps://github.com/rstudio/leaflet/blob/master/R/normalize-sf.R"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "title": "R spatial in 2017",
    "section": "rasters",
    "text": "rasters\nThe raster package is apparently being significantly upgraded and Edzer has plans for this as well.\nhttp://r-spatial.org/r/2016/09/26/future.html\nThere are some very interesting extensions to raster on CRAN recently, notably velox, fasteraster, and the unrelated but very nice dggridR.\nHDF5 is now fully supported by rhdf5 on Bioconductor, this could also replace many of the NetCDF4 formats supported by ncdf4, and notably can be used to read NetCDF4 files with compound types."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "title": "R spatial in 2017",
    "section": "Point clouds",
    "text": "Point clouds\nA recent contribution on CRAN is rlas and lidR for the LiDaR LAS format, and algorithms for working with point clouds. It is trivial to push these data into sf types, but it won’t always make sense to do so. You could have a MULTIPOINT with X, Y, Z, and M (but none of the other point attributes) or a point (XYZ) with all the attributes in one sf data frame. This package will be useful for driving interest in the exotic sf types.\nIt’s easy and readily doable right now to read LIDAR data with rlas, and plot it interactively with RGB styling and so in with plotly. Keen to try writing a “detect ground” algorithm? Try it! What does st_triangulate do with a XYZ multipoint? (hmm, it fails noisily - you aren’t supposed to do that)."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "title": "R spatial in 2017",
    "section": "mapview and leaflet",
    "text": "mapview and leaflet\nmapview has support from RConsortium to bring user interaction to spatial in R. Currently building in support for sf, which will be accelerated by the recent dev upgrades in leaflet itself and will eventually support the range of sf types, including full support for MULTIPOLYGON.\nleaflet now has a huge number of new extensions thanks to leaflet.extras, and there is ongoing updates around integrating crosstalk which will be very importatnt for interactive map applications."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "title": "R spatial in 2017",
    "section": "simple features in other packages",
    "text": "simple features in other packages\ntmap, mapview, spbabel, stplanr, …. all have internal versions of sf types converted to something else. It’s an interesting time for these packages that extend the sp and sf structures, and there are opportunities to ensure that best practices are being used."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "title": "R spatial in 2017",
    "section": "Exotic types in sf",
    "text": "Exotic types in sf\nThese are TINs, Polyhedral Surfaces (multipatch - basically polygons with shared “internal” edges), curves and various combinations and varieties of these. None of the triangulations use an indexed mesh, which makes them a bit clunky and probably only for very bespoke uses, but they provide interesting territory to explore. Certainly you can use them to build 3D plots in rgl and plotly (show examples, thanks to @timelyportfolio).\nNote that a GEOMETRYCOLLECTION of triangle POLYGONs is effectively the same as a simple features TIN, it doesn’t really add any structure improvement to the way the thing is put together:\nhttps://github.com/r-gris/sfct\nWe can bend the limits of simple features with “mesh” techniques, and the plotly packge provides easy publishing of interactive 3D visualizations of these.\nplotly is already useable for many applications, we can use techniques from rangl to put sf data into it, and we can use that to easily create exotic triangulated surfaces that are pretty inefficient in the simple features form:\nAlso check out Geotiff.js and timevis.\nHere are some rough examples with plotly.\nhttp://rpubs.com/cyclemumner/rangl-poly-topo-plotly\nhttp://rpubs.com/cyclemumner/raster-quads-to-triangles\nhttp://rpubs.com/cyclemumner/rangl-plotly\nI particularly want texture mapping to go with this kind of plotting, but apparently plotly cannot do that.\nhttps://twitter.com/mathinpython/status/818500905905561600"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "title": "R spatial in 2017",
    "section": "The limits of simple features",
    "text": "The limits of simple features\nSimple features can’t fully represent GPS and other track data, indexed meshes (like rgl mesh3d, segmented paths), or custom hierarchies like networks, nested objects like counties within states, or arc-node topology (like TopoJSON), and it can’t store aesthetics with primitives (like ggplot2/ggvis, rgl, plotly and others can). R can do all of these things, in many different ways and converting from and to sf is not too difficult.\nPlease don’t get the wrong idea though, sf is invaluable for developing more general tools that can work with these structures. Using the types in sf is very refreshing if you’ve been frustrated trying to pick apart a Spatial object in the past.\nIn terms of going beyong simple features itself, GDAL is also going in this direction: http://lists.osgeo.org/pipermail/gdal-dev/2016-December/045675.html\nWe already have most of this capability in R, it’s just scattered all over the place. I’m interested to collate together the best workflows and see where this can go."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "title": "R spatial in 2017",
    "section": "My plans",
    "text": "My plans\nI’m pretty comfortable now with sf and using it for what I want, I have converters to build the forms and workflows I need, and the support in mapview and leaflet and plotly provides more than enough to go with. I will work on making this as accessible and general as possible, and work on integrating it to replace my work on tracking data (the trip and SGAT packages), with 3D models (rbgm, quadmesh) and integrating it with htmlwidgets tools.\nI haven’t yet looked at curves, but I’m keen to see this capability in R both for sf and more generally, we could easily represent the forms made possible by TopoJSON (see “Bostock flawed example”), there is curve support in grid.\nWe could provide smart geo-spatial finite element forms (triangulations, quads),\nI’m keen to see how ggvis and ggplot2 represent geometric types for objects that shared vertices, such as intervals and bar charts, and how we can put in indexed data structures (unlike what ggraph is doing, it builds a mesh from a group of coordinates, you can’t provide it with and index-mesh). I think we can build spatial structures that can store all of these things, so we could throw sf at ggplot2, and use the output as a kind of super-form that knows how to wrap it up into an interactive 4D plot, how to display its primitives etc. etc.\nGIS itself needs what we can already do in R, it’s not a target we are aspring to it’s the other way around."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html",
    "title": "GDAL warper with R",
    "section": "",
    "text": "There’s several meanings floating around when you say the “GDAL warper”. It can mean\n\ncommand-line gdalwarp\nthe rasterio package in Python, and its WarpedVRT\nthe GDAL C++ API warping library\n\nWe can also mean\n\nthe sf package in R, and its gdal_utils() function\nthe stars package in R, and its st_warp() function\nthe terra package in R, and its project() function\nthe gdalUtils package in R\nthe gdalUtilities package in R\n\nBut, what I mean is the GDAL C++ API warping library."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#an-unfinished-post",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#an-unfinished-post",
    "title": "GDAL warper with R",
    "section": "",
    "text": "There’s several meanings floating around when you say the “GDAL warper”. It can mean\n\ncommand-line gdalwarp\nthe rasterio package in Python, and its WarpedVRT\nthe GDAL C++ API warping library\n\nWe can also mean\n\nthe sf package in R, and its gdal_utils() function\nthe stars package in R, and its st_warp() function\nthe terra package in R, and its project() function\nthe gdalUtils package in R\nthe gdalUtilities package in R\n\nBut, what I mean is the GDAL C++ API warping library."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#the-gdal-c-api-warping-library",
    "title": "GDAL warper with R",
    "section": "The GDAL C++ API warping library",
    "text": "The GDAL C++ API warping library\nGDAL is complex. It deals with very many different formats, and has many tools. In terms of this post, it has very low-level development facilities, written in C++. This means you can usually go as deep as you need into a geospatial problem by writing C++ code against the library directly. Often you don’t need C++ and can use Python, and sometimes you can use R depending on what is exposed there. Sometimes you need to modify GDAL itself, and you can do anything you want then(!), which is how open source is supposed to work.\nKey feature of the gdalwarp_lib.cpp GDALWarp() C++ function, versus the GDALWarpDirect(), GDALWarpIndirect() and the lower level GDALWarpMulti() and GDALWarpImage() functions.\nWhen we use gdalwarp_lib, we get handling of multiple-zoom level sources without intervention on our own.\nMost use cases I see of the warper facilities are for whole-sale conversion of large datasets, multiple input files converted to a another projection in a large output file or series of tiles."
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#the-warper-is-also-a-generalized-rasterio",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#the-warper-is-also-a-generalized-rasterio",
    "title": "GDAL warper with R",
    "section": "The warper is also a generalized RasterIO",
    "text": "The warper is also a generalized RasterIO\n(rasterio is a famous python package for using GDAL’s raster facilities, it’s not what we mean here).\nRasterIO is a C++ function in the GDAL library, its job is to take a source data set (i.e. a path to GeoTIFF) and to provide you with a window of pixels from that source. A window can be a subset, the entire raster, or a resampling (i.e. fewer pixels than native) of either the entire or a subset of the raster.\nIt’s really, cool and using it looks like this:\nerr = rasterBand-&gt;RasterIO(GF_Read, \n                           Xoffset, Yoffset, \n                           nXSize, nYSize,\n                           &double_scanline[0], \n                           outXSize, outYSize, \n                           GDT_Float64,\n                           0, 0, &psExtraArg);\nThere is a lot going on in that function call, but in short the key parts are (these are my argument-variable-names):\n\nrasterBand-&gt;RasterIO() is the way we read a pixel values, we have a raster band and we call the member function RasterIO()\nXoffset, Yoffset is the first column and row we should consider for reading (0,0 if we start at the top left corner)\nnXSize, nYSize\noutXSize, outYSize is the dimension of the window we get out\n\nThat last bits, the two kinds of Size is the magic, we can ask to start at a particular row,column and read out a given number of pixels in x and y. But, not only that we can specifiy where to end in the source. If nXSize and outXSize are not equal in value then we have asked for a resampling of the source “only read every nXSize / outXSize values in the x direction. If they are equal, just read every one. Note also that we might ask for an outX/YSize that is larger than the source nXSize/nYSize - and this would give us a resampling to higher resolution. This is really where the resampling algorithm comes in. ‘Nearest neighbour’ would give us copies of pixels, ‘Bilinear’ and interpolation between source pixels for new pixels in between. Other algorithms include ‘Cubic’, ‘Lanczos’, ‘Average’, ‘Sum’.\nThis is the key behind the fast and lazy reads provided by the terra and sf packages in R, this was originally available also in rgdal and raster made heavy use of it.\nBut, what is the offset and the size? We really want to think in geographic coordinates, and this is exactly what crop() does for example. We get a discretized crop, not an exact one because we only get to read by this raster-based mechanism - we are bound to the size and alignment of the source pixels.\nUnder the hood, functions like crop() do the following:\noffsets/scale vs. xlim,ylim\nThe other arguments in RasterIO.\n\nGF_Read controls the mode we are into (read or write or update)\nGDT_Float64 controls the type of data we get out (64 bit doubles here, GDAL will auto-convert if the source is different type)\n0, 0, &psExtraArg are further details we won’t discuss (though, the way resampling is done is controlled in the extra args options).\n\nWhat are its limitations:\n\nonly one source at a time, you can’t resample from multiple rasters at once (we are using offset/size indexing so the rasters must all be the same shape, you can read multiple bands)\n\n\nWhat is interesting about RasterIO vs. Warping is that we would never ever use warping for pure graphics. It doesn’t make sens"
  },
  {
    "objectID": "posts/2022-04-25_gdalwarper-in-R/index.html#enter-the-wutang-warper",
    "href": "posts/2022-04-25_gdalwarper-in-R/index.html#enter-the-wutang-warper",
    "title": "GDAL warper with R",
    "section": "Enter the WuTang Warper!",
    "text": "Enter the WuTang Warper!\nDoing discretized extent raster math is boring. With the warper we don’t need to do it.\nThis is because the warper is for changing projections, this is an entire re-modelling of raster data. (in the 2000s it was possible to do but still quite slow and problematic, hence you have very slowly changing standards and perceptions of what is possible/normal/appropriate in software comunities etc.).\nThis is a global longlat data set, but we want a local projection so we adopt one by finding its specification.\n\n\nIf we were using RasterIO we’d have to do this kind of math to run the index function.\nBut with the warper, we can use an extent (but we still need to align it, or we won’t get what the RasterIO facility would faithfully give)."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "",
    "text": "There are a lot of problems when it comes to array representation in data formats and software. There is a whole family of complex issues with sometimes subtle or even cryptic causes, and here we focus on one particular detail."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#degenerate-rectilinear-coordinates",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#degenerate-rectilinear-coordinates",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "Degenerate Rectilinear Coordinates",
    "text": "Degenerate Rectilinear Coordinates\nI have taken to saying this phrase to describe a particular situation in array handling. I get confused responses, or normally just no response or feedback on the term itself. I think it’s extremely important though. Python communities raced up to where R was a few years back and just sped on by without any nod to what R folks had learned, I think that’s a huge failure and I take it personally.\nHere’s what it is. Let’s say we have a pretty low resolution grid of the world, we break up the entire range of longitude and latitude into intervals, 360 for meridians and 180 for parallels.\n(This is neat because in whole number terms, this is exactly the right number).\nSo, define a grid, we make it a farily typical 1-degree-per-pixel grid of the entire globe flattened unceremoniously from an\nangular coordinate system “longitude and latitude” to one where we just plot those numbers in x,y on a flat plane.\n\n## define a grid in -180,180 -90,90 (360x180)\ndm &lt;- c(360, 180)\nxlim &lt;- c(-180, 180)\nylim &lt;- c(-90, 90)\n\n## create a grid\ndat &lt;- matrix(c(0, 1, -1, 0), 2L, 2L)[rep(1:2, each = 180), rep(1:2, each = 90)]\n## we don't get much from this because 0,1 0,1\n## and the world isn't 0,360 0,180 anyway (that would have been handy!)\n#image(dat)\n\n\nxs &lt;- seq(xlim[1], xlim[2], length.out = dm[1] + 1)\nys &lt;- seq(ylim[1], ylim[2], length.out = dm[2] + 1)\nimage(xs, ys, dat, asp = 1)\nmaps::map(add = TRUE)\n\n\n\n\n\n\n\n\nWhat happened there? Well, dat is a matrix, a 2D array in R. It has 360 rows and 180 columns (but yes, we are treating the world map as if it had 360 columns, 180 rows - but that is not this blogpost). Just don’t question it right now, but be assured we are treating the world as having 360 unique whole number longitudes, and 180 unique whole number latitudes.\nWe can image() that, by which I mean draw a pixel map on the screen of all the values in the matrix as if they were a field of little rectangles.\n\nimage(dat)\n\n\n\n\n\n\n\n\nBut, that’s very boring because we don’t have any idea where anything is in 0,1 0,1 space.\nWhat if we use xlim and ylim, these are plot() arguments in R (and exist in image() too).\n\nimage(dat, xlim = xlim, ylim = ylim)\n\n\n\n\n\n\n\n\nOk that was a trick, our matrix is drawn as a tiny dot right at 0,0, because xlim/ylim is about the plot not the data.\nNo more tricks, image() has 3 arguments x,y,z for normal usage.\n\nimage(x = xs, y = ys, z = dat, asp = 1)\n\n\n\n\n\n\n\n\nWhy does that work? We essentially have an x coordinate and a y coordinate for every pixel edge. We have one for the left side of the leftmost pixel column, and the second one is the right side of the left most pixel, and so on. We used dm[1] + 1 and dm[2] + 1 up there exactly because we wanted xlim and ylim to be the very extremes of the data.\nCool, explained - a matrix has a coordinate array for each side, we plot it like a map with those. Done."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#we-are-so-so-so-not-done.",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#we-are-so-so-so-not-done.",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "We are so so so not done.",
    "text": "We are so so so not done.\nI’m not here to explain how image() works in R, but that’s kind of necessary because I want people who don’t use R to also read this."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-is-rectilinear",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-is-rectilinear",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "What is Rectilinear?",
    "text": "What is Rectilinear?\nRectilinear means varying in one dimension. When applied to axis coordinates it just means you have a position along the axis for each step. These can vary in how big each step is, and that is exactly when we say it’s rectilinear, because otherwise it’s regular.\nOur ys above are a regular list of coordinates, each one is exactly the same distance apart (distance in the frame we are using, forget about the actual Earth).\n\nop &lt;- par(mfrow = c(2, 1))\nplot(ys)\nrange(diff(ys))\n\n[1] 1 1\n\nplot(diff(ys))\n\n\n\n\n\n\n\n\nSee? They just plod along with exactly the same delta between each, in this case the step size is 1 but it could be anything."
  },
  {
    "objectID": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-do-you-mean-degenerate",
    "href": "posts/2022-12-09_netcdf-degenerate-rectilinear/index.html#what-do-you-mean-degenerate",
    "title": "Degnerate Rectilinear (WIP)",
    "section": "What do you mean Degenerate?",
    "text": "What do you mean Degenerate?\nIsn’t this a bad word, pejorative? Well no, it means something very specific.\n\nIt means even though there is a quite a lot of data, the information is confused or confounded. The required information could be expressed very compactly, but it’s actually obscured by the details."
  },
  {
    "objectID": "posts/2024-12-11_conservative_gdal/conservative_regrid.html",
    "href": "posts/2024-12-11_conservative_gdal/conservative_regrid.html",
    "title": "Conservative regridding with GDAL (?)",
    "section": "",
    "text": "Can GDAL do conservative re-gridding? For cases of regular grid to regular grid, yes I think it can.\nPlease note that I’m using tools I’m comfortable with, because I wrote them. I will reframe in other tools and other languages in time. For some reason I’d been blocked on understanding this issue."
  },
  {
    "objectID": "posts/2024-12-11_conservative_gdal/conservative_regrid.html#simple-grid-with-four-values",
    "href": "posts/2024-12-11_conservative_gdal/conservative_regrid.html#simple-grid-with-four-values",
    "title": "Conservative regridding with GDAL (?)",
    "section": "Simple grid with four values",
    "text": "Simple grid with four values\nTake a grid, 2x2 with values 1,2,3,4 that sums to 10 and warp it to a new size.\n\n## target\ndm &lt;- c(2, 2)\n\ng1 &lt;- matrix(c(1, 2, 3, 4), dm[2], dm[1])\ne1 &lt;- c(-1, 1, -1, 1)\n\nlibrary(vapour)\nlibrary(terra)\n\nterra 1.7.83\n\n## we need this so we can use MEM: via dsn::mem()\nSys.setenv(GDAL_MEM_ENABLE_OPEN = \"YES\")\n\n\n## dsn::mem generates a Memory raster, \n## gdal_raster_data is the warper, here we get identity\nsg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"))\n\nunique(sg[[1]])\n\n[1] 1 2 3 4\n\nsum(sg[[1]])\n\n[1] 10\n\n## spatialize for easy plotting\ntor &lt;- function(x) {\n \n  dm &lt;- attr(x, \"dimension\")[2:1]\n  r &lt;- terra::rast(terra::ext(attr(x, \"extent\")), ncols = dm[1], nrows = dm[2], crs = attr(x, \"projection\"), \n                   vals = x[[1]])\n  r\n}\n\nr &lt;- \"sum\"\n\n##   (always use mem() \"live\" to avoid the garbage collector, and only for Float64 I'm afraid)\n\n\n## now use the same extent but reduce pixel size\ntg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"), target_ext = e1,\n                         target_dim = dm * 8, resample = r)\n\nstr(tg)\n\nList of 1\n $ : num [1:256] 0.0156 0.0156 0.0156 0.0156 0.0156 ...\n - attr(*, \"dimension\")= num [1:2] 16 16\n - attr(*, \"extent\")= num [1:4] -1 1 -1 1\n - attr(*, \"projection\")= chr \"GEOGCS[\\\"WGS 84\\\",DATUM[\\\"WGS_1984\\\",SPHEROID[\\\"WGS 84\\\",6378137,298.257223563,AUTHORITY[\\\"EPSG\\\",\\\"7030\\\"]],AU\"| __truncated__\n\n\nSo we have a lot more values as we resized from 2x2 to 16x16.\nIt still looks the same, but the overall quantity has been distributed.\n\nplot(tor(tg), main = r)\n## draw boundaries on\nabline(v = vaster::x_corner(dm * 8, e1), h = vaster::y_corner(dm * 8, e1))\n\n\n\n\n\n\n\nprint(sum(tg[[1]]))\n\n[1] 10\n\n## our sum of 10 was distributed across 256 pixels\nunique(tg[[1]])\n\n[1] 0.015625 0.031250 0.046875 0.062500\n\n\nNow a different example, an actual map projection change.\nBut, gawd … there’s a bug here. WIP\n\n## now try reprojecting our unit 1x1 longlat grid to LAEA centred a few degrees east and north\ncrs &lt;- \"+proj=laea +lon_0=5 +lat_0=0\"\n## reproject the extent, we use a densified boundary to find the new extent\nprex &lt;- reproj::reproj_extent(c(-1, 1, -1, 1), crs, source = \"EPSG:4326\")\nnewg &lt;- gdal_raster_data(dsn::mem(g1, extent = e1, projection = \"EPSG:4326\"), \n                         target_ext = prex, target_res = c(10000, 10000), target_crs = crs, resample = \"sum\", options = \"-tap\")\nplot(tor(newg))\n\n\n\n\n\n\n\nsum(newg[[1]])\n\n[1] 10\n\n\nAnd what about trying to return, of course this can only be approximate (I want to know how this compares to other tools.)\n\n## what if we go the other way, \n\nnewdm &lt;- attr(newg, \"dimension\")\nnewext &lt;- attr(newg, \"extent\")\nx &lt;- gdal_raster_data(dsn::mem(matrix(newg[[1]],newdm[2], byrow = TRUE),  extent = newext, projection = crs), \n                 target_dim = dm, target_ext = e1, target_crs = \"EPSG:4326\", resample = \"sum\")\nplot(tor(x))\n\n\n\n\n\n\n\nsum(x[[1]])\n\n[1] 9.496864"
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "",
    "text": "A new version of {ceramic} is now on CRAN, version 0.8.0.\nThe package exists for two purpose\nNOTE: we need a Mapbox key for the Mapbox servers see, please see ceramic::get_api_key().\nThe original versions of ceramic didn’t really separate these tasks but the new version does."
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-data-from-online",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-data-from-online",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Get raster data from online",
    "text": "Get raster data from online\nWe can read satellite imagery or elevation with a central point and a buffer (in metres).\n\nlibrary(ceramic)\n\nLoading required package: terra\n\n\nterra 1.7.23\n\npt &lt;- cbind(147.3257, -42.8826)\n\n(im &lt;- cc_location(pt, buffer = c(15000, 25000)))\n\nclass       : SpatRaster \ndimensions  : 960, 576, 3  (nrow, ncol, nlyr)\nresolution  : 52.08333, 52.08333  (x, y)\nextent      : 16385222, 16415222, -5319119, -5269119  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     0,    0 \nmax values  : 255,   255,  252 \n\n(el &lt;- cc_elevation(pt, buffer = c(15000, 25000)))\n\nclass       : SpatRaster \ndimensions  : 960, 576, 1  (nrow, ncol, nlyr)\nresolution  : 52.08333, 52.08333  (x, y)\nextent      : 16385222, 16415222, -5319119, -5269119  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\nname        :  lyr.1 \nmin value   :   -1.9 \nmax value   : 1264.9 \n\nop &lt;- par(mfcol = c(1, 2))\nplot(el, legend = F); plot(im, add = TRUE); plot(el, legend = F, \n                                                 col = hcl.colors(64)[tail(seq_len(64), 45)])\n\n\n\n\n\n\n\npar(op)\n\nThese raster objects are in terra ‘SpatRaster’ format (older ceramic used raster package).\nThese use the Mapbox ‘mapbox.satellite’ and ‘mapbox-terrain-rgb’ tile servers."
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-tiles-from-online",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#get-raster-tiles-from-online",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Get raster tiles from online",
    "text": "Get raster tiles from online\nIf we want the actual tiles, we can use the original get_tiles() function. (In older versions cc_location() and cc_elevation() would invoke get_tiles, but no longer).\n\npt &lt;- cbind(147.3257, -42.8826)\n\nimtiles &lt;- get_tiles(pt, buffer = c(15000, 25000))\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nstr(imtiles)\n\nList of 3\n $ files : chr [1:24] \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3722/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3723/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3724/2586.jpg\" \"/perm_storage/home/mdsumner/.cache/.ceramic/api.mapbox.com/v4/mapbox.satellite/12/3725/2586.jpg\" ...\n $ tiles :List of 2\n  ..$ tiles:'data.frame':   24 obs. of  2 variables:\n  .. ..$ x: int [1:24] 3722 3723 3724 3725 3722 3723 3724 3725 3722 3723 ...\n  .. ..$ y: int [1:24] 2586 2586 2586 2586 2587 2587 2587 2587 2588 2588 ...\n  .. ..- attr(*, \"out.attrs\")=List of 2\n  .. .. ..$ dim     : Named int [1:2] 4 6\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"x\" \"y\"\n  .. .. ..$ dimnames:List of 2\n  .. .. .. ..$ x: chr [1:4] \"x=3722\" \"x=3723\" \"x=3724\" \"x=3725\"\n  .. .. .. ..$ y: chr [1:6] \"y=2586\" \"y=2587\" \"y=2588\" \"y=2589\" ...\n  ..$ zoom : int 12\n  ..- attr(*, \"class\")= chr \"tile_grid\"\n $ extent: num [1:4] 16385222 16415222 -5319119 -5269119\n\neltiles &lt;- get_tiles(pt, buffer = c(15000, 25000))\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nhead(gsub(ceramic::ceramic_cache(), \"\",  eltiles$files))\n\n[1] \"/api.mapbox.com/v4/mapbox.satellite/12/3722/2586.jpg\"\n[2] \"/api.mapbox.com/v4/mapbox.satellite/12/3723/2586.jpg\"\n[3] \"/api.mapbox.com/v4/mapbox.satellite/12/3724/2586.jpg\"\n[4] \"/api.mapbox.com/v4/mapbox.satellite/12/3725/2586.jpg\"\n[5] \"/api.mapbox.com/v4/mapbox.satellite/12/3722/2587.jpg\"\n[6] \"/api.mapbox.com/v4/mapbox.satellite/12/3723/2587.jpg\"\n\n\nAnd to see what tiles we have we can materialize their footprint in wk::rct() form, this is way more efficient than having to use polygons.\n\nzoomtiles &lt;- ceramic::ceramic_tiles(imtiles$tiles$zoom)\n## sub out the ones we just triggered\nzoomtiles &lt;- dplyr::filter(zoomtiles, fullname %in% imtiles$files)\nrc &lt;- ceramic::tiles_to_polygon(zoomtiles)\nplot(rc)\nplot(ext(im), add = TRUE)  ## see the image from above is not the tiles, but the buffer around a point\n\n\n\n\n\n\n\nplot(read_tiles(pt, buffer = c(15000, 25000)))  ## but we can read thos tiles exactly\n\nPreparing to download: 24 tiles at zoom = 12 from \nhttps://api.mapbox.com/v4/mapbox.satellite/\n\nplot(rc, add = TRUE, border = \"white\")"
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#diverse-query-inputs",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#diverse-query-inputs",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Diverse query inputs",
    "text": "Diverse query inputs\nFinally, we can use a point and buffer to get imagery, or we can use a spatial object, currently supported are objects from {geos}, {wk}, {terra}, {sf}, {sp}, {raster}, and {stars}.\nHere’s an example.\n\nwkarea &lt;- wk::rct(xmin = 5e6, ymin = -2e6, xmax = 2e6, ymax = 3e6, crs = \"+proj=laea +lon_0=25 +lat_0=31\")\nim2 &lt;- cc_location(wkarea)\nel2 &lt;- cc_elevation(wkarea)\nop &lt;- par(mfcol = c(1, 2))\nplot(el2, legend = F); plot(im2, add = TRUE); plot(el2, col = grey.colors(128), legend = FALSE)\n\n\n\n\n\n\n\npar(op)\n\n(Please note that the result is still in Mercator, the query can be in any projection but we’re not matching that here, only its extent - ceramic version 0.8.0 is merely a stepping stone to some of the things we can do better with GDAL).\nPrevious versions of cc_location() and cc_elevation() were stuck with only ‘zoom’ and ‘max_tiles’ arguments, these make sense when you restrict exactly to the available tiles but when you just want a given area and a resolution, ‘dimension’ is more appropriate.\nBy default, the dimension is chosen relative to the graphics device. But, we can also specify exactly what we want. (Use zero for one of the dimensions to let the system figure out an appropriate size for the query you have).\n\ncc_location(wkarea)\n\nclass       : SpatRaster \ndimensions  : 960, 776, 3  (nrow, ncol, nlyr)\nresolution  : 7094.906, 7094.906  (x, y)\nextent      : 4203326, 9708973, 105375.6, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     0,    0 \nmax values  : 255,   255,  255 \n\ncc_location(wkarea, dimension = c(400, 700))\n\nclass       : SpatRaster \ndimensions  : 700, 400, 3  (nrow, ncol, nlyr)\nresolution  : 13766.3, 9730.157  (x, y)\nextent      : 4203326, 9709847, 105375.6, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     2,    0 \nmax values  : 255,   255,  255 \n\ncc_location(wkarea, dimension = c(1024, 0))\n\nclass       : SpatRaster \ndimensions  : 1267, 1024, 3  (nrow, ncol, nlyr)\nresolution  : 5377.463, 5377.463  (x, y)\nextent      : 4203326, 9709847, 103240.4, 6916485  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / Pseudo-Mercator (EPSG:3857) \nsource(s)   : memory\ncolors RGB  : 1, 2, 3 \nnames       : red, green, blue \nmin values  :   0,     1,    0 \nmax values  : 255,   255,  255"
  },
  {
    "objectID": "posts/2023-04-22_gdal_image_tiles/index.html#some-other-improvements",
    "href": "posts/2023-04-22_gdal_image_tiles/index.html#some-other-improvements",
    "title": "GDAL and image tiles, the {ceramic} package",
    "section": "Some other improvements",
    "text": "Some other improvements\n\nit’s faster, for loading reads data from image servers directly with the GDAL warper API\nwe have separation of tile downloading and interaction from raster loading via GDAL\nwe don’t materialize tiles as polygons, we have the tile description, or the compact rct representation\nnew functions read_tiles(), unpack_rgb(),\n\nSee the full list of changes here: https://hypertidy.github.io/ceramic/news/index.html.\nceramic is my final CRAN package that had direct dependencies on rgdal, rgeos, or maptools - in some ways it was the most challenging for me and had loomed as a problem for some time. But, I’ve progressed my own tool kit well in the time and I learnt a lot with this update.\nA future version will probably make the separation (are we tiles, or are we loading raster?) more complete, at any rate there are better ways to organize these things now. There are data sources, data readers and writers, data structures, and algorithms. This work is part of a family of tools that aims to make that orthogonality real and accessible."
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html",
    "title": "GDAL raster read/write by blocks",
    "section": "",
    "text": "A block is another word for a tile, a tile in a small-ish raster window within a larger raster. Tiles can be very clever, such as 256x256, they make a nice way to organize large data- keeping pieces of data that are nearby spatial nearby to each other in memory. There is a pair of functions internal to vapour that will 1) read data from a raster block 2) write data to a raster block. At the moment only Float64 data is handled."
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html#read-and-write-raster-by-blocks.",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html#read-and-write-raster-by-blocks.",
    "title": "GDAL raster read/write by blocks",
    "section": "",
    "text": "A block is another word for a tile, a tile in a small-ish raster window within a larger raster. Tiles can be very clever, such as 256x256, they make a nice way to organize large data- keeping pieces of data that are nearby spatial nearby to each other in memory. There is a pair of functions internal to vapour that will 1) read data from a raster block 2) write data to a raster block. At the moment only Float64 data is handled."
  },
  {
    "objectID": "posts/2022-04-05_gdal_raster_blocks/index.html#tiles",
    "href": "posts/2022-04-05_gdal_raster_blocks/index.html#tiles",
    "title": "GDAL raster read/write by blocks",
    "section": "Tiles",
    "text": "Tiles\nTiles are a way of organizing rasters, a tile may be 256x256, 512x512 (typically powers of 2 for sensible reasons), or they may be of higher dimensions 256x256x8 - this is really a private detail for a storage format, such as a file. The data values stored in a tile are a nother matter, these are like variables - we might have 1 variable (say elevation) stored in a double floating point value, or we might have 3 variables of byte values for storing an RGB image. These are independent concepts to the size of the tile, and strictly there may be no variables at all, just the abstract idea of the tiling.\nAnother kind of block, or tile is a the scanline of a raster. Imagine reading each row of the raster, from the top row to the bottom. Each line is its own kind of degenerate tile. We can read from a raster this way no matter what its internal tiling is, just software might be opening several tiles and reading just one line from each. So, best if we match our “tile attack” to the native structure of the data.\nHere we find a large, tiled raster, obtain its internal tiling, and use that to update a copy of the same file.\nPlease note how we copy the source file, then write to that. There’s probably other software better suited to doing this atm, this post simply aims to air the topic a little in an R context.\nHere we use only temporary files that we download, and copy as needed. In practice, you should set this up to work across different physical disks, and for better workflow we need the ability to open an empty file to write to (WIP).\n\nraster_url &lt;- \"ftp://ftp.data.pgc.umn.edu/elev/dem/setsm/REMA/mosaic/v1.1/200m/REMA_200m_dem.tif\"\n\nreadfrom_file &lt;- tempfile(pattern = \"readfrom\", fileext = \".tif\")\nwriteto_file &lt;- tempfile(pattern = \"writeto\", fileext = \".tif\")\n## file size is 1.3Gb\ncurl::curl_download(raster_url, readfrom_file)\n\n## this now makes a copy of the 1.3Gb file, so we have two of them\nfs::file_copy(readfrom_file, writeto_file)\n\nNow we want the tiling\n\ninfo &lt;- vapour::vapour_raster_info(readfrom_file)\ntiling &lt;- list(dimension = info$dimXY, tiles = info$tilesXY)\nfac &lt;- 100\nfac * tiling$tiles\nif (tiling$tiles[2] == 1) {\n  ## let's take fac scanlines at a time\n  tiling$tiles[2] &lt;- fac\n}\ncalc_steps &lt;- function(dimension, tiles) {\n  bounds_x &lt;- seq(0, dimension, by = tiles)\n  steps_x &lt;-  rep(tiles, length.out = length(bounds_x)-1)\n  dangle_x &lt;- sum(steps_x) -dimension\n  if (dangle_x &gt; 0) steps_x[length(steps_x)] &lt;- steps_x[length(steps_x)] - dangle_x\n  \n  list(head(bounds_x, -1), steps_x)\n}\nx_step &lt;- calc_steps(tiling$dimension[1], tiling$tiles[1])\ny_step &lt;- calc_steps(tiling$dimension[2], tiling$tiles[2])\ny_step\nsystem.time({\nfor (i in seq_along(x_step[[1]])) {\n  startx &lt;- x_step[[1]][i]\n  countx &lt;- x_step[[2]][i]\n  for (j in seq_along(y_step[[1]])) {\n  starty &lt;- y_step[[1]][j]\n  county &lt;- y_step[[2]][j]\n    \n  ## now read\n  offset &lt;- c(startx, starty)\n  dimension &lt;- c(countx, county)\n  vals &lt;- vapour:::vapour_read_raster_block(readfrom_file, offset = offset, dimension = dimension, band_output_type = info$datatype, band = 1)[[1L]]\n  \n  ## do something to the values\n  if (any(na.omit(vals) &gt; info$nodata_value)) {\n    vals &lt;- vals * -1\n    vapour:::vapour_write_raster_block(writeto_file, vals, offset, dimension, band = 1, overwrite = TRUE)\n  }\n  }\n}"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "title": "Web services for scientific data in R",
    "section": "",
    "text": "NOTE: this post has been resurrected from a 2017 post (April 2022).\nThis post is in-progress …\nTwo very important packages in R are rerddap and plotdap providing straightforward access to and visualization of time-varying gridded data sets. Traditionally this is handled by individuals who either have or quickly gain expert knowledge about the minute details regarding obtaining data sources, exploring and extracting data from them, manipulating the data for the required purpose and reporting results. Often this task is not the primary goal, it’s simply a requirement for comparison to environmental data or validation of direct measurements or modelled scenarios against independent data.\nThis is a complex area, it touches on big data, web services, complex and sophisticated multi-dimensional scientific data in array formats (primarily NetCDF), map projections, and data aesthetics or scaling methods by which data values are converted into visualizations.\nR is not known to be strong in this area for handling large and complex array-based data, although it has had good support for every piece in the chain many of them were either not designed to work together or or languished without modernization for some time. There are many many approaches to bring it all together but unfortunately no concerted effort to sort it all out. What there is however is a very exciting and productive wave of experimentation and new packages to try, there’s a lot of exploration occurring and a lot of powerful new approaches.\nROpenSci is producing a variety of valuable new R packages for scientific exploration and analysis. It and the RConsortium are both contributing directly into this ecosystem, with the latter helping to foster developments in simple features (polygons, lines and points), interactive map editing and an integration of large raster data handling."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "title": "Web services for scientific data in R",
    "section": "Cool right!?",
    "text": "Cool right!?\nThis is extremely cool, there is a lot of exciting new support for these sometimes challenging sources of data. However, there is unfortunately no concerted vision for integration of multi-dimensional data into the tidyverse and many of the projects created for data getting and data extraction must include their own internal handlers for downloading and caching data sources and converting data into required forms. This is a complex area, but in some places it is harder and more complex than it really needs to be.\nTo put some guides in this discussion, the rest of this post is informed by the following themes.\n\nthe tidyverse is not just cool, it’s totally awesome (also provides a long-term foundation for the future)\n“good software design” facilitates powerful APIs and strong useability: composable, orthogonal components and effortless exploration and workflow development\nnew abstractions are still to be found\n\nThe tidyverse is loudly loved and hated. Critics complain that they already understood long-form data frames and that constant effusive praise on twitter is really annoying. Supporters just sit agog at a constant stream of pointless shiny fashion parading before their eyes … I mean, they fall into a pit of success and never climb out. Developers who choose the tidyverse as framework for their work appreciate its seamless integration of database principles and actual databases, the consistent and systematic syntax of composable single-purpose functions with predictable return types, the modularization and abstractability of magrittr piping, and the exciting and disruptive impact of tidy evaluation seen clearly already in dplyr and family, but which will clearly make it very easy to traverse the boundary between being a user and being a developer. What could be a better environment for the future of science and research?"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "title": "Web services for scientific data in R",
    "section": "What about the rasters!",
    "text": "What about the rasters!\nI’ve been using gridded data in R since 2002. I remember clearly learning about the use of the graphics::image function for visualizing 2D kernel density maps created using sm. I have a life-long shudder reflex at the heat.colors palette, and at KDE maps generally. I also remember my first encounter with the NetCDF format which would have looked exactly like this (after waiting half and hour to download this file).\n\nprint(sst.file)\n\n[1] \"ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc\"\n\nlibrary(RNetCDF)\ncon &lt;- open.nc(file.path(dp, sst.file))\nlon &lt;- var.get.nc(con, \"lon\")\nlat &lt;- var.get.nc(con, \"lat\")\nxlim &lt;- c(140, 155)\nylim &lt;- c(-50, -35)\nxsub &lt;- lon &gt;= xlim[1] & lon &lt;= xlim[2]\nysub &lt;- lat &gt;= ylim[1] & lat &lt;= ylim[2]\ntlim  &lt;- \"oh just give up, it's too painful ...\"\ntime &lt;- var.get.nc(con, \"time\")\n## you get the idea, who can be bothered indexing time as well these days\nv &lt;- var.get.nc(con, \"sst\", start = c(which(xsub)[1], length(ysub) - max(which(ysub)), length(time)), count = c(sum(xsub), sum(ysub), 1))\nimage(lon[xsub], rev(lat[ysub]), v[nrow(v):1, ], asp = 1/cos(42 * pi/180))\n\n\n\n\n\n\n\n\nWhat a hassle! Let’s just use raster. (These aren’t the same but I really don’t care about making sure the old way works, the new way is much better - when it works, which is mostly …).\n\nlibrary(raster)\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nb &lt;- brick(file.path(dp, sst.file))\n\nLoading required namespace: ncdf4\n\nnlayers(b)\n\n[1] 1685\n\nraster(b)\n\nclass      : RasterLayer \ndimensions : 180, 360, 64800  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n\n## what time did you want?\n\nplot(crop(subset(b, nlayers(b) - c(1, 0)), extent(xlim, ylim), snap = \"out\"), col = heat.colors(12))\n\n\n\n\n\n\n\n\nThese are pretty cruddy data anyway, 1 degree resolution, weekly time steps? Come on man!\nWhy is this data set relevant? For a very long time the Optimally Interpolated Sea Surface Temperature data set, known fondly as Reynolds SST in some circles, was a very important touchstone for those working in marine animal tracking. From the earliest days of tuna tracking by (PDF): Northwest Pacific by light-level geo-locators, a regional or global data set of surface ocean temperatures was a critical comparison for tag-measured water temperatures. The strong and primarily zonal-gradients (i.e. varying by latitude, it gets cold as you move towards the poles) in the oceans provided an informative corrective to “light level geo-location” latitude estimates, especially when plagued by total zonal ambiguity (see Figure 12.3) around the equinoxes.\nToday we can use much finer resoution blended products for the entire globe. Blended means it’s a combination of measured (remote-sensing, bucket off a ship) and modelled observations, that’s been interpolated to “fill gaps”. This is not a simple topic of course, remotely sensed temperatures must consider whether it is day or night, how windy it is, the presence of sea ice, and many other factors - but as a global science community we have developed to the point of delivering a single agreed data set for this property. And now that it’s 2017, you have the chance of downloading all 5000 or so daily files, the total is only 2000 Gb.\nSo nothing’s free right? You want high-resolution, you get a big download bill."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "title": "Web services for scientific data in R",
    "section": "Web services for scientific array data",
    "text": "Web services for scientific array data\nAh, no - we don’t have to download every large data set. That’s where ERDDAP comes in!\nThis makes it easy, but I’m still not happy. In this code a raw NetCDF file is downloaded but is not readily useable by other processes, it’s not obvious how to connect directly to the source with NetCDF API, the raster data itself is turned into both a data frame, and turned into a grid ignoring irregularity in the coordinates, the raster is then resized and possibly reprojected, then turned into a polygon layer (eek) and finally delivered to the user as a very simple high level function that accepts standard grammar expressions of the tidyverse.\nWhat follows is some raw but real examples of using an in-development package tidync in the hypertidy family. It’s very much work-in-progress, as is this blog post …\nPlease reach out to discuss any of this if you are interested!\n\n#install.packages(\"rerddap\")\n#devtools::install_github(\"ropensci/plotdap\")\nlibrary(rerddap)\nlibrary(plotdap)\nlibrary(ggplot2)\nsstInfo &lt;- info('jplMURSST41')\n#system.time({  ## 26 seconds\nmurSST &lt;- griddap(sstInfo, latitude = c(22., 51.), longitude = c(-140., -105),\n                  time = c('last','last'), fields = 'analysed_sst')\n\ninfo() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap\n\n#})\nf &lt;- attr(murSST, \"path\")\n#unlink(f)\n\n## the murSST (it's a GHRSST L4  foundational SST product ) is an extremely detailed raster source, it's really the only\n## daily blended (remote sensing + model) and interpolated (no-missing values)\n## Sea Surface Temperature for global general usage that is high resolution.\n## The other daily blended product Optimally Interpolated (OISST) is only 0.25 degree resolution\n## The GHRSST product is available since 2002, whereas OISST is available since\n## 1981 (the start of the AVHRR sensor era)\nmaxpixels &lt;- 50000\ndres &lt;- c(mean(diff(sort(unique(murSST$data$lon)))), mean(diff(sort(unique(murSST$data$lat)))))\nlibrary(raster)\nr &lt;- raster(extent(range(murSST$data$lon) + c(-1, 1) * dres[1]/2, range(murSST$data$lat) + c(-1, 1) * dres[2]/2),\n     res = dres, crs = \"+init=epsg:4326\")\n\ndim(r) &lt;- dim(r)[1:2] %/% sqrt(ceiling(ncell(r) / maxpixels))\n\ndat &lt;- murSST$data %&gt;%\n mutate(bigcell = cellFromXY(r, cbind(lon, lat))) %&gt;%\n    group_by(time, bigcell) %&gt;%\n  summarize(analysed_sst = mean(analysed_sst, na.rm = FALSE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(lon = xFromCell(r, bigcell), lat = yFromCell(r, bigcell))\n\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n\nr[] &lt;- NA\nr[dat$bigcell] &lt;- dat$analysed_sst\nnames(r) &lt;- \"analysed_sst\"\ndat$bigcell &lt;- NA\n\n#m &lt;- sf::st_as_sf(maps::map(\"world\", region = \"USA\"))\nbgMap &lt;- sf::st_as_sf( maps::map('world', plot = FALSE, fill = TRUE))\n\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_raster(data = dat, aes(x = lon, y = lat, fill = analysed_sst))\n\n\n\n\n\n\n\n## now, what happened before?\n\n#system.time({p &lt;- sf::st_as_sf(raster::rasterToPolygons(r))})\n## should be a bit faster due to use of implicit coordinate mesh\nsystem.time({p &lt;- sf::st_as_sf(spex::polygonize(r, na.rm = TRUE))})\n\n   user  system elapsed \n  0.800   0.004   0.804 \n\n## plot(p, border = NA)\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_sf(data = p, aes(fill = analysed_sst), colour = \"transparent\")\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\n\n\n\n\nu &lt;- \"http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41\"\n\nlibrary(tidync)\nlibrary(dplyr)\ntnc &lt;- tidync::tidync(u)\n\nnot a file: \n' http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41 '\n\n... attempting remote connection\n\n\nConnection succeeded.\n\ntnc  ## notice there are four variables in this active space\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) &lt;dimension family&gt; : &lt;associated variables&gt; \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name    length     min    max start count    dmin   dmax unlim coord_dim \n  &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;     \n1 D0    latitu…  17999 -9.00e1 9.00e1     1 17999 -9.00e1 9.00e1 FALSE TRUE      \n2 D1    longit…  36000 -1.80e2 1.8 e2     1 36000 -1.80e2 1.8 e2 FALSE TRUE      \n3 D2    time      7266  1.02e9 1.65e9     1  7266  1.02e9 1.65e9 FALSE TRUE      \n\nhf &lt;- tnc %&gt;% hyper_filter(longitude = longitude &gt;= -140 & longitude &lt;= -105, latitude = latitude &gt;= 22 & latitude &lt;= 51,\n                       time = index == max(index))\nhf\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) &lt;dimension family&gt; : &lt;associated variables&gt; \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name   length     min    max start count    dmin    dmax unlim coord_dim \n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt;     \n1 D0    latit…  17999 -9.00e1 9.00e1 11200  2901  2.2 e1  5.1 e1 FALSE TRUE      \n2 D1    longi…  36000 -1.80e2 1.8 e2  4000  3501 -1.4 e2 -1.05e2 FALSE TRUE      \n3 D2    time     7266  1.02e9 1.65e9  7266     1  1.65e9  1.65e9 FALSE TRUE      \n\n## looking ok, so let's go for gold!\n## specify just sst, otherwise we will get all four\n## hyper_tibble gets the raw arrays with ncvar_get(conn, start = , count = ) calls\n## then expands out the axes based on the values from the filtered axis tables\nsystem.time({\n tab &lt;- hf %&gt;% hyper_tibble(select_var = \"analysed_sst\")\n})\n\n   user  system elapsed \n  3.452   2.750  23.779 \n\n# system.time({  ## 210 seconds\n#   hs &lt;- hyper_slice(hf, select_var = \"analysed_sst\")\n# })\n# hyper_index(hf)\n# nc &lt;- ncdf4::nc_open(u)\n# system.time({  ## 144 seconds\n#   l &lt;- ncdf4::ncvar_get(nc, \"analysed_sst\", start = c(4000, 2901, 5531), count = c(3501, 2901, 1))\n# })"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m &lt;- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] &lt;- 0\nm[2, 1] &lt;- -10\nm[3, 2] &lt;- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#matrix-and-image",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#matrix-and-image",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m &lt;- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] &lt;- 0\nm[2, 1] &lt;- -10\nm[3, 2] &lt;- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "title": "R matrices and image",
    "section": "Data placement with image()",
    "text": "Data placement with image()\nThis is fairly obvious, each cell is painted as a discrete block with cell centres evenly spaced between 0 and 1.\n\nm &lt;- matrix(1:12, 3)\nimage(m)\n\n\n\n\n\n\n\n\nWe didn’t give it any coordinates to position the image, so it made some up.\n\nimage(m, main = \"input coordinates are cell centres\")\nxx &lt;- seq.int(0, 1, length.out = nrow(m))\nyy &lt;- seq.int(0, 1, length.out = ncol(m))\nabline(h = yy, v = xx, lty = 2)\n\n\n\n\n\n\n\n\nThis lends itself to a convenient data structure.\n\ndat &lt;- list(x = xx, y = yy, z = m)\nimage(dat)\ntext(expand.grid(xx, yy), lab = as.vector(m))\n\n\n\n\n\n\n\n\n\n## points(expand.grid(xx, yy))\n\nThe function image() has some hidden tricks.\n\nxcorner &lt;- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner &lt;- seq.int(0, 1, length.out = ncol(m) + 1L)\nprint(xcorner)\n\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\nprint(ycorner)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\n## [1] 0.00 0.25 0.50 0.75 1.00\n\nimage(xcorner, ycorner, m, main = \"input coordinates are cell corners\")\nabline(h = ycorner, v = xcorner)\n\n\n\n\n\n\n\n\nWe can even use non-regular coordinates.\n\nycorner &lt;- 1.5^seq_along(ycorner)\nimage(xcorner, ycorner, m)\nabline(h = ycorner, v = xcorner)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "title": "R matrices and image",
    "section": "Under the hood",
    "text": "Under the hood\n\nprint(image.default)\n\nfunction (x = seq(0, 1, length.out = nrow(z)), y = seq(0, 1, \n    length.out = ncol(z)), z, zlim = range(z[is.finite(z)]), \n    xlim = range(x), ylim = range(y), col = hcl.colors(12, \"YlOrRd\", \n        rev = TRUE), add = FALSE, xaxs = \"i\", yaxs = \"i\", xlab, \n    ylab, breaks, oldstyle = FALSE, useRaster, ...) \n{\n    if (missing(z)) {\n        if (!missing(x)) {\n            if (is.list(x)) {\n                z &lt;- x$z\n                y &lt;- x$y\n                x &lt;- x$x\n            }\n            else {\n                if (is.null(dim(x))) \n                  stop(\"argument must be matrix-like\")\n                z &lt;- x\n                x &lt;- seq.int(0, 1, length.out = nrow(z))\n            }\n            if (missing(xlab)) \n                xlab &lt;- \"\"\n            if (missing(ylab)) \n                ylab &lt;- \"\"\n        }\n        else stop(\"no 'z' matrix specified\")\n    }\n    else if (is.list(x)) {\n        xn &lt;- deparse1(substitute(x))\n        if (missing(xlab)) \n            xlab &lt;- paste0(xn, \"$x\")\n        if (missing(ylab)) \n            ylab &lt;- paste0(xn, \"$y\")\n        y &lt;- x$y\n        x &lt;- x$x\n    }\n    else {\n        if (missing(xlab)) \n            xlab &lt;- if (missing(x)) \n                \"\"\n            else deparse1(substitute(x))\n        if (missing(ylab)) \n            ylab &lt;- if (missing(y)) \n                \"\"\n            else deparse1(substitute(y))\n    }\n    if (any(!is.finite(x)) || any(!is.finite(y))) \n        stop(\"'x' and 'y' values must be finite and non-missing\")\n    if (any(diff(x) &lt;= 0) || any(diff(y) &lt;= 0)) \n        stop(\"increasing 'x' and 'y' values expected\")\n    if (!is.matrix(z)) \n        stop(\"'z' must be a matrix\")\n    if (!typeof(z) %in% c(\"logical\", \"integer\", \"double\")) \n        stop(\"'z' must be numeric or logical\")\n    if (length(x) &gt; 1 && length(x) == nrow(z)) {\n        dx &lt;- 0.5 * diff(x)\n        x &lt;- c(x[1L] - dx[1L], x[-length(x)] + dx, x[length(x)] + \n            dx[length(x) - 1])\n    }\n    if (length(y) &gt; 1 && length(y) == ncol(z)) {\n        dy &lt;- 0.5 * diff(y)\n        y &lt;- c(y[1L] - dy[1L], y[-length(y)] + dy, y[length(y)] + \n            dy[length(y) - 1L])\n    }\n    if (missing(breaks)) {\n        nc &lt;- length(col)\n        if (!missing(zlim) && (any(!is.finite(zlim)) || diff(zlim) &lt; \n            0)) \n            stop(\"invalid z limits\")\n        if (diff(zlim) == 0) \n            zlim &lt;- if (zlim[1L] == 0) \n                c(-1, 1)\n            else zlim[1L] + c(-0.4, 0.4) * abs(zlim[1L])\n        z &lt;- (z - zlim[1L])/diff(zlim)\n        zi &lt;- if (oldstyle) \n            floor((nc - 1) * z + 0.5)\n        else floor((nc - 1e-05) * z + 1e-07)\n        zi[zi &lt; 0 | zi &gt;= nc] &lt;- NA\n    }\n    else {\n        if (length(breaks) != length(col) + 1) \n            stop(\"must have one more break than colour\")\n        if (any(!is.finite(breaks))) \n            stop(\"'breaks' must all be finite\")\n        if (is.unsorted(breaks)) {\n            warning(\"unsorted 'breaks' will be sorted before use\")\n            breaks &lt;- sort(breaks)\n        }\n        zi &lt;- .bincode(z, breaks, TRUE, TRUE) - 1L\n    }\n    if (!add) \n        plot(xlim, ylim, xlim = xlim, ylim = ylim, type = \"n\", \n            xaxs = xaxs, yaxs = yaxs, xlab = xlab, ylab = ylab, \n            ...)\n    if (length(x) &lt;= 1) \n        x &lt;- par(\"usr\")[1L:2]\n    if (length(y) &lt;= 1) \n        y &lt;- par(\"usr\")[3:4]\n    if (length(x) != nrow(z) + 1 || length(y) != ncol(z) + 1) \n        stop(\"dimensions of z are not length(x)(-1) times length(y)(-1)\")\n    check_irregular &lt;- function(x, y) {\n        dx &lt;- diff(x)\n        dy &lt;- diff(y)\n        (length(dx) && !isTRUE(all.equal(dx, rep(dx[1], length(dx))))) || \n            (length(dy) && !isTRUE(all.equal(dy, rep(dy[1], length(dy)))))\n    }\n    if (missing(useRaster)) {\n        useRaster &lt;- getOption(\"preferRaster\", FALSE)\n        if (useRaster && check_irregular(x, y)) \n            useRaster &lt;- FALSE\n        if (useRaster) {\n            useRaster &lt;- FALSE\n            ras &lt;- dev.capabilities(\"rasterImage\")$rasterImage\n            if (identical(ras, \"yes\")) \n                useRaster &lt;- TRUE\n            if (identical(ras, \"non-missing\")) \n                useRaster &lt;- all(!is.na(zi))\n        }\n    }\n    if (useRaster) {\n        if (check_irregular(x, y)) \n            stop(gettextf(\"%s can only be used with a regular grid\", \n                sQuote(\"useRaster = TRUE\")), domain = NA)\n        if (!is.character(col)) {\n            col &lt;- as.integer(col)\n            if (any(!is.na(col) & col &lt; 0L)) \n                stop(\"integer colors must be non-negative\")\n            col[col &lt; 1L] &lt;- NA_integer_\n            p &lt;- palette()\n            col &lt;- p[((col - 1L)%%length(p)) + 1L]\n        }\n        zc &lt;- col[zi + 1L]\n        dim(zc) &lt;- dim(z)\n        zc &lt;- t(zc)[ncol(zc):1L, , drop = FALSE]\n        rasterImage(as.raster(zc), min(x), min(y), max(x), max(y), \n            interpolate = FALSE)\n    }\n    else .External.graphics(C_image, x, y, zi, col)\n    invisible()\n}\n&lt;bytecode: 0x55b5b27527a8&gt;\n&lt;environment: namespace:graphics&gt;\n\n\nThis is like looping with rect()\n\nop &lt;- par(mfrow = c(1, 2))\n## life is hard\ncols &lt;- topo.colors(25)\nscale &lt;- round((m - min(m))/diff(range(m)) * (length(cols) - 1) + 1)\nplot(NA, type = \"n\", xlim = range(xcorner), ylim = range(ycorner), asp = 1)\nfor (i in seq_along(xcorner[-1L])) {\n    for (j in seq_along(ycorner[-1L])) {\n        rect(xleft = xcorner[i], ybottom = ycorner[j], xright = xcorner[i + \n            1L], ytop = ycorner[j + 1L], col = cols[scale[i, j]], angle = 45 * \n            (i + j)%%2, density = 20, lwd = 2)\n    }\n    \n}\n\n## life is good\nimage(list(x = xcorner, y = ycorner, z = m), col = topo.colors(25), asp = 1)\n\n\n\n\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "title": "R matrices and image",
    "section": "“Raster graphics” (not the raster package)",
    "text": "“Raster graphics” (not the raster package)\nRelatively recently native image-graphics support was added to R.\nOld style\n\nm &lt;- matrix(1:12, nrow = 3)\nxcorner &lt;- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner &lt;- seq.int(0, 1, length.out = ncol(m) + 1L)\nimage(xcorner, ycorner, m, col = topo.colors(25))"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#section",
    "href": "posts/2024-12-05_idea_update/idea-update.html#section",
    "title": "IDEA - data and software",
    "section": "",
    "text": "raadtools, software to extract maps of ocean properties and values at points-in-time"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#raadtools-10-years-old-r-package",
    "href": "posts/2024-12-05_idea_update/idea-update.html#raadtools-10-years-old-r-package",
    "title": "IDEA - data and software",
    "section": "raadtools > 10 years old R package",
    "text": "raadtools &gt; 10 years old R package\n\n\n\n\n\n\n\nR function\nPurpose\n\n\n\n\nreadsst()\nglobal sea surface temperature\n\n\nreadice()\npolar sea ice concentrations\n\n\nreadghrsst()\nhigh resolution sea surface temperature\n\n\nread_adt/ugos/vgos_daily()\nglobal altimetry, sea height, surface currents\n\n\nread_chla_daily()\nglobal ocean colour\n\n\nreadtopo()\nglobal or local bathymetry\n\n\n…\n\n\n\n\nother functions we don’t have, yet …"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#software-we-want",
    "href": "posts/2024-12-05_idea_update/idea-update.html#software-we-want",
    "title": "IDEA - data and software",
    "section": "software we want",
    "text": "software we want\n\n\n\nfeatures we want\nraadtools 🤔\n&lt;new tool&gt;\n\n\n\n\nusers don’t download files\n✅\n✅\n\n\ndata is up to date\n✅\n✅\n\n\nwe can add new data\n✅\n✅\n\n\nyou can add new data\n❌\n✅\n\n\nuse outside AAD without Mike or Ben\n❌\n✅\n\n\nuse outside of R\n❌\n✅\n\n\nscale up on super computing\n❌\n✅\n\n\nrobust to research/local outage\n❌\n✅\n\n\navailable offline on Nuyina\n❌\n??"
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#python-support",
    "href": "posts/2024-12-05_idea_update/idea-update.html#python-support",
    "title": "IDEA - data and software",
    "section": "Python support",
    "text": "Python support\nAddress entire data cubes, with one line of code e.g. daily data 1993 to November 2024\nimport xarray; &lt;some settings&gt;\n\nds = xarray.open_dataset('s3://vzarr/SEALEVEL_GLO_PHY_L4.parquet', &lt;more settings&gt;)\n&lt;xarray.Dataset&gt; Size: 574GB\nDimensions:    (time: 11538, latitude: 720, longitude: 1440)\nCoordinates:\n  * latitude   (latitude) float32 3kB -89.88 -89.62 -89.38 ... 89.38 89.62 89.88\n  * longitude  (longitude) float32 6kB -179.9 -179.6 -179.4 ... 179.6 179.9\n  * time       (time) datetime64[ns] 92kB 1993-01-01 1993-01-02 ... 2024-11-25\nData variables:\n    adt        (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    sla        (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    ugos       (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    ugosa      (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    vgos       (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\n    vgosa      (time, latitude, longitude) float64 96GB dask.array&lt;chunksize=(1, 50, 50), meta=np.ndarray&gt;\nAttributes: (12/44) ..."
  },
  {
    "objectID": "posts/2024-12-05_idea_update/idea-update.html#how-are-we-doing-this",
    "href": "posts/2024-12-05_idea_update/idea-update.html#how-are-we-doing-this",
    "title": "IDEA - data and software",
    "section": "How are we doing this",
    "text": "How are we doing this\n\nThe old and new tools reflect user-demand, tell us your ideas!\nModern tech: cloud-native and efficient public-available files\nData curation and cataloguing tools: {bowerbird}, STAC, VirtualiZarr\nExploring best-practice usage in Python xarray, odc, and in R terra, gdalraster, rsi\nContributing to software libraries GDAL.org, and community with AADC, SCAR, rOpenSci, Pangeo, Radiant Earth, Opendatacube, Digital Earth Antarctica"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hypertidy-blog",
    "section": "",
    "text": "IDEA - data and software\n\n\n\n\n\n\n\n\n\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nConservative regridding with GDAL (?)\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 11, 2024\n\n\nMichael Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nPlot at native resolution, with R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 4, 2024\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL and image tiles, the {ceramic} package\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 22, 2023\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nDegenerate Rectilinear (WIP)\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL raster read/write by blocks\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nMay 4, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL warper with R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nmesh3d - recent changes in rgl workhorse format\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nMay 29, 2019\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGDAL in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nSep 1, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nWeb services for scientific data in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 25, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nR spatial in 2017\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nJan 10, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nGIS for 3D in R\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\n\n\n\n\n\nDec 28, 2015\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\n\n\n\n\nR matrices and image\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\nApr 17, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "",
    "text": "For some time the {raadtools} and {bowerbird} packages have been used by data science admins at the Australian Antarctic Division.\nThe adminstrator experience is one of maintaining a collection of data-getter configurations that results in a local copy of heavily-used environmental data products.\nThe user experience is one of loading the raadtools package for access to functions that read time-steps from the data library of various environmental products. These products include sea surface temperature, sea ice concentration, ocean altimetry, ocean colour, bathymetry and topography, and ocean currents.\nThe user can invoke functions such as ‘readice()’, ‘readsst()’, ‘readghrsst()’, ‘read_adt_daily()’ that by default return the latest available time step, or a specific time step. We can easily find the earliest available by setting latest = FALSE.\n\nlibrary(raadtools)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\n\nglobal option 'raadfiles.data.roots' set:\n'\n\n\n/rdsi/PRIVATE/raad/data               \n /rdsi/PRIVATE/raad/data_local         \n /rdsi/PRIVATE/raad/data_staging       \n /rdsi/PRIVATE/raad/data_deprecated    \n /rdsi/PUBLIC/raad/data                \n\n\n'\n\n\nUploading raad file cache as at 2025-02-06 12:04:40 (1649115 files listed) \n\nreadice()\n\nclass      : RasterLayer \ndimensions : 332, 316, 104912  (nrow, ncol, ncell)\nresolution : 25000, 25000  (x, y)\nextent     : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\ncrs        : +proj=stere +lat_0=-90 +lat_ts=-70 +lon_0=0 +x_0=0 +y_0=0 +a=6378273 +rf=298.279411123061 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 0.4, 100  (min, max)\ntime       : 2025-02-03 \n\nreadsst()\n\nclass      : RasterLayer \ndimensions : 720, 1440, 1036800  (nrow, ncol, ncell)\nresolution : 0.25, 0.25  (x, y)\nextent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : Daily.sea.surface.temperature \nvalues     : -1.8, 32.79  (min, max)\ntime       : 2025-02-03 \n\n#readghrsst(latest = FALSE)\nread_adt_daily(latest = FALSE)\n\nclass      : RasterBrick \ndimensions : 720, 1440, 1036800, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.25, 0.25  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5009 \nmax values :                      1.6582 \ntime       : 1993-01-01 \n\nread_adt_daily(\"2025-01-30\")\n\nclass      : RasterBrick \ndimensions : 1440, 2880, 4147200, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.125, 0.125  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5703 \nmax values :                      1.9685 \ntime       : 2025-01-30 \n\n\nThese data objects are geospatial-standard raster layers, for extracting values at points, aggregating values under polygons and lines, and calculating other aggregate summaries over time. There are helpers specific to raadtools that will extract values at long,lat,time points, this is a convenience enabled by the geospatial-standard status of these layers (extent, shape, crs).\n\nnuyina$adt &lt;- extract(read_adt_daily, nuyina)\ndplyr::sample_n(nuyina[, c(\"longitude\", \"latitude\", \"adt\")], 100)\n\n# A tibble: 100 × 3\n   longitude latitude    adt\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1      148.    -43.5  0.565\n 2      129.    -53.2 -0.379\n 3      147.    -42.9 NA    \n 4      111.    -59.9 -1.12 \n 5      140.    -47.0  0.569\n 6      110.    -61.6 -1.26 \n 7      131.    -52.1 -0.222\n 8      151.    -45.4  0.575\n 9      147.    -42.9 NA    \n10      117.    -58.7 -0.874\n# ℹ 90 more rows\n\n\nJust under the hood of raadtools are file-collection lists that implicitly define a “data cube” that exists for a given variable. These look like this:\n\n(files &lt;- sstfiles(returnfiles = TRUE))\n\n# A tibble: 15,862 × 3\n   date                fullname                                            root \n   &lt;dttm&gt;              &lt;chr&gt;                                               &lt;chr&gt;\n 1 1981-09-01 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 2 1981-09-02 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 3 1981-09-03 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 4 1981-09-04 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 5 1981-09-05 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 6 1981-09-06 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 7 1981-09-07 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 8 1981-09-08 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 9 1981-09-09 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n10 1981-09-10 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n# ℹ 15,852 more rows\n\n\nThe ‘fullname’ is the local path to the file, the ‘date’ is the derived time stamp of the data layer, and there’s implicit ‘band=1’ for these where its assumed one-file is one-time-step. When that’s not true we simply copy the file name and set explicity “band=i” for whatever local position in the file the time step lives.\nThis relationship between the file set and the user-access function is informal, it’s obviously not a general solution - we’re assuming time series of 2D raster layers indexed by date, but this covers a huge amount of use cases and has been a helpful model for us.\nIn most part we can update the local root ‘/rdsi/PUBLIC/raad/data’ to ‘https://’ and that allows us to know where the source file was obtained from.\nAnyone can setup and run raadtools, but we are already in a post-download-data world. In practice it is used on curated cloud computing that we manage in a small-scale way. No on external can just install raadtools and use it. (Administrators can follow our lead, but again only a handful of people ever do this),\nAdministrators manage the data library by setting up configurations such as this one, and {bowerbird} uses this to go-get the data every day. Files are downloaded if they are not already available locally, or if they have been updated at the source. There is an ongoing slow change in most datasets as they get reprocessed, added to, or conventions change orientation, resolution, coverage in time and space.\nThe {blueant} package contains a set of pre-configured templates for data sets we use routinely.\n\nmysrc &lt;- blueant::sources(\"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\", hemisphere = \"south\",time_resolutions = \"day\")\n\nstr(mysrc)\n\ntibble [1 × 16] (S3: tbl_df/tbl/data.frame)\n $ id                 : chr \"10.5067/MPYG15WAA4WX\"\n $ name               : chr \"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\"\n $ description        : chr \"Passive microwave estimates of sea ice concentration at 25km spatial resolution. Daily and monthly resolution, \"| __truncated__\n $ doc_url            : chr \"https://nsidc.org/data/nsidc-0051/versions/2\"\n $ source_url         :List of 1\n  ..$ : chr \"https://n5eil01u.ecs.nsidc.org/PM/NSIDC-0051.002/\"\n $ citation           : chr \"DiGirolamo NE, Parkinson CL, Cavalieri DJ, Gloersen P, Zwally HJ (2022, updated yearly). Sea Ice Concentrations\"| __truncated__\n $ license            : chr \"As a condition of using these data, you must include a citation.\"\n $ comment            : chr NA\n $ method             :List of 1\n  ..$ :List of 7\n  .. ..$                        : chr \"bb_handler_earthdata\"\n  .. ..$ relative               : logi TRUE\n  .. ..$ accept_follow          : chr \"[[:digit:]]{4}\\\\.[[:digit:]]{2}\\\\.[[:digit:]]{2}/\"\n  .. ..$ accept_download        : chr \"PS_S25km.*\\\\.nc$\"\n  .. ..$ reject_download        : chr \"\\\\.(xml|png)$\"\n  .. ..$ level                  : num 2\n  .. ..$ allow_unrestricted_auth: logi TRUE\n $ postprocess        :List of 1\n  ..$ : list()\n $ authentication_note: chr \"Requires Earthdata login, see https://urs.earthdata.nasa.gov/. Note that you will also need to authorize the ap\"| __truncated__\n $ user               : chr \"\"\n $ password           : chr \"\"\n $ access_function    : chr \"raadtools::readice\"\n $ data_group         : chr \"Sea ice\"\n $ collection_size    : num 5\n\n\nEach configuration contains details about the source, where to find the source urls, how to filter for files of interest, slots for authentication, and some provenance. This can be use to set up a spidering job as well, to simply find files that are available without downloading them.\n\nlibrary(bowerbird)\n## we won't download anything\nmy_directory &lt;- tempdir()\ncf &lt;- bb_config(local_file_root = my_directory)\n\nmysrc &lt;-  bb_modify_source(mysrc, user = Sys.getenv(\"EARTHDATA_USER\"), password = Sys.getenv(\"EARTHDATA_PASS\"),\n          method = list(accept_follow = \"/(2025.02|2025.01)\", accept_download = \".*nc$\", no_host = FALSE))\n\ncf &lt;- bb_add(cf, mysrc)\nresult &lt;- bb_sync(cf, dry_run = TRUE)\n\nAs a result we have a list of available files, … (edit: leaving this as WIP for the moment).\nThis blog post shows a more complete example of setting up a mini-raadtools environment from scratch: https://ropensci.org/blog/2018/11/13/antarctic/"
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#raadtools-and-bowerbird",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#raadtools-and-bowerbird",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "",
    "text": "For some time the {raadtools} and {bowerbird} packages have been used by data science admins at the Australian Antarctic Division.\nThe adminstrator experience is one of maintaining a collection of data-getter configurations that results in a local copy of heavily-used environmental data products.\nThe user experience is one of loading the raadtools package for access to functions that read time-steps from the data library of various environmental products. These products include sea surface temperature, sea ice concentration, ocean altimetry, ocean colour, bathymetry and topography, and ocean currents.\nThe user can invoke functions such as ‘readice()’, ‘readsst()’, ‘readghrsst()’, ‘read_adt_daily()’ that by default return the latest available time step, or a specific time step. We can easily find the earliest available by setting latest = FALSE.\n\nlibrary(raadtools)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\n\nglobal option 'raadfiles.data.roots' set:\n'\n\n\n/rdsi/PRIVATE/raad/data               \n /rdsi/PRIVATE/raad/data_local         \n /rdsi/PRIVATE/raad/data_staging       \n /rdsi/PRIVATE/raad/data_deprecated    \n /rdsi/PUBLIC/raad/data                \n\n\n'\n\n\nUploading raad file cache as at 2025-02-06 12:04:40 (1649115 files listed) \n\nreadice()\n\nclass      : RasterLayer \ndimensions : 332, 316, 104912  (nrow, ncol, ncell)\nresolution : 25000, 25000  (x, y)\nextent     : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\ncrs        : +proj=stere +lat_0=-90 +lat_ts=-70 +lon_0=0 +x_0=0 +y_0=0 +a=6378273 +rf=298.279411123061 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 0.4, 100  (min, max)\ntime       : 2025-02-03 \n\nreadsst()\n\nclass      : RasterLayer \ndimensions : 720, 1440, 1036800  (nrow, ncol, ncell)\nresolution : 0.25, 0.25  (x, y)\nextent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : Daily.sea.surface.temperature \nvalues     : -1.8, 32.79  (min, max)\ntime       : 2025-02-03 \n\n#readghrsst(latest = FALSE)\nread_adt_daily(latest = FALSE)\n\nclass      : RasterBrick \ndimensions : 720, 1440, 1036800, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.25, 0.25  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5009 \nmax values :                      1.6582 \ntime       : 1993-01-01 \n\nread_adt_daily(\"2025-01-30\")\n\nclass      : RasterBrick \ndimensions : 1440, 2880, 4147200, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.125, 0.125  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5703 \nmax values :                      1.9685 \ntime       : 2025-01-30 \n\n\nThese data objects are geospatial-standard raster layers, for extracting values at points, aggregating values under polygons and lines, and calculating other aggregate summaries over time. There are helpers specific to raadtools that will extract values at long,lat,time points, this is a convenience enabled by the geospatial-standard status of these layers (extent, shape, crs).\n\nnuyina$adt &lt;- extract(read_adt_daily, nuyina)\ndplyr::sample_n(nuyina[, c(\"longitude\", \"latitude\", \"adt\")], 100)\n\n# A tibble: 100 × 3\n   longitude latitude    adt\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n 1      148.    -43.5  0.565\n 2      129.    -53.2 -0.379\n 3      147.    -42.9 NA    \n 4      111.    -59.9 -1.12 \n 5      140.    -47.0  0.569\n 6      110.    -61.6 -1.26 \n 7      131.    -52.1 -0.222\n 8      151.    -45.4  0.575\n 9      147.    -42.9 NA    \n10      117.    -58.7 -0.874\n# ℹ 90 more rows\n\n\nJust under the hood of raadtools are file-collection lists that implicitly define a “data cube” that exists for a given variable. These look like this:\n\n(files &lt;- sstfiles(returnfiles = TRUE))\n\n# A tibble: 15,862 × 3\n   date                fullname                                            root \n   &lt;dttm&gt;              &lt;chr&gt;                                               &lt;chr&gt;\n 1 1981-09-01 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 2 1981-09-02 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 3 1981-09-03 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 4 1981-09-04 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 5 1981-09-05 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 6 1981-09-06 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 7 1981-09-07 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 8 1981-09-08 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 9 1981-09-09 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n10 1981-09-10 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n# ℹ 15,852 more rows\n\n\nThe ‘fullname’ is the local path to the file, the ‘date’ is the derived time stamp of the data layer, and there’s implicit ‘band=1’ for these where its assumed one-file is one-time-step. When that’s not true we simply copy the file name and set explicity “band=i” for whatever local position in the file the time step lives.\nThis relationship between the file set and the user-access function is informal, it’s obviously not a general solution - we’re assuming time series of 2D raster layers indexed by date, but this covers a huge amount of use cases and has been a helpful model for us.\nIn most part we can update the local root ‘/rdsi/PUBLIC/raad/data’ to ‘https://’ and that allows us to know where the source file was obtained from.\nAnyone can setup and run raadtools, but we are already in a post-download-data world. In practice it is used on curated cloud computing that we manage in a small-scale way. No on external can just install raadtools and use it. (Administrators can follow our lead, but again only a handful of people ever do this),\nAdministrators manage the data library by setting up configurations such as this one, and {bowerbird} uses this to go-get the data every day. Files are downloaded if they are not already available locally, or if they have been updated at the source. There is an ongoing slow change in most datasets as they get reprocessed, added to, or conventions change orientation, resolution, coverage in time and space.\nThe {blueant} package contains a set of pre-configured templates for data sets we use routinely.\n\nmysrc &lt;- blueant::sources(\"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\", hemisphere = \"south\",time_resolutions = \"day\")\n\nstr(mysrc)\n\ntibble [1 × 16] (S3: tbl_df/tbl/data.frame)\n $ id                 : chr \"10.5067/MPYG15WAA4WX\"\n $ name               : chr \"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\"\n $ description        : chr \"Passive microwave estimates of sea ice concentration at 25km spatial resolution. Daily and monthly resolution, \"| __truncated__\n $ doc_url            : chr \"https://nsidc.org/data/nsidc-0051/versions/2\"\n $ source_url         :List of 1\n  ..$ : chr \"https://n5eil01u.ecs.nsidc.org/PM/NSIDC-0051.002/\"\n $ citation           : chr \"DiGirolamo NE, Parkinson CL, Cavalieri DJ, Gloersen P, Zwally HJ (2022, updated yearly). Sea Ice Concentrations\"| __truncated__\n $ license            : chr \"As a condition of using these data, you must include a citation.\"\n $ comment            : chr NA\n $ method             :List of 1\n  ..$ :List of 7\n  .. ..$                        : chr \"bb_handler_earthdata\"\n  .. ..$ relative               : logi TRUE\n  .. ..$ accept_follow          : chr \"[[:digit:]]{4}\\\\.[[:digit:]]{2}\\\\.[[:digit:]]{2}/\"\n  .. ..$ accept_download        : chr \"PS_S25km.*\\\\.nc$\"\n  .. ..$ reject_download        : chr \"\\\\.(xml|png)$\"\n  .. ..$ level                  : num 2\n  .. ..$ allow_unrestricted_auth: logi TRUE\n $ postprocess        :List of 1\n  ..$ : list()\n $ authentication_note: chr \"Requires Earthdata login, see https://urs.earthdata.nasa.gov/. Note that you will also need to authorize the ap\"| __truncated__\n $ user               : chr \"\"\n $ password           : chr \"\"\n $ access_function    : chr \"raadtools::readice\"\n $ data_group         : chr \"Sea ice\"\n $ collection_size    : num 5\n\n\nEach configuration contains details about the source, where to find the source urls, how to filter for files of interest, slots for authentication, and some provenance. This can be use to set up a spidering job as well, to simply find files that are available without downloading them.\n\nlibrary(bowerbird)\n## we won't download anything\nmy_directory &lt;- tempdir()\ncf &lt;- bb_config(local_file_root = my_directory)\n\nmysrc &lt;-  bb_modify_source(mysrc, user = Sys.getenv(\"EARTHDATA_USER\"), password = Sys.getenv(\"EARTHDATA_PASS\"),\n          method = list(accept_follow = \"/(2025.02|2025.01)\", accept_download = \".*nc$\", no_host = FALSE))\n\ncf &lt;- bb_add(cf, mysrc)\nresult &lt;- bb_sync(cf, dry_run = TRUE)\n\nAs a result we have a list of available files, … (edit: leaving this as WIP for the moment).\nThis blog post shows a more complete example of setting up a mini-raadtools environment from scratch: https://ropensci.org/blog/2018/11/13/antarctic/"
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#modern-perspectives",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#modern-perspectives",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "Modern perspectives",
    "text": "Modern perspectives\nWe recognize that this is a dated model and doesn’t mesh well with modern practice.\nData cubes: we now expect that “whole datasets” from the earliest time available to the very latest available can be loaded in one abstract object. This is a reasonable expectation and has been a long time coming.\nWe also expect that non-aligned data can be streamed at use-time into a standardized grid, this has been most clearly demonstrated by STAC and use of Sentinel and Landsat, but is also true for any disparate sources of data that are measured for a given area. We can define an output grid for analysis, and regrid on-the-fly using GDAL’s warp engine or tools like ESMF.\nWe want to maintain a toolkit like this for our existing and future R users, but at every point we are reviewing how much commitment we make and what changes we will actually support. We can also provide python tooling, but that is well-defined and we just enable that usage where we need."
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#a-prototype-raadtools-replacement",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#a-prototype-raadtools-replacement",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "A prototype raadtools-replacement",
    "text": "A prototype raadtools-replacement\nThe most valuable thing we have from the bowerbird/raadtools system is bowerbird itself (a huge body of knowledge for spidering and downloading from a diverse set of data providers).\n{bowerbird} is really the magic behind this system, and as adminstrators and data-accessors we will continue to use it to discover data that is not already well catalogued in modern ways …\nWhat would we update if raadtols as-is was to be upgraded?\nWe should use {terra} as the default output format. (It is currently still {raster} which was born on top of {rgdal}/{sp}, down defunct and disfavoured respectively).\nUse of more GDAL VRT. There are tasks that can be easily replaced by VRT, that include\n\naugmenting missing metadata (netcdf typically doesn’t store an explicit crs for longlat data)\nswapping hemispheres from 0,360 or -180,180 convention data\naugmenting incorrect georeferencing (GHRSST/MURSST benefits from an assigned geotransform, to fix the broken 1D coordinate arrays that aren’t sufficiently precise in Float32)\ncombining disparate layers, the two polar grids of sea ice concentration can be easily streamed into a global grid (transverse merctor, longlat)\nmany others, specific to dataset vagaries\n\nWe explored many of these options, and then developed quite a few of them into GDAL itself, if GDAL can recognized or workaround a problem data set then we can remove R code that understands them and it will also work in other programming languages. For us these included\n\nGRIB files that weren’t recognized as being in polar projections\nissues with the warper when using degenerate rectlinear coordinates\nmany new “gdal_translate” options for the “vrt://” connection that provides a general mechanism to easily inline “VRT fixes” in a single line of text.\n\nThe resource should publically available on any machine. We could do this by exposing our data library via web services, but we have concerns about permission to do that and so we are working on a case-by-case basis to determine some of these overal questions.\nIt should work as well in Python. R and Python are very different in terms of their assumed support for gridded data. They have a lot in common, but we can’t assumed GDAL-layers via terra in Python (rasterio is close functionally but it wouldn’t be sensible to equate a SpatRaster with active GDAL bindings to a rasterio object with active GDAL bindings, they are idiomatically different because of differences in how people have used those languages). This is a primary reason why we have settled on the barest-in-common features. What we can share seamlessly between R and Python is a set of data sources (files or objects) with some kind of catalogue."
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#example-of-working-with-problem-datasets",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#example-of-working-with-problem-datasets",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "Example of working with problem datasets",
    "text": "Example of working with problem datasets\nOn 20 November 2024 the 0.25 degree grids from Copernicus altimetry had finished being published. This means that there is a new dataset that is on a 0.125 degree grid, and these are now separate datasets.\nIt looks like this in raadtools:\nlibrary(raadtools)\nread_adt_daily(\"2024-11-19\")\nclass      : RasterBrick \ndimensions : 720, 1440, 1036800, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.25, 0.25  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5285 \nmax values :                      2.1569 \ntime       : 2024-11-19 \n\nread_adt_daily(\"2024-11-20\")\nclass      : RasterBrick \ndimensions : 1440, 2880, 4147200, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.125, 0.125  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.6685 \nmax values :             \nThat is a huge problem if someone tries to read a multi-day dataset with this function, but otherwise won’t cause problems for value extraction. It’s a break in the assumptions our file-handling has made though and we need to fix it.\nIf we get these files from our object storage we can do this now in a less language-specific way.\n(here we are accessing files that are otherwise available directly from Copernicus marine, but please treat this as an example of how we can increment support in our existing tools rather than as an example of “best practice” if we were to choose the best options off the shelf today)\n\nobjects &lt;- arrow::read_parquet(\"https://projects.pawsey.org.au/idea-objects/idea-curated-objects.parquet\")\n## dataset identifier we want\ndsid &lt;- \"SEALEVEL_GLO_PHY_L4\"\n\n(altimetryfiles &lt;- dplyr::filter(objects, Dataset == dsid))\n\n# A tibble: 11,610 × 4\n   Bucket                                      Key   date                Dataset\n   &lt;chr&gt;                                       &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n 1 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-01 00:00:00 SEALEV…\n 2 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-02 00:00:00 SEALEV…\n 3 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-03 00:00:00 SEALEV…\n 4 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-04 00:00:00 SEALEV…\n 5 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-05 00:00:00 SEALEV…\n 6 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-06 00:00:00 SEALEV…\n 7 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-07 00:00:00 SEALEV…\n 8 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-08 00:00:00 SEALEV…\n 9 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-09 00:00:00 SEALEV…\n10 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-10 00:00:00 SEALEV…\n# ℹ 11,600 more rows\n\n\nFirst a function to match input date/date-time/date-timestring to a set of reference dates. We won’t go into the details of this here, but this captures all the logic used in raadtools to give a consistent and reasonable set of date-layers for user input.\n\n## validation for input dates (qdate) against reference dates (fdate)\n## returns the index of qdate within fdate where fdate are the actual time steps, strictly monotonic increasing\n## this function returns an index into fdate for every qdate that is 1) valid 2) near enough to an fdate (tolerance 2x minimum interval)\n## sort and uniqueness are forced on the input, returns 0-length quietly if none are valid \nifun &lt;- function(qdate, fdate) {\n  if (length(qdate) &lt; 1 || length(fdate) &lt; 1) return(NULL)\n qdate &lt;-  try(as.POSIXct(qdate, tz = \"UTC\"), silent = TRUE)\n fdate &lt;- try(as.POSIXct(fdate, tz = \"UTC\"), silent = TRUE)\n if (inherits(qdate, \"try-error\") || inherits(fdate, \"try-error\")) return(NULL)\n mintimeres &lt;- min(diff(unclass(as.POSIXct(fdate, tz = \"UTC\"))))\n \n df &lt;- data.frame(qdate = qdate, idate = findInterval(qdate, fdate))\n df &lt;- df[!is.na(df$idate), ]\n df &lt;- df[!duplicated(df$idate), ]\n df &lt;- df[order(df$idate), ]\n idate &lt;- df$idate\n ## clamp index\n idate[idate &lt; 1] &lt;- 1\n idate[idate &gt; length(fdate)] &lt;- length(fdate)\n difdate &lt;- abs(unclass(fdate[idate]) - unclass(df$qdate))\n ## now finally kill all matches that are different by 2x the minimum interval\n bad &lt;- difdate &gt; (2 * mintimeres)\n idate[!bad]\n}\n\nNow, we can write a function to take user input and return the relevant files.\n\nget_altimetry &lt;- function(date) {\n  objects &lt;- arrow::read_parquet(\"https://projects.pawsey.org.au/idea-objects/idea-curated-objects.parquet\")\n  ## dataset identifier we want\n  dsid &lt;- \"SEALEVEL_GLO_PHY_L4\"\n\n  altimetryfiles &lt;- dplyr::filter(objects, Dataset == dsid)\n  index &lt;- ifun(date, altimetryfiles$date)\n  if (length(index) &lt; 1) stop(\"no input dates match available altimetry data\")\n  \n  altimetryfiles[index, ]\n}\n\nget_altimetry(\"2024-11-11\")\n\n# A tibble: 1 × 4\n  Bucket                               Key           date                Dataset\n  &lt;chr&gt;                                &lt;chr&gt;         &lt;dttm&gt;              &lt;chr&gt;  \n1 idea-sealevel-glo-phy-l4-nrt-008-046 data.marine.… 2024-11-11 00:00:00 SEALEV…\n\nget_altimetry(seq(as.Date(\"1994-01-01\"), as.Date(\"2024-01-01\"), by = \"1 year\"))\n\n# A tibble: 31 × 4\n   Bucket                                      Key   date                Dataset\n   &lt;chr&gt;                                       &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;  \n 1 idea-sealevel-glo-phy-l4-rep-observations-… data… 1994-01-01 00:00:00 SEALEV…\n 2 idea-sealevel-glo-phy-l4-rep-observations-… data… 1995-01-01 00:00:00 SEALEV…\n 3 idea-sealevel-glo-phy-l4-rep-observations-… data… 1996-01-01 00:00:00 SEALEV…\n 4 idea-sealevel-glo-phy-l4-rep-observations-… data… 1997-01-01 00:00:00 SEALEV…\n 5 idea-sealevel-glo-phy-l4-rep-observations-… data… 1998-01-01 00:00:00 SEALEV…\n 6 idea-sealevel-glo-phy-l4-rep-observations-… data… 1999-01-01 00:00:00 SEALEV…\n 7 idea-sealevel-glo-phy-l4-rep-observations-… data… 2000-01-01 00:00:00 SEALEV…\n 8 idea-sealevel-glo-phy-l4-rep-observations-… data… 2001-01-01 00:00:00 SEALEV…\n 9 idea-sealevel-glo-phy-l4-rep-observations-… data… 2002-01-01 00:00:00 SEALEV…\n10 idea-sealevel-glo-phy-l4-rep-observations-… data… 2003-01-01 00:00:00 SEALEV…\n# ℹ 21 more rows\n\nget_altimetry(\"2025-01-01\")$Key\n\n[1] \"data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_NRT_008_046/cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.125deg_P1D_202411/2025/01/nrt_global_allsat_phy_l4_20250101_20250104.nc\"\n\n\nWe know that files with 0.125 in them are different grids to those with 0.25deg, but as a workaround we can standardize the output to always be of the higher resolution.\nWe do this for an example with only one of the available variables, adt. (absolute dynamic topography, which is sea surface height).\n\nSys.setenv(\"AWS_S3_ENDPOINT\" = \"projects.pawsey.org.au\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\" = \"FALSE\")\nget_adt &lt;- function(date) {\n  files &lt;- get_altimetry(date)\n  sourcepath &lt;- sprintf(\"/vsis3/%s/%s\", files$Bucket, files$Key)\n  tibble::tibble(date = files$date, source = sprintf(\"vrt://%s?sd_name=adt&outsize=2880,1440\", sourcepath))\n}\nget_adt(\"2003-01-01\")$source\n\n[1] \"vrt:///vsis3/idea-sealevel-glo-phy-l4-rep-observations-008-047/data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_MY_008_047/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D_202112/2003/01/dt_global_allsat_phy_l4_20030101_20210726.nc?sd_name=adt&outsize=2880,1440\"\n\nget_adt(\"2025-01-01\")$source\n\n[1] \"vrt:///vsis3/idea-sealevel-glo-phy-l4-nrt-008-046/data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_NRT_008_046/cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.125deg_P1D_202411/2025/01/nrt_global_allsat_phy_l4_20250101_20250104.nc?sd_name=adt&outsize=2880,1440\"\n\n## with which we can now write a standard and entirely lazy reader with geospatial tools\nread_adt &lt;- function(date, grid = NULL) {\n  files &lt;- get_adt(date)\n  if (!is.null(grid)) {\n    out &lt;- terra::project(terra::rast(files$source), grid)\n  } else {\n    out &lt;- terra::rast(files$source)\n  }\n  out\n}"
  },
  {
    "objectID": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#sooty-demo",
    "href": "posts/2025-02-16_r-cloud-ready/r-cloud-ready.html#sooty-demo",
    "title": "Integrated Digital East Antarctica, software and tools",
    "section": "sooty demo",
    "text": "sooty demo\nTaking what we have back to its foundations, the set of files that we have knowledge of for each dataset is very general and cross-language. We can get the set of curated files for each dataset and then create an object-oriented interface like this in R.\n\nlibrary(S7)\ndataset &lt;- new_class(name = \"dataset\",                   \n                     properties = list(\n                       id = new_property(class = class_character, default = \"id\"),\n                       n = new_property(class = class_integer, getter = function(self) nrow(self@source)),\n                       mindate = new_property(class = class_POSIXct, getter = function(self) min(self@source$date)), \n                       maxdate = new_property(class = class_POSIXct, getter = function(self) max(self@source$date)),\n                       source = new_property(\n                         class = class_data.frame,\n                         getter = function(self) {\n                           sooty::sooty_files(curated = TRUE) |&gt; dplyr::filter(Dataset == self@id)\n                         } \n                       ))\n                     \n)\n\nd &lt;- dataset()\n\nd@id &lt;- \"NSIDC_SEAICE_PS_S25km\"\n\n\n\n\nfiles &lt;- d@source\nifun(c(\"2025-02-02\"), files$date)\n\n[1] 16896"
  }
]