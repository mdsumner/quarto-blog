[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a quarto website for the hypertidy family of R packages for multi-dimensional and spatial data.\nHypertidy is an approach to spatial or multi-dimensional data in R based on the following principles:\nExamples of these principles are seen in these R packages.\nvapour\nraadtools and angstroms\ntidync, lazyrraster,\nsilicate, anglr, rbgm"
  },
  {
    "objectID": "about.html#gridded-data",
    "href": "about.html#gridded-data",
    "title": "About",
    "section": "Gridded data",
    "text": "Gridded data\nHypertidy recognizes that not all gridded data fit into the GIS raster conventions. Gridded data comes in many forms, geographic with longitude-latitude or projected spaces, with time and or depth dimensions, with different orderings of axes (i.e. time-first, the latitude-longitude), and with generally any arbitrary space. A space is simply a set of axes with particular units and projection, and yes we mean “space” and “projection” in the more general mathematical sense. Date-time data is a projection, there is a mapping of a particular set of values to the real line and the position on that line for particular instant is defined by the axis units and epoch.\nMesh and grid share the same meaning in some contexts."
  },
  {
    "objectID": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "href": "about.html#structured-vs.-unstructured-topology-vs.-geometry",
    "title": "About",
    "section": "Structured vs. unstructured, topology vs. geometry",
    "text": "Structured vs. unstructured, topology vs. geometry\nArray-based data have a straight-forward relationship between a set of axes that have discrete steps. These are “structured grids”. Unstructured grids include triangulations, non-regularly binned histograms, tetrahedral meshes and ragged arrays.\nAn unstructured mesh (grid) is able to represent any data structure, but structured meshes have some advantages because of the regular indexing relationship between dimensions.\nGIS vector constructs “polygons”, “lines”, “points” are special case optimizations of the unstructured grid case. Polygons really are topologically identical to lines, and they are a dead-end in the broader scheme of dimensionality. Points and lines can are topologicaly 0-dimensional and 1-dimensional respectively, and this shape-constraint is the same no matter what geometric dimension they are defined in. A line can twist around a 4D space with x, y, z, t coordinates at its segment nodes or it can be constrained to single dimension with only one of those coordinates specifying its position. The topology of the line is completely independent of the geometry, if we treat the line as composed of topological primitives.\nPolygons are not composed of topological primitives, but they can be treated as being composed of line primitives."
  },
  {
    "objectID": "a.html",
    "href": "a.html",
    "title": "Untitled",
    "section": "",
    "text": "import netCDF4 as nc\nimport xarray as xr\nal = xr.open_dataset(\"nwm.t00z.medium_range.land_1.f003.conus.nc\")\n\n\nfrom pyproj import Transformer\ntransformer = Transformer.from_crs(\"OGC:CRS84\", al.crs.spatial_ref)\np1 = transformer.transform(-87.840711, 33.003379)\np2 = transformer.transform(-87.065738, 33.609833)\n\n\nal_sel = al.FSNO.sel(x = p1[0], y = p1[1], method = \"nearest\")\nal_sel.plot()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hypertidy-blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nGDAL raster read/write by blocks\n\n\n\n\n\n\ntitle: “GDAL raster read/write by blocks” author: “Michael D. Sumner” date: “2022-05-04” categories: [news, code]\n\n\n\n\n \n\n\n\n\n\n\n’ #’ https://stackoverflow.com/questions/74705976/how-to-import-sinusoidal-remotesensed-data-into-raster-in-r\n\n\n\n\n\nf <- “~/ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_SIN_PML_OCx-20070101-fv5.0.nc”\n\n\n\n\n \n\n\n\n\n\n\nmesh3d - recent changes in rgl workhorse format\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nPLEASE NOTE (April 2022): this post has been migrated from an old site, and some details may have changed. There might an update to this post to reflect the rgl package as it is now. —\n\n\n\n\n\n\nMay 29, 2019\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\nGDAL in R\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nFor some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\n\n\n\n\n\n\nSep 1, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\nWeb services for scientific data in R\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nNOTE: this post has been resurrected from a 2017 post (April 2022).\n\n\n\n\n\n\nJul 25, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\nR spatial in 2017\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nThis document is a broad overview of what I see as most relevant to future spatial, for 2017 and beyond. I’ve tried to be as broad as possible, without going into too much detail, but it’s also quite personal and opinionated.\n\n\n\n\n\n\nJan 10, 2017\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\nGIS for 3D in R\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nGIS data structures are not well suited for generalization, and visualizations and models in 3D require pretty forceful and ad hoc approaches.\n\n\n\n\n\n\nDec 28, 2015\n\n\nMichael D. Sumner\n\n\n\n\n\n\n\n\nR matrices and image\n\n\n\n\n\n\n\ncode\n\n\n\n\nIn R, matrices are ordered row-wise:\n\n\n\n\n\n\nApr 17, 2014\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "",
    "text": "PLEASE NOTE (April 2022): this post has been migrated from an old site, and some details may have changed. There might an update to this post to reflect the rgl package as it is now. —\nThis post describes the mesh3d format used in the rgl package and particularly how colour properties are stored and used. There are recent changes to this behaviour (see ‘meshColor’), and previously the situation was not clearly documented."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#rgl",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "rgl",
    "text": "rgl\nThe rgl package has long provided interactive 3D graphics for R. The neat thing for me about 3D graphics is the requirement for mesh forms of data, and the fact that meshes are extremely useful for very many tasks. When we plot data in 3D we necessarily have to convert the usual spatial types into mesh forms. You can see me discuss that in more detail in this talk."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#the-mesh3d-format",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "The mesh3d format",
    "text": "The mesh3d format\nHere is an example of a mesh3d object, it stores two polygonal areas in a form ready for 3D graphics.\n\nmesh0 <- structure(list(vb = structure(c(0, 0, 0, 1, 0, 1, 0, 1, 0.75, \n                                1, 0, 1, 1, 0.8, 0, 1, 0.5, 0.7, 0, 1, 0.8, 0.6, 0, 1, 0.69, \n                                0, 0, 1, 0.2, 0.2, 0, 1, 0.5, 0.2, 0, 1, 0.5, 0.4, 0, 1, 0.3, \n                                0.6, 0, 1, 0.2, 0.4, 0, 1, 1.1, 0.63, 0, 1, 1.23, 0.3, 0, 1), .Dim = c(4L, 14L)), \n               it = structure(c(1L, 8L, 12L, 9L, 8L, 1L, 7L, 6L, 5L, \n                                5L, 4L, 3L, 2L, 1L, 12L, 9L, 1L, 7L, 5L, 3L, 2L, 2L, 12L, 11L, \n                                10L, 9L, 7L, 5L, 2L, 11L, 10L, 7L, 5L, 5L, 11L, 10L, 6L, 7L, \n                                14L, 14L, 13L, 6L), .Dim = c(3L, 14L)), \n               primitivetype = \"triangle\", \n               material = list(), \n               normals = NULL, \n               texcoords = NULL), \n               class = c(\"mesh3d\", \"shape3d\"))\n\n\nstr(mesh0)\n\nList of 6\n $ vb           : num [1:4, 1:14] 0 0 0 1 0 1 0 1 0.75 1 ...\n $ it           : int [1:3, 1:14] 1 8 12 9 8 1 7 6 5 5 ...\n $ primitivetype: chr \"triangle\"\n $ material     : list()\n $ normals      : NULL\n $ texcoords    : NULL\n - attr(*, \"class\")= chr [1:2] \"mesh3d\" \"shape3d\"\n\n\n(It’s not obvious about the polygons, please bear with me).\nThe following characterizes the structure.\n\ntwo matrix arrays vb and it\nvb has 4 rows and 14 columns, and contains floating point numbers\nit has 3 rows and 14 columns, and contains integers (starting at 1)\na primitivetype which is “triangle”\nan empty list of material propertes (this is the missing link for the polygons)\na NULL value for normals and texcoords, these won’t be discussed further (but see ?quadmesh::quadmesh for texture coordinates from spatial)\na class, this object is a mesh3d and inherits from shape3d\n\nThe vb array is the vertices, these are the corner coordinates of the elements of the mesh.\n\nplot(t(mesh0$vb), main = \"t(vb) - vertices\", xlab = \"X\", ylab = \"Y\")\n\n\n\n\nThe elements of this mesh are triangles, and these are specified by the index array it. Elements of a mesh are called primitives, hence the primitivetype here.\n\nplot(t(mesh0$vb), main = \"t(vb[, it]) - primitives\", xlab = \"X\", ylab = \"Y\")\npolygon(t(mesh0$vb[, rbind(mesh0$it, NA)]), col = rgb(0.6, 0.6, 0.6, 0.5))"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#transpose",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Transpose",
    "text": "Transpose\nThese matrix arrays are transpose the way we usually use them in R, for now just remember that you must t()ranspose them for normal plotting, e.g. plot(t(mesh0$vb[1:2, ])) will give the expected scatter plot of the vertices. The reason these arrays are transpose is because each coordinate value is then contiguous in memory, each Y value is right next to its counterpart X, and Z (and W), and vb[it, ] provides a flat vector of XYZW values in a continuous block - this is a very important efficiency, and help explains why computer graphics use elements in a mesh form like this."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#colours",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Colours",
    "text": "Colours\nUnsurprisingly, if we set the material property to a constant we get a constant colour.\n\nwidgetfun <- function() {\n  view3d(0, phi = 8)\n  rglwidget()\n}\nmesh0$material$color <- \"red\"\nlibrary(rgl)\nclear3d()\nshade3d(mesh0, lit = FALSE); \nwidgetfun()\n\n\n\n\n\nIn the usual R way our singleton colour value is magically recycled across every part of the shape, and it’s all red. But, is it recycled by vertices or by primitive? Until recently it was only possible to tell by trying (or reading the source code).\nHere I think it’s easy to see that the two colours are specified at the vertices, and they bleed across each triangle accordingly. We also get a warning that the behaviour has recently changed.\n\nclear3d()\nmesh0$material$color <- c(\"firebrick\", \"black\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0)\nwidgetfun()\n\n\n\n\n\nThe default is to meshColor = \"vertices\", so let’s specify faces.\n\nclear3d()\nmesh0$material$color <- c(\"firebrick\", \"dodgerblue\")\nmaterial3d(lit = FALSE)\nshade3d(mesh0, meshColor = \"faces\")\nwidgetfun()\n\n\n\n\n\nSometimes we get neighbouring triangles with the same colour, so let’s also add the edges.\n\nmesh0$vb[3, ] <- 0.01  ## vertical bias avoids z-fighting\n## material properties here override the recycling of internal colours\n## onto edges\nwire3d(mesh0, lwd = 5, color = \"black\")\nwidgetfun()\n\n\n\n\n\nIf we go a bit further we can see the original arrangement for this shape, two individual polygons that share a single edge.\nThis only works because I happen to know how this was created, and I know how this control of behaviour occurs in new rgl.\nThere are 12 triangles in the first polygon, and 2 in the second. (The original polygons can be seen here (left panel)).\n\nclear3d()\nmesh0$material$color <- rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2))\nshade3d(mesh0, meshColor = \"faces\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nIf we treat the colours as applying to each vertex, then we needed to propagate it to each vertex around each face (triangle), and this is what rgl now calls legacy behaviour.\n\nclear3d()\nmesh0$material$color <- rep(rep(c(\"firebrick\", \"dodgerblue\"), c(12, 2)), each = 3)\nshade3d(mesh0, meshColor = \"legacy\", lit = FALSE)\nwidgetfun()\n\n\n\n\n\nWe cannot recreate this effect with meshColor = \"vertices\", because each of our vertices is actually unique. (It could be done by making the vb array every repeated vertex, and updating the index array but I can’t summon this up atm).\n\nclear3d()\nmesh0$material$color <- rep_len(c(\"firebrick\", \"dodgerblue\"), length.out = ncol(mesh0$vb))\nshade3d(mesh0, meshColor = \"vertices\", lit = FALSE)\nwidgetfun()"
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#primitives",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Primitives",
    "text": "Primitives\nThe other kind of element supported by mesh3d is a quad, specified by an ib array with 4 rows (ib versus it, 4 vertices versus 3) and the primitivetype = \"quad\".\nThe it values are an index into, i.e. the column number of the vertex array. The vertices, or coordinates, are stored by column in this structure, whereas normally we would store a coordinate per row.\nWhen I first explored mesh3d I was looking at a quad type mesh - and I was completely confused. Both vb and ib had four rows, and so while I understood that a quad must have 4 vertices (4 index values for every primitive), I did not understand why the vertices also had four rows.\n(There are other kinds of primitives in common use are edge, point, tetrahedron - but rgl has no formal class for these - in practice the edge type is referred to as segment in rgl, and tetrahedra are approximated by enclosing their shape with triangles)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#why-does-the-vertex-array-have-4-rows",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "Why does the vertex array have 4 rows?",
    "text": "Why does the vertex array have 4 rows?\nAll mesh3d objects have a vb array, and it always includes 4 rows.\nThe reason there are 4 rows in the vertex array is that these are homogeneous coordinates which …\n\nare ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied\n\n… yeah. For our purposes just think\n\nX, Y, Z in the usual sense and set W = 1.\n\n(Do not set W = 0 because your data will vanish to infinity when plotted with rgl, which is what those math folks are saying more or less)."
  },
  {
    "objectID": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "href": "posts/2019-05-29_recent-rgl-format-changes/index.html#quads",
    "title": "mesh3d - recent changes in rgl workhorse format",
    "section": "QUADS",
    "text": "QUADS\nNow let’s get a quad type mesh from the real world.\n\n## remotes::install_github(\"hypertidy/ceramic\")\nlibrary(ceramic)\ntopo <- cc_elevation(raster::extent(-72, -69, -34, -32), zoom = 6)\n\nPreparing to download: 1 tiles at zoom = 6 from \nhttps://api.mapbox.com/v4/mapbox.terrain-rgb/\n\nqm <- quadmesh::quadmesh(topo)\n\nstr(qm)\n\nList of 8\n $ vb             : num [1:4, 1:60225] -8015493 -3761925 0 1 -8014270 ...\n  ..- attr(*, \"dimnames\")=List of 2\n  .. ..$ : chr [1:4] \"x\" \"y\" \"z\" \"1\"\n  .. ..$ : NULL\n $ ib             : int [1:4, 1:59732] 1 2 277 276 2 3 278 277 3 4 ...\n $ primitivetype  : chr \"quad\"\n $ material       : list()\n $ normals        : NULL\n $ texcoords      : NULL\n $ raster_metadata:List of 7\n  ..$ xmn  : num -8015493\n  ..$ xmx  : num -7680393\n  ..$ ymn  : num -4028537\n  ..$ ymx  : num -3761925\n  ..$ ncols: int 274\n  ..$ nrows: int 218\n  ..$ crs  : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n $ crs            : chr \"+proj=merc +lon_0=0 +k=1 +x_0=0 +y_0=0 +R=6378137 +units=m +no_defs\"\n - attr(*, \"class\")= chr [1:3] \"quadmesh\" \"mesh3d\" \"shape3d\"\n\n\nThis topographic raster from near Santiago is now a mesh3d subclassed to quadmesh. This adds two properties raster_metadata and crs, which under limited conditions allows reconstruction of the original raster data. To drop back to a generic mesh3d the easiest is to reproject the data.\n\n##remotes::install_github(\"hypertidy/reproj\")\nlibrary(reproj)\nqm_ll <- reproj(qm, \"+proj=longlat +datum=WGS84\")\n\nWarning in reproj.quadmesh(qm, \"+proj=longlat +datum=WGS84\"): quadmesh raster\ninformation cannot be preserved after reprojection, dropping to mesh3d class\n\n\nThis is a lossless reprojection, as it is equivalent to sf::sf_project(t(qm$vb[1:2, ]), from = qm$crs, to = \"+proj=longlat +datum=WGS84\") or with rgdal::project(, qm$crs, inv = TRUE).\nWe can plot this in the usual way with rgl, or see upcoming features in the mapdeck package.\n\nclear3d()\nshade3d(qm_ll, lit = TRUE, col = \"grey\")\naspect3d(1, 1, 0.1); \nview3d(0, phi = -60)\nrglwidget()\n\n\n\n\n\nTo put colours on this, we can do it by faces\n\nclear3d()\nqm_ll$material$color <- colourvalues::color_values(raster::values(topo))\nshade3d(qm_ll, meshColor = \"faces\", lit = TRUE)\nrglwidget()\n\n\n\n\n\n(each face is discretely coloured), or by vertex in the legacy mode.\nNot run, to save the size of the document.\n\nclear3d()\nqm_ll$material$color <-colourvalues::color_values(qm_ll$vb[3, qm_ll$ib])\n                                                   \nshade3d(qm_ll, meshColor = \"legacy\", lit = TRUE)\nrglwidget()"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html",
    "title": "Web services for scientific data in R",
    "section": "",
    "text": "NOTE: this post has been resurrected from a 2017 post (April 2022).\nThis post is in-progress …\nTwo very important packages in R are rerddap and plotdap providing straightforward access to and visualization of time-varying gridded data sets. Traditionally this is handled by individuals who either have or quickly gain expert knowledge about the minute details regarding obtaining data sources, exploring and extracting data from them, manipulating the data for the required purpose and reporting results. Often this task is not the primary goal, it’s simply a requirement for comparison to environmental data or validation of direct measurements or modelled scenarios against independent data.\nThis is a complex area, it touches on big data, web services, complex and sophisticated multi-dimensional scientific data in array formats (primarily NetCDF), map projections, and data aesthetics or scaling methods by which data values are converted into visualizations.\nR is not known to be strong in this area for handling large and complex array-based data, although it has had good support for every piece in the chain many of them were either not designed to work together or or languished without modernization for some time. There are many many approaches to bring it all together but unfortunately no concerted effort to sort it all out. What there is however is a very exciting and productive wave of experimentation and new packages to try, there’s a lot of exploration occurring and a lot of powerful new approaches.\nROpenSci is producing a variety of valuable new R packages for scientific exploration and analysis. It and the RConsortium are both contributing directly into this ecosystem, with the latter helping to foster developments in simple features (polygons, lines and points), interactive map editing and an integration of large raster data handling."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#cool-right",
    "title": "Web services for scientific data in R",
    "section": "Cool right!?",
    "text": "Cool right!?\nThis is extremely cool, there is a lot of exciting new support for these sometimes challenging sources of data. However, there is unfortunately no concerted vision for integration of multi-dimensional data into the tidyverse and many of the projects created for data getting and data extraction must include their own internal handlers for downloading and caching data sources and converting data into required forms. This is a complex area, but in some places it is harder and more complex than it really needs to be.\nTo put some guides in this discussion, the rest of this post is informed by the following themes.\n\nthe tidyverse is not just cool, it’s totally awesome (also provides a long-term foundation for the future)\n“good software design” facilitates powerful APIs and strong useability: composable, orthogonal components and effortless exploration and workflow development\nnew abstractions are still to be found\n\nThe tidyverse is loudly loved and hated. Critics complain that they already understood long-form data frames and that constant effusive praise on twitter is really annoying. Supporters just sit agog at a constant stream of pointless shiny fashion parading before their eyes … I mean, they fall into a pit of success and never climb out. Developers who choose the tidyverse as framework for their work appreciate its seamless integration of database principles and actual databases, the consistent and systematic syntax of composable single-purpose functions with predictable return types, the modularization and abstractability of magrittr piping, and the exciting and disruptive impact of tidy evaluation seen clearly already in dplyr and family, but which will clearly make it very easy to traverse the boundary between being a user and being a developer. What could be a better environment for the future of science and research?"
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#what-about-the-rasters",
    "title": "Web services for scientific data in R",
    "section": "What about the rasters!",
    "text": "What about the rasters!\nI’ve been using gridded data in R since 2002. I remember clearly learning about the use of the graphics::image function for visualizing 2D kernel density maps created using sm. I have a life-long shudder reflex at the heat.colors palette, and at KDE maps generally. I also remember my first encounter with the NetCDF format which would have looked exactly like this (after waiting half and hour to download this file).\n\nprint(sst.file)\n\n[1] \"ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc\"\n\nlibrary(RNetCDF)\ncon <- open.nc(file.path(dp, sst.file))\nlon <- var.get.nc(con, \"lon\")\nlat <- var.get.nc(con, \"lat\")\nxlim <- c(140, 155)\nylim <- c(-50, -35)\nxsub <- lon >= xlim[1] & lon <= xlim[2]\nysub <- lat >= ylim[1] & lat <= ylim[2]\ntlim  <- \"oh just give up, it's too painful ...\"\ntime <- var.get.nc(con, \"time\")\n## you get the idea, who can be bothered indexing time as well these days\nv <- var.get.nc(con, \"sst\", start = c(which(xsub)[1], length(ysub) - max(which(ysub)), length(time)), count = c(sum(xsub), sum(ysub), 1))\nimage(lon[xsub], rev(lat[ysub]), v[nrow(v):1, ], asp = 1/cos(42 * pi/180))\n\n\n\n\nWhat a hassle! Let’s just use raster. (These aren’t the same but I really don’t care about making sure the old way works, the new way is much better - when it works, which is mostly …).\n\nlibrary(raster)\n\nLoading required package: sp\n\n\n\nAttaching package: 'raster'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nb <- brick(file.path(dp, sst.file))\n\nLoading required namespace: ncdf4\n\nnlayers(b)\n\n[1] 1685\n\nraster(b)\n\nclass      : RasterLayer \ndimensions : 180, 360, 64800  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n\n## what time did you want?\n\nplot(crop(subset(b, nlayers(b) - c(1, 0)), extent(xlim, ylim), snap = \"out\"), col = heat.colors(12))\n\n\n\n\nThese are pretty cruddy data anyway, 1 degree resolution, weekly time steps? Come on man!\nWhy is this data set relevant? For a very long time the Optimally Interpolated Sea Surface Temperature data set, known fondly as Reynolds SST in some circles, was a very important touchstone for those working in marine animal tracking. From the earliest days of tuna tracking by (PDF): Northwest Pacific by light-level geo-locators, a regional or global data set of surface ocean temperatures was a critical comparison for tag-measured water temperatures. The strong and primarily zonal-gradients (i.e. varying by latitude, it gets cold as you move towards the poles) in the oceans provided an informative corrective to “light level geo-location” latitude estimates, especially when plagued by total zonal ambiguity (see Figure 12.3) around the equinoxes.\nToday we can use much finer resoution blended products for the entire globe. Blended means it’s a combination of measured (remote-sensing, bucket off a ship) and modelled observations, that’s been interpolated to “fill gaps”. This is not a simple topic of course, remotely sensed temperatures must consider whether it is day or night, how windy it is, the presence of sea ice, and many other factors - but as a global science community we have developed to the point of delivering a single agreed data set for this property. And now that it’s 2017, you have the chance of downloading all 5000 or so daily files, the total is only 2000 Gb.\nSo nothing’s free right? You want high-resolution, you get a big download bill."
  },
  {
    "objectID": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "href": "posts/2017-07-25_erddap-in-sf-and-ggplot2/index.html#web-services-for-scientific-array-data",
    "title": "Web services for scientific data in R",
    "section": "Web services for scientific array data",
    "text": "Web services for scientific array data\nAh, no - we don’t have to download every large data set. That’s where ERDDAP comes in!\nThis makes it easy, but I’m still not happy. In this code a raw NetCDF file is downloaded but is not readily useable by other processes, it’s not obvious how to connect directly to the source with NetCDF API, the raster data itself is turned into both a data frame, and turned into a grid ignoring irregularity in the coordinates, the raster is then resized and possibly reprojected, then turned into a polygon layer (eek) and finally delivered to the user as a very simple high level function that accepts standard grammar expressions of the tidyverse.\nWhat follows is some raw but real examples of using an in-development package tidync in the hypertidy family. It’s very much work-in-progress, as is this blog post …\nPlease reach out to discuss any of this if you are interested!\n\n#install.packages(\"rerddap\")\n#devtools::install_github(\"ropensci/plotdap\")\nlibrary(rerddap)\nlibrary(plotdap)\nlibrary(ggplot2)\nsstInfo <- info('jplMURSST41')\n#system.time({  ## 26 seconds\nmurSST <- griddap(sstInfo, latitude = c(22., 51.), longitude = c(-140., -105),\n                  time = c('last','last'), fields = 'analysed_sst')\n\ninfo() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap\n\n#})\nf <- attr(murSST, \"path\")\n#unlink(f)\n\n## the murSST (it's a GHRSST L4  foundational SST product ) is an extremely detailed raster source, it's really the only\n## daily blended (remote sensing + model) and interpolated (no-missing values)\n## Sea Surface Temperature for global general usage that is high resolution.\n## The other daily blended product Optimally Interpolated (OISST) is only 0.25 degree resolution\n## The GHRSST product is available since 2002, whereas OISST is available since\n## 1981 (the start of the AVHRR sensor era)\nmaxpixels <- 50000\ndres <- c(mean(diff(sort(unique(murSST$data$lon)))), mean(diff(sort(unique(murSST$data$lat)))))\nlibrary(raster)\nr <- raster(extent(range(murSST$data$lon) + c(-1, 1) * dres[1]/2, range(murSST$data$lat) + c(-1, 1) * dres[2]/2),\n     res = dres, crs = \"+init=epsg:4326\")\n\ndim(r) <- dim(r)[1:2] %/% sqrt(ceiling(ncell(r) / maxpixels))\n\ndat <- murSST$data %>%\n mutate(bigcell = cellFromXY(r, cbind(lon, lat))) %>%\n    group_by(time, bigcell) %>%\n  summarize(analysed_sst = mean(analysed_sst, na.rm = FALSE)) %>%\n  ungroup() %>%\n  mutate(lon = xFromCell(r, bigcell), lat = yFromCell(r, bigcell))\n\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n\nr[] <- NA\nr[dat$bigcell] <- dat$analysed_sst\nnames(r) <- \"analysed_sst\"\ndat$bigcell <- NA\n\n#m <- sf::st_as_sf(maps::map(\"world\", region = \"USA\"))\nbgMap <- sf::st_as_sf( maps::map('world', plot = FALSE, fill = TRUE))\n\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_raster(data = dat, aes(x = lon, y = lat, fill = analysed_sst))\n\n\n\n## now, what happened before?\n\n#system.time({p <- sf::st_as_sf(raster::rasterToPolygons(r))})\n## should be a bit faster due to use of implicit coordinate mesh\nsystem.time({p <- sf::st_as_sf(spex::polygonize(r, na.rm = TRUE))})\n\n   user  system elapsed \n  0.800   0.004   0.804 \n\n## plot(p, border = NA)\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_sf(data = p, aes(fill = analysed_sst), colour = \"transparent\")\n\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n\n\n\n\n\n\nu <- \"http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41\"\n\nlibrary(tidync)\nlibrary(dplyr)\ntnc <- tidync::tidync(u)\n\nnot a file: \n' http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41 '\n\n... attempting remote connection\n\n\nConnection succeeded.\n\ntnc  ## notice there are four variables in this active space\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name    length     min    max start count    dmin   dmax unlim coord_dim \n  <chr> <chr>    <dbl>   <dbl>  <dbl> <int> <int>   <dbl>  <dbl> <lgl> <lgl>     \n1 D0    latitu…  17999 -9.00e1 9.00e1     1 17999 -9.00e1 9.00e1 FALSE TRUE      \n2 D1    longit…  36000 -1.80e2 1.8 e2     1 36000 -1.80e2 1.8 e2 FALSE TRUE      \n3 D2    time      7266  1.02e9 1.65e9     1  7266  1.02e9 1.65e9 FALSE TRUE      \n\nhf <- tnc %>% hyper_filter(longitude = longitude >= -140 & longitude <= -105, latitude = latitude >= 22 & latitude <= 51,\n                       time = index == max(index))\nhf\n\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.708106e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name   length     min    max start count    dmin    dmax unlim coord_dim \n  <chr> <chr>   <dbl>   <dbl>  <dbl> <int> <int>   <dbl>   <dbl> <lgl> <lgl>     \n1 D0    latit…  17999 -9.00e1 9.00e1 11200  2901  2.2 e1  5.1 e1 FALSE TRUE      \n2 D1    longi…  36000 -1.80e2 1.8 e2  4000  3501 -1.4 e2 -1.05e2 FALSE TRUE      \n3 D2    time     7266  1.02e9 1.65e9  7266     1  1.65e9  1.65e9 FALSE TRUE      \n\n## looking ok, so let's go for gold!\n## specify just sst, otherwise we will get all four\n## hyper_tibble gets the raw arrays with ncvar_get(conn, start = , count = ) calls\n## then expands out the axes based on the values from the filtered axis tables\nsystem.time({\n tab <- hf %>% hyper_tibble(select_var = \"analysed_sst\")\n})\n\n   user  system elapsed \n  3.452   2.750  23.779 \n\n# system.time({  ## 210 seconds\n#   hs <- hyper_slice(hf, select_var = \"analysed_sst\")\n# })\n# hyper_index(hf)\n# nc <- ncdf4::nc_open(u)\n# system.time({  ## 144 seconds\n#   l <- ncdf4::ncvar_get(nc, \"analysed_sst\", start = c(4000, 2901, 5531), count = c(3501, 2901, 1))\n# })"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html",
    "href": "posts/2015-12-28_gis3d/index.html",
    "title": "GIS for 3D in R",
    "section": "",
    "text": "GIS data structures are not well suited for generalization, and visualizations and models in 3D require pretty forceful and ad hoc approaches.\nHere I describe a simple example, showing several ways of visualizing a simple polygon data set. I use the programming environment R for the data manipulation and the creation of this document via several extensions (packages) to base R."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "href": "posts/2015-12-28_gis3d/index.html#polygon-layer",
    "title": "GIS for 3D in R",
    "section": "Polygon “layer”",
    "text": "Polygon “layer”\nThe R package maptools contains an in-built data set called wrld_simpl, which is a basic (and out of date) set of polygons describing the land masses of the world by country. This code loads the data set and plots it with a basic grey-scale scheme for individual countries.\n\nlibrary(maptools)\ndata(wrld_simpl)\nprint(wrld_simpl)\n\nclass       : SpatialPolygonsDataFrame \nfeatures    : 246 \nextent      : -180, 180, -90, 83.57027  (xmin, xmax, ymin, ymax)\nvariables   : 11\n# A tibble: 246 × 11\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   <fct> <fct> <fct> <int> <fct>    <int>   <dbl>  <int>     <int>   <dbl> <dbl>\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows\n\nplot(wrld_simpl, col = grey(sample(seq(0, 1, length = nrow(wrld_simpl)))))\n\n\n\n\nWe also include a print statement to get a description of the data set, this is a SpatialPolygonsDataFrame which is basically a table of attributes with one row for each country, linked to a recursive data structure holding sets of arrays of coordinates for each individual piece of these complex polygons.\nThese structures are quite complicated, involving nested lists of matrices with X-Y coordinates. I can use class coercion from polygons, to lines, then to points as the most straightforward way of obtaining every XY coordinate by dropping the recursive hierarchy structure to get at every single vertex in one matrix.\n\nallcoords <- coordinates(as(as(wrld_simpl, \"SpatialLines\"), \"SpatialPoints\"))\ndim(allcoords)\n\n[1] 26264     2\n\nhead(allcoords)  ## print top few rows\n\n     coords.x1 coords.x2\n[1,] -61.68667  17.02444\n[2,] -61.88722  17.10527\n[3,] -61.79445  17.16333\n[4,] -61.68667  17.02444\n[5,] -61.72917  17.60861\n[6,] -61.85306  17.58305\n\n\n(There are other methods to obtain all coordinates while retaining information about the country objects and their component “pieces”, but I’m ignoring that for now.)\nWe need to put these “X/Y” coordinates in 3D so I simply add another column filled with zeroes.\n\nallcoords <- cbind(allcoords, 0)\nhead(allcoords)\n\n     coords.x1 coords.x2  \n[1,] -61.68667  17.02444 0\n[2,] -61.88722  17.10527 0\n[3,] -61.79445  17.16333 0\n[4,] -61.68667  17.02444 0\n[5,] -61.72917  17.60861 0\n[6,] -61.85306  17.58305 0\n\n\n(Note for non-R users: in R expressions that don’t include assignment to an object with <- are generally just a side-effect, here the side effect of the head(allcoords) here is to print the top few rows of allcoords, just for illustration, there’s no other consequence of this code)."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "href": "posts/2015-12-28_gis3d/index.html#opengl-in-r",
    "title": "GIS for 3D in R",
    "section": "OpenGL in R",
    "text": "OpenGL in R\nIn R we have access to 3D visualizations in OpenGL via the rgl package, but the model for data representation is very different so I first plot the vertices of the wrld_simpl layer as points only.\n\nlibrary(rgl)\nplot3d(allcoords, xlab = \"\", ylab = \"\") ## smart enough to treat 3-columns as X,Y,Z\nrglwidget()\n\n\n\n\n\nPlotting in the plane is one thing, but more striking is to convert the vertices from planar longitude-latitude to Cartesizan XYZ. Define an R function to take “longitude-latitude-height” and return spherical coordinates (we can leave WGS84 for another day).\n\nllh2xyz <- \nfunction (lonlatheight, rad = 6378137, exag = 1) \n{\n    cosLat = cos(lonlatheight[, 2] * pi/180)\n    sinLat = sin(lonlatheight[, 2] * pi/180)\n    cosLon = cos(lonlatheight[, 1] * pi/180)\n    sinLon = sin(lonlatheight[, 1] * pi/180)\n    rad <- (exag * lonlatheight[, 3] + rad)\n    x = rad * cosLat * cosLon\n    y = rad * cosLat * sinLon\n    z = rad * sinLat\n    cbind(x, y, z)\n}\n\n## deploy our custom function on the longitude-latitude values\nxyzcoords <- llh2xyz(allcoords)\n\nNow we can visualize these XYZ coordinates in a more natural setting, and even add a blue sphere for visual effect.\n\nplot3d(xyzcoords, xlab = \"\", ylab = \"\")\nspheres3d(0, 0, 0, radius = 6370000, col = \"lightblue\")\nrglwidget()\n\n\n\n\n\nThis is still not very exciting, since our plot knows nothing about the connectivity between vertices."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "href": "posts/2015-12-28_gis3d/index.html#organization-of-polygons",
    "title": "GIS for 3D in R",
    "section": "Organization of polygons",
    "text": "Organization of polygons\nThe in-development R package gris provides a way to represent spatial objects as a set of relational tables. I’m leaving out the details because it’s not the point I want to make, but in short a gris object has tables “o” (objects), “b” (for branches), “bXv” (links between branches and vertices) and “v” the vertices.\nIf we ingest the wrld_simpl layer we get a list with several tables.\nEDITOR NOTE (April 2022): see anglr function DEL0() for an updated way to create what gris was doing in 2015.\n\nlibrary(gris)  ## devtools::install_github(\"r-gris/gris\")\nlibrary(dplyr)\ngobject <- gris(wrld_simpl)\n\nThe objects, these are individual countries with several attributes including the NAME.\n\ngobject$o\n\n# A tibble: 246 × 12\n   FIPS  ISO2  ISO3     UN NAME      AREA POP2005 REGION SUBREGION     LON   LAT\n   <fct> <fct> <fct> <int> <fct>    <int>   <dbl>  <int>     <int>   <dbl> <dbl>\n 1 AC    AG    ATG      28 Antigu…     44  8.30e4     19        29  -61.8   17.1\n 2 AG    DZ    DZA      12 Algeria 238174  3.29e7      2        15    2.63  28.2\n 3 AJ    AZ    AZE      31 Azerba…   8260  8.35e6    142       145   47.4   40.4\n 4 AL    AL    ALB       8 Albania   2740  3.15e6    150        39   20.1   41.1\n 5 AM    AM    ARM      51 Armenia   2820  3.02e6    142       145   44.6   40.5\n 6 AO    AO    AGO      24 Angola  124670  1.61e7      2        17   17.5  -12.3\n 7 AQ    AS    ASM      16 Americ…     20  6.41e4      9        61 -171.   -14.3\n 8 AR    AR    ARG      32 Argent… 273669  3.87e7     19         5  -65.2  -35.4\n 9 AS    AU    AUS      36 Austra… 768230  2.03e7      9        53  136.   -25.0\n10 BA    BH    BHR      48 Bahrain     71  7.25e5    142       145   50.6   26.0\n# … with 236 more rows, and 1 more variable: object_ <int>\n\n\nThe branches, these are individual simple, one-piece “ring polygons”. Every object may have one or more branches (branches may be an “island” or a “hole” but this is not currently recorded). Note how branch 1 and 2 (branch_) both belong to object 1, but branch 3 is the only piece of object 2.\n\ngobject$b\n\n# A tibble: 3,768 × 3\n   object_ branch_ island_\n     <int>   <int> <lgl>  \n 1       1       1 TRUE   \n 2       1       2 TRUE   \n 3       2       3 TRUE   \n 4       3       4 TRUE   \n 5       3       5 TRUE   \n 6       3       6 TRUE   \n 7       3       7 TRUE   \n 8       3       8 TRUE   \n 9       4       9 TRUE   \n10       5      10 TRUE   \n# … with 3,758 more rows\n\nplot(gobject[1, ], col = \"#333333\")\ntitle(gobject$o$NAME[1])\n\n\n\nplot(gobject[2, ], col = \"#909090\")\ntitle(gobject$o$NAME[2])\n\n\n\n\n(Antigua and Barbuda sadly don’t get a particularly good picture here, but this is not the point of the story.)\nThe links between branches and vertices.\n\ngobject$bXv\n\n# A tibble: 26,264 × 3\n   branch_ order_ vertex_\n     <int>  <int>   <int>\n 1       1      1    5589\n 2       1      2    5620\n 3       1      3    5605\n 4       1      4    5589\n 5       2      1    5596\n 6       2      2    5611\n 7       2      3    5613\n 8       2      4    5596\n 9       3      1   16101\n10       3      2   17581\n# … with 26,254 more rows\n\n\nThis table is required so that we can normalize the vertices by removing any duplicates based on X/Y pairs. This is required for the triangulation engine used below, although not by the visualization strictly. (Note that we could also normalize branches for objects, since multiple objects might use the same branch - but again off-topic).\nFinally, the vertices themselves. Here we only have X and Y, but these table structures can hold any number of attributes and of many types.\n\ngobject$v\n\n# A tibble: 21,165 × 3\n       x_    y_ vertex_\n    <dbl> <dbl>   <int>\n 1 -61.7   17.0    5589\n 2 -61.9   17.1    5620\n 3 -61.8   17.2    5605\n 4 -61.7   17.6    5596\n 5 -61.9   17.6    5611\n 6 -61.9   17.7    5613\n 7   2.96  36.8   16101\n 8   4.79  36.9   17581\n 9   5.33  36.6   18103\n10   6.40  37.1   18790\n# … with 21,155 more rows\n\n\nThe normalization is only relevant for particular choices of vertices, so if we had X/Y/Z in use there might be a different version of “unique”. I think this is a key point for flexibility, some of these tasks must be done on-demand and some ahead of time.\nIndices here are numeric, but there’s actually no reason that they couldn’t be character or other identifier. Under the hood the dplyr package is in use for doing straightforward (and fast!) table manipulations including joins between tables and filtering on values."
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "href": "posts/2015-12-28_gis3d/index.html#more-3d-already",
    "title": "GIS for 3D in R",
    "section": "More 3D already!",
    "text": "More 3D already!\nWhy go to all this effort just for a few polygons? The structure of the gris objects gives us much more flexibility, so I can for example store the XYZ Cartesian coordinates right on the same data set. I don’t need to recursively visit nested objects, it’s just a straightforward calculation and update - although we’re only making a simple point, this could be generalized a lot more for user code.\n\ngobject$v$zlonlat <- 0\ndo_xyz <- function(table) {\n  xyz <- llh2xyz(dplyr::select(table, x_, y_, zlonlat))\n  table$X <- xyz[,1]\n  table$Y <- xyz[,2]\n  table$Z <- xyz[,3]\n  table\n}\n\ngobject$v <- do_xyz(gobject$v)\n\ngobject$v\n\n# A tibble: 21,165 × 7\n       x_    y_ vertex_ zlonlat        X         Y        Z\n    <dbl> <dbl>   <int>   <dbl>    <dbl>     <dbl>    <dbl>\n 1 -61.7   17.0    5589       0 2892546. -5369047. 1867388.\n 2 -61.9   17.1    5620       0 2872491. -5376810. 1875991.\n 3 -61.8   17.2    5605       0 2880293. -5370474. 1882167.\n 4 -61.7   17.6    5596       0 2879394. -5354145. 1929470.\n 5 -61.9   17.6    5611       0 2868217. -5361116. 1926758.\n 6 -61.9   17.7    5613       0 2864423. -5358522. 1939577.\n 7   2.96  36.8   16101       0 5100196.   264042. 3820852.\n 8   4.79  36.9   17581       0 5083067.   425571. 3829093.\n 9   5.33  36.6   18103       0 5095693.   475230. 3806402.\n10   6.40  37.1   18790       0 5056321.   567008. 3846135.\n# … with 21,155 more rows\n\n\nI now have XYZ coordinates for my data set, and so for example I will extract out a few nearby countries and plot them.\n\nlocalarea <- gobject[gobject$o$NAME %in% c(\"Australia\", \"New Zealand\"), ]\n## plot in traditional 2d\nplot(localarea, col = c(\"dodgerblue\", \"firebrick\"))\n\n\n\n\nThe plot is a bit crazy since parts of NZ that are over the 180 meridian skews everything, and we could fix that easily by modifiying the vertex values for longitude, but it’s more sensible in 3D.\n\nrgl::plot3d(as.matrix(localarea$v[c(\"X\", \"Y\", \"Z\")]), xlab = \"\", ylab = \"\")\nrglwidget()\n\n\n\n\n\nFinally, to get to the entire point of this discussion let’s triangulate the polygons and make a nice plot of the world.\nThe R package RTriangle wraps Jonathan Shewchuk’s Triangle library, allowing constrained Delaunay triangulations. To run this we need to make a Planar Straight Line Graph from the polygons, but this is fairly straightforward by tracing through paired vertices in the data set. The key parts of the PSLG are the vertices P and the segment indexes S defining paired vertices for each line segment. This is a “structural” index where the index values are bound to the actual size and shape of the vertices, as opposed to a more general but perhaps less efficient relational index.\n\npslgraph <- gris:::mkpslg(gobject)\ndim(pslgraph$P)\n\n[1] 21165     2\n\nrange(pslgraph$S)\n\n[1]     1 21165\n\nhead(pslgraph$P)\n\n            x_       y_\n[1,] -61.68667 17.02444\n[2,] -61.88722 17.10527\n[3,] -61.79445 17.16333\n[4,] -61.72917 17.60861\n[5,] -61.85306 17.58305\n[6,] -61.87306 17.70389\n\nhead(pslgraph$S)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    2    3\n[3,]    3    1\n[4,]    1    1\n[5,]    4    5\n[6,]    5    6\n\n\nThe PSLG is what we need for the triangulation.\n\ntri <- RTriangle::triangulate(pslgraph)\n\nThe triangulation vertices (long-lat) can be converted to XYZ, and plotted.\n\nxyz <- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   5 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\naspect3d(\"iso\"); bg3d(\"grey12\")\nrglwidget()\n\n\n\n\n\nThese are very ugly polygons since there’s no internal vertices to carry the curvature of this sphere. This is the same problem we’d face if we tried to drape these polygons over topography: at some point we need internal structure.\nLuckily Triangle can set a minimum triangle size. We set a constant minimum area, which means no individual triangle can be larger in area than so many “square degrees”. This gives a lot more internal structure so the polygons are more elegantly draped around the surface of the sphere. (There’s not really enough internal structure added with this minimum area, but I’ve kept it simpler to make the size of this document more manageable).\n\ntri <- RTriangle::triangulate(pslgraph, a = 9)  ## a (area) is in degrees, same as our vertices\nxyz <- llh2xyz(cbind(tri$P, 0))\nopen3d()\n\nnull \n   6 \n\ntriangles3d(xyz[t(tri$T), ], col = \"grey\", specular = \"black\")\nbg3d(\"gray12\")\nrglwidget()\n\n\n\n\n\nWe still can’t identify individual polygons as we smashed that information after putting the polygon boundary segments through the triangulator. With more careful work we could build a set of tables to store particular triangles between our vertices and objects, but to finish this story I just loop over each object adding them to the scene.\n\n## loop over objects\ncols <- sample(grey(seq(0, 1, length = nrow(gobject$o))))\nopen3d()\n\nnull \n   7 \n\nfor (iobj in seq(nrow(gobject$o))) {\n  pslgraph <- gris:::mkpslg(gobject[iobj, ])\n  tri <- RTriangle::triangulate(pslgraph, a = 9)  ## a is in units of degrees, same as our vertices\n  xyz <- llh2xyz(cbind(tri$P, 0))\n  triangles3d(xyz[t(tri$T), ], col = cols[iobj], specular = \"black\")\n}\nbg3d(\"gray12\")\nrglwidget()"
  },
  {
    "objectID": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "href": "posts/2015-12-28_gis3d/index.html#real-world-topography-image-textures",
    "title": "GIS for 3D in R",
    "section": "Real world topography, Image textures",
    "text": "Real world topography, Image textures\nfuture work …\nSee anglr package"
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html",
    "href": "posts/2017-09-01_gdal-in-r/index.html",
    "title": "GDAL in R",
    "section": "",
    "text": "For some time I have used GDAL as a standard tool in my kit, I was introduced to the concept by the rgdal package authors and it slowly dawned on me what it meant to have a geo-spatial data abstraction library. To realize what this meant I had spent a lot of time in R, reading (primarily) MapInfo TAB and MIF format files as well (of course) as shapefiles, and the occasional GeoTIFF.\nI already knew how immensely powerful R was, with its epic flexibity and useability and I could just sense there was a brighter way once I understood many more details. As my experience grew I was able to do amazing tasks like, merge a few shapefiles together into one, or plot a window of data from a georeferenced grid. Previously the best I’d done in this space was VBScript in Manifold GIS, which I could use to automate some data tasks - but the prospects of full automation from raw data files to output, end-to-end with a software tool that anyone could use was absolutely mind-blowing. I was super-powered, I remember earning a carton of beer from a colleague of my father, for munging some SHP or TAB files between AGD66 and GDA94 … or something, and I knew I had a bright future ahead.\nSo what’s the abstraction? GDAL does not care what format the data is in, it could be points, lines, areas, a raster DEM, a time series of remote sensing, or an actual image. It just doesn’t mind, there’s an interpretation in its model for what’s in the file (or data base, or web server) and it will deliver that interpretation to you, very efficiently. If you understand that intepretation you can do a whole lot of amazing stuff. When this works well it’s because the tasks are modular, you have a series of basic tools designed to work together, and it’s up to you as a developer or a user to chain together the pieces to solve your particular problem."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "href": "posts/2017-09-01_gdal-in-r/index.html#where-does-this-get-difficult",
    "title": "GDAL in R",
    "section": "Where does this get difficult?",
    "text": "Where does this get difficult?\nGDAL is a C++ library, and that’s not accessible to most users or developers. The other key user-interface is the set of command line utilities, these are called gdal_translate, gdalinfo, ogr2ogr, ogrinfo, and many others. The command line is also not that accessible to many users, though it’s more so than C++ - this is why command line is a key topic for Software Carpentry. These interfaces give very high fidelity to the native interpretation provided by the GDAL model.\nGDAL is used from many languages, there’s Python, R, Perl, C#, Fortran, and it is bundled into many, many softwares - a very long list. The original author wrote code for some of the most influential geo-spatial software the world has, and some of that is in GDAL, some is locked up forever in propietary forms. He saw this as a problem and very early on engineered the work to be able to be open, in the do anything with me, including privatize me-license called MIT. Have a look in the source code for gdalwarp, you’ll see the company who was the best at raster reprojection in the late 1990s and early 2000s.\nPython is surely the closest other language to the native interpretation, but then it’s not that simple, and this is not that story …\nR has a very particular interpretation of the GDAL interpretation, it’s called rgdal and if you are familiar with the GDAL model and with R you can see a very clear extra layer there. This extra level is there partly because of when it was done, the goals of the authors, the community response to the amazingly powerful facilities it provided, but also and perhaps mainly because it was very hard. R’s rather peculiar API meant that in the early 2000s the authors had to write in another language, a language between the native GDAL C++ and the R user language - this is the R API, it’s full of SEXP and UNPROTECTs and if you search this issue you’ll see clear signals not to bother - now you should just get on with learning Rcpp.\nThese extra levels are there in the R API, the hard stuff down in the rgdal/src/ folder but also in the R code itself. There’s a bunch of rigorous rules applied there, things to help protect us from that lower level, and to save from making serious analytical mistakes. All of this was very hard work and very well-intended, but it’s clear that it takes us away from the magic of the GDAL abstraction, we have a contract with rgdal, the R code has a contract with the R API, and the rgdal/src has a contract with GDAL. All of these things divorce R users and developers from the original schemes that GDAL provides, because at the R level rgdal itself has to provide certain guarantees and contracts with both R and with R users. I think that is too much for one package.\nAdd to this the complex zoo of formats, the other libraries that GDAL requires for full use. The Windows rgdal on CRAN doesn’t include HDF4, or HDF5, or NetCDF, or DODS - there are many missing things in this list, and it’s not clear if it’s because it’s hard, it’s against the license (ECW might be tricky, MrSID most definitely would be), or because no one has asked or maybe no one knows how, or maybe CRAN doesn’t want to. (Would you know how to find out?) All of these things add up to being way too much for one package. It’s kind of impossible, though now there are many more eyes on the problem and progress is being made. Who should decide these things? How would anyone know it’s even an issue?\nI wonder if many of us see rgdal as the definition of the GDAL abstraction. I see pretty clearly the difference, and while the package has been extremely useful for me I’ve long wanted lower level control and access to the GDAL core itself. (I had rather influential guidance from extremely expert programmers I’ve worked with, and I’ve discussed GDAL with many others, including employees of various companies, and across various projects and with many users. I assume most R users don’t know much about the details, and why would they want to?).\nThere is active work to modernize rgdal, and you should be aware of the immensely successful sf and the soon to be stars project. sf is an R interpretation of the Simple Features Standard (GDAL has an interpretation of that standard too, sf starts there when it reads with GDAL). stars will start with GDAL as a model for gridded data, and it’s not yet fleshed out what the details of that will be. None of these interpretations are permanent, though while the simple features standard is unlikely to change, there is no doubt that GDAL will evolve and include more features that don’t involve that standard. These things do change and very few people, relatively, are engaged in the decisions that are made. (GDAL could definitely benefit from more input, also something I’ve long wanted to do more of).\nIt’s been a long time, but I’ve recently found a way over the key obstacle I had - building a package to compile for use in R with bindings to GDAL itself. I have a lot to thank Roger Bivand and Edzer Pebesma for many years of instruction and guidance in that, in many different ways. I also am extremely grateful to R-Core for the overall environment, and the tireless work done by the CRAN maintainers. I have to mention Jeroen Ooms and Mark Padgham who’ve been extremely helpful very recently. This is something I’ve wanted to be able to do for a really long time, I hope this post helps provide context to why, and I hope it encourages some more interest in the general topic."
  },
  {
    "objectID": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "href": "posts/2017-09-01_gdal-in-r/index.html#vapour",
    "title": "GDAL in R",
    "section": "vapour",
    "text": "vapour\nMy response to the interpretation layers is vapour, this is my version of a very minimal interpretation of the core GDAL facilities. There’s a function to read the attribute data from geometric features, a function to read the geometry data as raw binary, or various text formats, a function to read only the bounding box of each feature, and there’s a function to read the raw pixel values from a local window within a gridded data set.\nNone of this is new, we have R packages that do these things and the vapour functions will have bugs, and will need maintenance, and maybe no one but me will ever use them. I’ve needed them and I’ve started to learn a whole lot more about what I’m going to do next with them. I recommend that any R user with an interest in geo-spatial or GDAL facilities have a closer look at how they work - and if you know there’s a lower level below the interpretations provide in R you should explore them. Rcpp and the modern tools for R really do make this immensely more easy than in the past (RStudio has intellisense for C++ …).\nI also believe strongly that R is well-placed to write the future of multi-dimensional and hierarchical and complex structured and geo-spatial data. Do you know what that future should look like?"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html",
    "title": "R matrices and image",
    "section": "",
    "text": "In R, matrices are ordered row-wise:\n\n(m <- matrix(1:12, nrow = 3, ncol = 4))\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n\nThe image() function presents this as the transpose of what we see printed.\n\nm[] <- 0\nm[2, 1] <- -10\nm[3, 2] <- 30\nm\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]  -10    0    0    0\n[3,]    0   30    0    0\n\n\n\nt(m[, ncol(m):1])\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0   30\n[4,]    0  -10    0\n\n\n\n… Notice that image interprets the z matrix as a table of f(x[i], y[j]) values, so that the x axis corresponds to row number and the y axis to column number, with column 1 at the bottom, i.e. a 90 degree counter-clockwise rotation of the conventional printed layout of a matrix. …\n\n\nimage(m)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#data-placement-with-image",
    "title": "R matrices and image",
    "section": "Data placement with image()",
    "text": "Data placement with image()\nThis is fairly obvious, each cell is painted as a discrete block with cell centres evenly spaced between 0 and 1.\n\nm <- matrix(1:12, 3)\nimage(m)\n\n\n\n\nWe didn’t give it any coordinates to position the image, so it made some up.\n\nimage(m, main = \"input coordinates are cell centres\")\nxx <- seq.int(0, 1, length.out = nrow(m))\nyy <- seq.int(0, 1, length.out = ncol(m))\nabline(h = yy, v = xx, lty = 2)\n\n\n\n\nThis lends itself to a convenient data structure.\n\ndat <- list(x = xx, y = yy, z = m)\nimage(dat)\ntext(expand.grid(xx, yy), lab = as.vector(m))\n\n\n\n\n\n## points(expand.grid(xx, yy))\n\nThe function image() has some hidden tricks.\n\nxcorner <- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner <- seq.int(0, 1, length.out = ncol(m) + 1L)\nprint(xcorner)\n\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\nprint(ycorner)\n\n[1] 0.00 0.25 0.50 0.75 1.00\n\n\n## [1] 0.00 0.25 0.50 0.75 1.00\n\nimage(xcorner, ycorner, m, main = \"input coordinates are cell corners\")\nabline(h = ycorner, v = xcorner)\n\n\n\n\nWe can even use non-regular coordinates.\n\nycorner <- 1.5^seq_along(ycorner)\nimage(xcorner, ycorner, m)\nabline(h = ycorner, v = xcorner)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#under-the-hood",
    "title": "R matrices and image",
    "section": "Under the hood",
    "text": "Under the hood\n\nprint(image.default)\n\nfunction (x = seq(0, 1, length.out = nrow(z)), y = seq(0, 1, \n    length.out = ncol(z)), z, zlim = range(z[is.finite(z)]), \n    xlim = range(x), ylim = range(y), col = hcl.colors(12, \"YlOrRd\", \n        rev = TRUE), add = FALSE, xaxs = \"i\", yaxs = \"i\", xlab, \n    ylab, breaks, oldstyle = FALSE, useRaster, ...) \n{\n    if (missing(z)) {\n        if (!missing(x)) {\n            if (is.list(x)) {\n                z <- x$z\n                y <- x$y\n                x <- x$x\n            }\n            else {\n                if (is.null(dim(x))) \n                  stop(\"argument must be matrix-like\")\n                z <- x\n                x <- seq.int(0, 1, length.out = nrow(z))\n            }\n            if (missing(xlab)) \n                xlab <- \"\"\n            if (missing(ylab)) \n                ylab <- \"\"\n        }\n        else stop(\"no 'z' matrix specified\")\n    }\n    else if (is.list(x)) {\n        xn <- deparse1(substitute(x))\n        if (missing(xlab)) \n            xlab <- paste0(xn, \"$x\")\n        if (missing(ylab)) \n            ylab <- paste0(xn, \"$y\")\n        y <- x$y\n        x <- x$x\n    }\n    else {\n        if (missing(xlab)) \n            xlab <- if (missing(x)) \n                \"\"\n            else deparse1(substitute(x))\n        if (missing(ylab)) \n            ylab <- if (missing(y)) \n                \"\"\n            else deparse1(substitute(y))\n    }\n    if (any(!is.finite(x)) || any(!is.finite(y))) \n        stop(\"'x' and 'y' values must be finite and non-missing\")\n    if (any(diff(x) <= 0) || any(diff(y) <= 0)) \n        stop(\"increasing 'x' and 'y' values expected\")\n    if (!is.matrix(z)) \n        stop(\"'z' must be a matrix\")\n    if (!typeof(z) %in% c(\"logical\", \"integer\", \"double\")) \n        stop(\"'z' must be numeric or logical\")\n    if (length(x) > 1 && length(x) == nrow(z)) {\n        dx <- 0.5 * diff(x)\n        x <- c(x[1L] - dx[1L], x[-length(x)] + dx, x[length(x)] + \n            dx[length(x) - 1])\n    }\n    if (length(y) > 1 && length(y) == ncol(z)) {\n        dy <- 0.5 * diff(y)\n        y <- c(y[1L] - dy[1L], y[-length(y)] + dy, y[length(y)] + \n            dy[length(y) - 1L])\n    }\n    if (missing(breaks)) {\n        nc <- length(col)\n        if (!missing(zlim) && (any(!is.finite(zlim)) || diff(zlim) < \n            0)) \n            stop(\"invalid z limits\")\n        if (diff(zlim) == 0) \n            zlim <- if (zlim[1L] == 0) \n                c(-1, 1)\n            else zlim[1L] + c(-0.4, 0.4) * abs(zlim[1L])\n        z <- (z - zlim[1L])/diff(zlim)\n        zi <- if (oldstyle) \n            floor((nc - 1) * z + 0.5)\n        else floor((nc - 1e-05) * z + 1e-07)\n        zi[zi < 0 | zi >= nc] <- NA\n    }\n    else {\n        if (length(breaks) != length(col) + 1) \n            stop(\"must have one more break than colour\")\n        if (any(!is.finite(breaks))) \n            stop(\"'breaks' must all be finite\")\n        if (is.unsorted(breaks)) {\n            warning(\"unsorted 'breaks' will be sorted before use\")\n            breaks <- sort(breaks)\n        }\n        zi <- .bincode(z, breaks, TRUE, TRUE) - 1L\n    }\n    if (!add) \n        plot(xlim, ylim, xlim = xlim, ylim = ylim, type = \"n\", \n            xaxs = xaxs, yaxs = yaxs, xlab = xlab, ylab = ylab, \n            ...)\n    if (length(x) <= 1) \n        x <- par(\"usr\")[1L:2]\n    if (length(y) <= 1) \n        y <- par(\"usr\")[3:4]\n    if (length(x) != nrow(z) + 1 || length(y) != ncol(z) + 1) \n        stop(\"dimensions of z are not length(x)(-1) times length(y)(-1)\")\n    check_irregular <- function(x, y) {\n        dx <- diff(x)\n        dy <- diff(y)\n        (length(dx) && !isTRUE(all.equal(dx, rep(dx[1], length(dx))))) || \n            (length(dy) && !isTRUE(all.equal(dy, rep(dy[1], length(dy)))))\n    }\n    if (missing(useRaster)) {\n        useRaster <- getOption(\"preferRaster\", FALSE)\n        if (useRaster && check_irregular(x, y)) \n            useRaster <- FALSE\n        if (useRaster) {\n            useRaster <- FALSE\n            ras <- dev.capabilities(\"rasterImage\")$rasterImage\n            if (identical(ras, \"yes\")) \n                useRaster <- TRUE\n            if (identical(ras, \"non-missing\")) \n                useRaster <- all(!is.na(zi))\n        }\n    }\n    if (useRaster) {\n        if (check_irregular(x, y)) \n            stop(gettextf(\"%s can only be used with a regular grid\", \n                sQuote(\"useRaster = TRUE\")), domain = NA)\n        if (!is.character(col)) {\n            col <- as.integer(col)\n            if (any(!is.na(col) & col < 0L)) \n                stop(\"integer colors must be non-negative\")\n            col[col < 1L] <- NA_integer_\n            p <- palette()\n            col <- p[((col - 1L)%%length(p)) + 1L]\n        }\n        zc <- col[zi + 1L]\n        dim(zc) <- dim(z)\n        zc <- t(zc)[ncol(zc):1L, , drop = FALSE]\n        rasterImage(as.raster(zc), min(x), min(y), max(x), max(y), \n            interpolate = FALSE)\n    }\n    else .External.graphics(C_image, x, y, zi, col)\n    invisible()\n}\n<bytecode: 0x55b5b27527a8>\n<environment: namespace:graphics>\n\n\nThis is like looping with rect()\n\nop <- par(mfrow = c(1, 2))\n## life is hard\ncols <- topo.colors(25)\nscale <- round((m - min(m))/diff(range(m)) * (length(cols) - 1) + 1)\nplot(NA, type = \"n\", xlim = range(xcorner), ylim = range(ycorner), asp = 1)\nfor (i in seq_along(xcorner[-1L])) {\n    for (j in seq_along(ycorner[-1L])) {\n        rect(xleft = xcorner[i], ybottom = ycorner[j], xright = xcorner[i + \n            1L], ytop = ycorner[j + 1L], col = cols[scale[i, j]], angle = 45 * \n            (i + j)%%2, density = 20, lwd = 2)\n    }\n    \n}\n\n## life is good\nimage(list(x = xcorner, y = ycorner, z = m), col = topo.colors(25), asp = 1)\n\n\n\n\n\npar(op)"
  },
  {
    "objectID": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "href": "posts/2014-04-17_r-matrices-and-image/index.html#raster-graphics-not-the-raster-package",
    "title": "R matrices and image",
    "section": "“Raster graphics” (not the raster package)",
    "text": "“Raster graphics” (not the raster package)\nRelatively recently native image-graphics support was added to R.\nOld style\n\nm <- matrix(1:12, nrow = 3)\nxcorner <- seq.int(0, 1, length.out = nrow(m) + 1L)\nycorner <- seq.int(0, 1, length.out = ncol(m) + 1L)\nimage(xcorner, ycorner, m, col = topo.colors(25))"
  },
  {
    "objectID": "posts/2022-12-07_L3bin-chlor_a/readme.html",
    "href": "posts/2022-12-07_L3bin-chlor_a/readme.html",
    "title": "hypertidy-blog",
    "section": "",
    "text": "f <- “~/ESACCI-OC-L3S-CHLOR_A-MERGED-1D_DAILY_4km_SIN_PML_OCx-20070101-fv5.0.nc”\nlibrary(tidync)\n##\n## the bins, this is every single bin for this sinusoidal grid\nbins <- tidync::tidync(f) |> activate(\"D1\") |>\n hyper_tibble()\n\n\nminbin <- bins |> dplyr::filter(lat >=40) |> summarize(bin = min(bin_index)) |> dplyr::pull(bin)\n## the bin is for joining on which bin_index is used here\nd <- tidync::tidync(f) |> activate(\"D1,D0\") |>\n  hyper_filter(bin_index = bin_index >= minbin) |> \n  hyper_tibble(select_var = c(\"chlor_a\")) |> \n  dplyr::inner_join(bins, \"bin_index\")\n  \nd$time <- NULL\n\nplot(d$lon, d$lat, pch = \".\", col = palr::chl_pal(d$chlor_a), asp = 1/cos(mean(d$lat) * pi/180))\n  \n\n## define a raster\nlibrary(terra)\nr <- terra::rast(terra::ext(c(-180, 180, min(d$lat), max(d$lat))), nrows = 90, ncols = 180, crs = \"OGC:CRS84\")\n\ncells <- tibble::tibble(cell = terra::cellFromXY(r, cbind(d$lon, d$lat)), chlor_a = d$chlor_a) |> \n  group_by(cell) |> summarize(chlor_a = mean(chlor_a))\nr[cells$cell] <- cells$chlor_a\npal <- palr::chl_pal(palette = TRUE)\nimage(r, col = pal$cols[-1], breaks = pal$breaks)"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html",
    "href": "posts/2017-01-10_r-spatial-2017/index.html",
    "title": "R spatial in 2017",
    "section": "",
    "text": "This document is a broad overview of what I see as most relevant to future spatial, for 2017 and beyond. I’ve tried to be as broad as possible, without going into too much detail, but it’s also quite personal and opinionated."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-state-of-things",
    "title": "R spatial in 2017",
    "section": "The state of things",
    "text": "The state of things\nAn enormous amount of activity has been going on in R spatial. The keystone activity is the new simple features for R package sf, and the many responses to “supporting sf” in various packages but there are many other non-obvious linkages.\nPersonally, I have learnt quite a lot recently about the broader context within R and I’m keen to help consolidate some of the “non-central” tools that we use. There are also some surprisingly helpful implications of the new simple features support both for within the central package, and for the ecosystem around it.\n\nsimple features for R via the sf package\nupcoming replacement for raster\npoint clouds\nmapview and leaflet\nsimple features in other packages\n“exotic types” in sf\nspatial data that simple features cannot support\nhtmlwidgets and plotly"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#sf",
    "title": "R spatial in 2017",
    "section": "sf",
    "text": "sf\nThe simple features package sf was one of the first supported projects of the RConsortium and has been created by Edzer Pebesma. This package replaces sp vector data completely, includes a replacement for rgdal and rgeos and there is a long list of important improvements. These are described in full in the package vignettes and blog posts.\nThe key changes relevant here are\n\nno S4 classes, everything is S3 and so is more immediately manipulable/accessible\nmuch better methods support overall, for printing/summary etc.\nthe key objects are list-vectors, and data frames\nsupport for simple features standard, no more ambiguity for multi-polygons/holes, mixed types, NULL geometry, XYZ-M support\nnative support for dplyr verbs, tibbles (some is WIP but group_by/summarize reprojection and geometric union are good examples that already work)\nreprojection and geometry manipulation and file format support is now GDAL 2.0 and GEOS(?), modernized, more reliable, easier\n\nI strongly recommend getting familiar with the sf data structures, it’s really important to understand the hierarchy levels and the ways the vectors (POINT) and matrices (everything else) are stored. POINT is a vector, MULTIPOINT is a matrix, POLYGON is a list of matrices (one island, zero or more holes), LINESTRING is a matrix (same as m-point), MULTIPOLYGON is a list of lists of matrices (a list of POLYGONs, effectively), and MULTILINESTRING is a list of matrices (a list of LINESTRINGs, effectively and structurally the same as a POLYGON).\nIf you want to see how to convert between sf forms and extract the raw coordinates I would look at st_cast in, and the leaflet approach here:\nhttps://github.com/rstudio/leaflet/blob/master/R/normalize-sf.R"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#rasters",
    "title": "R spatial in 2017",
    "section": "rasters",
    "text": "rasters\nThe raster package is apparently being significantly upgraded and Edzer has plans for this as well.\nhttp://r-spatial.org/r/2016/09/26/future.html\nThere are some very interesting extensions to raster on CRAN recently, notably velox, fasteraster, and the unrelated but very nice dggridR.\nHDF5 is now fully supported by rhdf5 on Bioconductor, this could also replace many of the NetCDF4 formats supported by ncdf4, and notably can be used to read NetCDF4 files with compound types."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#point-clouds",
    "title": "R spatial in 2017",
    "section": "Point clouds",
    "text": "Point clouds\nA recent contribution on CRAN is rlas and lidR for the LiDaR LAS format, and algorithms for working with point clouds. It is trivial to push these data into sf types, but it won’t always make sense to do so. You could have a MULTIPOINT with X, Y, Z, and M (but none of the other point attributes) or a point (XYZ) with all the attributes in one sf data frame. This package will be useful for driving interest in the exotic sf types.\nIt’s easy and readily doable right now to read LIDAR data with rlas, and plot it interactively with RGB styling and so in with plotly. Keen to try writing a “detect ground” algorithm? Try it! What does st_triangulate do with a XYZ multipoint? (hmm, it fails noisily - you aren’t supposed to do that)."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#mapview-and-leaflet",
    "title": "R spatial in 2017",
    "section": "mapview and leaflet",
    "text": "mapview and leaflet\nmapview has support from RConsortium to bring user interaction to spatial in R. Currently building in support for sf, which will be accelerated by the recent dev upgrades in leaflet itself and will eventually support the range of sf types, including full support for MULTIPOLYGON.\nleaflet now has a huge number of new extensions thanks to leaflet.extras, and there is ongoing updates around integrating crosstalk which will be very importatnt for interactive map applications."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#simple-features-in-other-packages",
    "title": "R spatial in 2017",
    "section": "simple features in other packages",
    "text": "simple features in other packages\ntmap, mapview, spbabel, stplanr, …. all have internal versions of sf types converted to something else. It’s an interesting time for these packages that extend the sp and sf structures, and there are opportunities to ensure that best practices are being used."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#exotic-types-in-sf",
    "title": "R spatial in 2017",
    "section": "Exotic types in sf",
    "text": "Exotic types in sf\nThese are TINs, Polyhedral Surfaces (multipatch - basically polygons with shared “internal” edges), curves and various combinations and varieties of these. None of the triangulations use an indexed mesh, which makes them a bit clunky and probably only for very bespoke uses, but they provide interesting territory to explore. Certainly you can use them to build 3D plots in rgl and plotly (show examples, thanks to @timelyportfolio).\nNote that a GEOMETRYCOLLECTION of triangle POLYGONs is effectively the same as a simple features TIN, it doesn’t really add any structure improvement to the way the thing is put together:\nhttps://github.com/r-gris/sfct\nWe can bend the limits of simple features with “mesh” techniques, and the plotly packge provides easy publishing of interactive 3D visualizations of these.\nplotly is already useable for many applications, we can use techniques from rangl to put sf data into it, and we can use that to easily create exotic triangulated surfaces that are pretty inefficient in the simple features form:\nAlso check out Geotiff.js and timevis.\nHere are some rough examples with plotly.\nhttp://rpubs.com/cyclemumner/rangl-poly-topo-plotly\nhttp://rpubs.com/cyclemumner/raster-quads-to-triangles\nhttp://rpubs.com/cyclemumner/rangl-plotly\nI particularly want texture mapping to go with this kind of plotting, but apparently plotly cannot do that.\nhttps://twitter.com/mathinpython/status/818500905905561600"
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#the-limits-of-simple-features",
    "title": "R spatial in 2017",
    "section": "The limits of simple features",
    "text": "The limits of simple features\nSimple features can’t fully represent GPS and other track data, indexed meshes (like rgl mesh3d, segmented paths), or custom hierarchies like networks, nested objects like counties within states, or arc-node topology (like TopoJSON), and it can’t store aesthetics with primitives (like ggplot2/ggvis, rgl, plotly and others can). R can do all of these things, in many different ways and converting from and to sf is not too difficult.\nPlease don’t get the wrong idea though, sf is invaluable for developing more general tools that can work with these structures. Using the types in sf is very refreshing if you’ve been frustrated trying to pick apart a Spatial object in the past.\nIn terms of going beyong simple features itself, GDAL is also going in this direction: http://lists.osgeo.org/pipermail/gdal-dev/2016-December/045675.html\nWe already have most of this capability in R, it’s just scattered all over the place. I’m interested to collate together the best workflows and see where this can go."
  },
  {
    "objectID": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "href": "posts/2017-01-10_r-spatial-2017/index.html#my-plans",
    "title": "R spatial in 2017",
    "section": "My plans",
    "text": "My plans\nI’m pretty comfortable now with sf and using it for what I want, I have converters to build the forms and workflows I need, and the support in mapview and leaflet and plotly provides more than enough to go with. I will work on making this as accessible and general as possible, and work on integrating it to replace my work on tracking data (the trip and SGAT packages), with 3D models (rbgm, quadmesh) and integrating it with htmlwidgets tools.\nI haven’t yet looked at curves, but I’m keen to see this capability in R both for sf and more generally, we could easily represent the forms made possible by TopoJSON (see “Bostock flawed example”), there is curve support in grid.\nWe could provide smart geo-spatial finite element forms (triangulations, quads),\nI’m keen to see how ggvis and ggplot2 represent geometric types for objects that shared vertices, such as intervals and bar charts, and how we can put in indexed data structures (unlike what ggraph is doing, it builds a mesh from a group of coordinates, you can’t provide it with and index-mesh). I think we can build spatial structures that can store all of these things, so we could throw sf at ggplot2, and use the output as a kind of super-form that knows how to wrap it up into an interactive 4D plot, how to display its primitives etc. etc.\nGIS itself needs what we can already do in R, it’s not a target we are aspring to it’s the other way around."
  }
]