{
  "hash": "779c063fa33c949536e3259f042080f3",
  "result": {
    "markdown": "---\ntitle: \"Web services for scientific data in R\"\nauthor: \"Michael D. Sumner\"\ndate: \"2017-07-25\"\ncategories: [news, code]\ntags: [NetCDF]\n---\n\n\nNOTE: this post has been resurrected from a 2017 post (April 2022). \n\n*This post is in-progress ...*\n\n\n\nTwo very important packages in R are [rerddap](https://CRAN.R-project.org/package=rerddap) and [plotdap](https://github.com/ropensci/plotdap) providing straightforward access to and visualization of time-varying gridded data sets.  Traditionally this is handled by individuals who either have or quickly gain *expert knowledge* about the minute details regarding obtaining data sources, exploring and extracting data from them, manipulating the data for the required purpose and reporting results. Often this task is not the primary goal, it's simply a requirement for *comparison to environmental data* or validation of direct measurements or modelled scenarios against independent data. \n\nThis is a complex area, it touches on **big data**, **web services**, complex and sophisticated **multi-dimensional scientific data in array formats** (primarily *NetCDF*), **map projections**, and **data aesthetics** or *scaling* methods by which data values are converted into visualizations. \n\nR is not known to be strong in this area for **handling large and complex array-based data**, although it has had good support for \nevery piece in the chain many of them were either not designed to work together or or languished without modernization for some time. There are many many approaches to *bring it all together* but unfortunately no concerted effort to sort it all out. What there is however is a very exciting and productive wave of experimentation and new packages to try, there's a lot of exploration occurring and a lot of powerful new approaches. \n\n[ROpenSci](https://ropensci.org/) is producing a variety of valuable new R packages for scientific exploration and analysis. It and the **RConsortium** are both contributing directly into this ecosystem, with the latter helping to foster developments in [simple features (polygons, lines and points)](https://CRAN.R-project.org/package=rerddap), [interactive map editing](https://CRAN.R-project.org/package=rerddap) and an [integration of large raster data handling](https://github.com/r-spatial/stars). \n\n\n## Cool right!?\n\nThis is extremely cool, there is a lot of exciting new support for these sometimes challenging sources of data. However, there is unfortunately no concerted vision for integration of multi-dimensional data into the [tidyverse](https:://tidyverse.org/) and many of the projects created for *data getting* and *data extraction*  must include their own internal handlers for downloading and caching data sources and converting data into required forms. This *is* a complex area, but in some places it is *harder and more complex* than it really needs to be. \n\nTo put some guides in this discussion, the rest of this post is informed by the following themes. \n\n* the tidyverse is not just cool, it's totally awesome (also provides a long-term foundation for the future)\n* \"good software design\" facilitates powerful APIs and strong useability: composable, orthogonal components **and** effortless exploration and workflow development\n* new abstractions are still to be found\n\nThe tidyverse is loudly loved and hated. Critics complain that they already understood long-form data frames and that constant effusive praise on twitter is really annoying. Supporters just sit agog at a constant stream of pointless shiny fashion parading before their eyes ... I mean, they fall into a pit of success and never climb out. Developers who choose the tidyverse as framework for their work appreciate its seamless integration of database principles and *actual databases*, the consistent and systematic syntax of composable single-purpose functions with predictable return types, the modularization and abstractability of *magrittr* piping, and the exciting and disruptive impact of **tidy evaluation** seen clearly already in `dplyr` and family, but which will clearly make it very easy to  traverse the boundary between *being a user* and *being a developer*. What could be a better environment for the future of science and research? \n\n\n## What about the rasters!\n\nI've been using **gridded data** in R since 2002. I remember clearly learning about the use of the `graphics::image` function for visualizing *2D kernel density* maps created using [sm](https://CRAN.R-project.org/package=sm). I have a life-long shudder reflex at the `heat.colors` palette, and at KDE maps generally. I also remember my first encounter with the [NetCDF format](https://www.unidata.ucar.edu/software/netcdf/) which would have looked exactly like this (after waiting half and hour to download [this file](ftp://ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc)). \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(sst.file)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ftp.cdc.noaa.gov/Datasets/noaa.oisst.v2/sst.wkmean.1990-present.nc\"\n```\n:::\n\n```{.r .cell-code}\nlibrary(RNetCDF)\ncon <- open.nc(file.path(dp, sst.file))\nlon <- var.get.nc(con, \"lon\")\nlat <- var.get.nc(con, \"lat\")\nxlim <- c(140, 155)\nylim <- c(-50, -35)\nxsub <- lon >= xlim[1] & lon <= xlim[2]\nysub <- lat >= ylim[1] & lat <= ylim[2]\ntlim  <- \"oh just give up, it's too painful ...\"\ntime <- var.get.nc(con, \"time\")\n## you get the idea, who can be bothered indexing time as well these days\nv <- var.get.nc(con, \"sst\", start = c(which(xsub)[1], length(ysub) - max(which(ysub)), length(time)), count = c(sum(xsub), sum(ysub), 1))\nimage(lon[xsub], rev(lat[ysub]), v[nrow(v):1, ], asp = 1/cos(42 * pi/180))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\nWhat a hassle!  Let's just use [raster](https://CRAN.R-project.org/package=raster). \n(These aren't the same but I really don't care about making sure the old way works, the new way is much better - when it works, which is *mostly* ...). \n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(raster)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: sp\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'raster'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nb <- brick(file.path(dp, sst.file))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required namespace: ncdf4\n```\n:::\n\n```{.r .cell-code}\nnlayers(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1685\n```\n:::\n\n```{.r .cell-code}\nraster(b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclass      : RasterLayer \ndimensions : 180, 360, 64800  (nrow, ncol, ncell)\nresolution : 1, 1  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \n```\n:::\n\n```{.r .cell-code}\n## what time did you want?\n\nplot(crop(subset(b, nlayers(b) - c(1, 0)), extent(xlim, ylim), snap = \"out\"), col = heat.colors(12))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\nThese are pretty cruddy data anyway, *1 degree resolution*, *weekly time steps*? Come on man!  \n\nWhy is this data set relevant? For a very long time the **Optimally Interpolated Sea Surface Temperature** data set, known fondly as *Reynolds SST* in some circles, was a very important touchstone for those working in *marine animal tracking*. From the earliest days of tuna tracking by [(PDF): Northwest Pacific by light-level geo-locators](https://swfsc.noaa.gov/publications/TM/SWFSC/NOAA-TM-NMFS-SWFC-60.PDF), a [regional or global data set of surface ocean temperatures](http://onlinelibrary.wiley.com/doi/10.1029/JC091iC11p12879/full) was a critical  comparison for tag-measured water temperatures. The strong and primarily *zonal*-gradients (i.e. varying by latitude, it gets cold as you move towards the poles) in the oceans provided an informative corrective to \"light level geo-location\" latitude estimates, especially when plagued by [*total zonal ambiguity* (see Figure 12.3)](http://publishing.cdlib.org/ucpressebooks/view?docId=ft7b69p131&chunk.id=d0e20466&toc.depth=1&brand=ucpress) around the equinoxes. \n\nToday we can use much finer resoution *blended* products for the entire globe. Blended means it's a combination of measured (remote-sensing, bucket off a ship) and modelled observations, that's been interpolated to \"fill gaps\". This is not a simple topic of course, remotely sensed temperatures must consider whether it is day or night, how windy it is, the presence of sea ice, and many other factors - but as a global science community we have developed to the point of delivering a single agreed data set for this property. And now that it's 2017, you have the chance of downloading all 5000 or so daily files, the total is only 2000 Gb. \n\nSo nothing's free right? You want high-resolution, you get a big download bill. \n\n## Web services for scientific array data\n\nAh, no - we don't have to download every large data set. That's where [ERDDAP](http://coastwatch.pfeg.noaa.gov/erddap/index.html) comes in! \n\n\nThis makes it easy, but I'm still not happy. In this code a raw NetCDF file is downloaded but is not readily useable by other processes, it's not obvious how to connect directly to the source with NetCDF API, the raster data itself is turned into both a data frame, and  turned into a grid ignoring irregularity in the coordinates, the raster is then resized and possibly reprojected, then turned into a *polygon layer* (eek) and finally delivered to the user as a very simple high level function that accepts standard grammar expressions of the tidyverse. \n\n\n\nWhat follows is some raw but real examples of using an in-development package `tidync` in the hypertidy family. It's very much work-in-progress, as is this blog post ...\n\nPlease reach out to discuss any of this if you are interested!\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"rerddap\")\n#devtools::install_github(\"ropensci/plotdap\")\nlibrary(rerddap)\nlibrary(plotdap)\nlibrary(ggplot2)\nsstInfo <- info('jplMURSST41')\n#system.time({  ## 26 seconds\nmurSST <- griddap(sstInfo, latitude = c(22., 51.), longitude = c(-140., -105),\n                  time = c('last','last'), fields = 'analysed_sst')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ninfo() output passed to x; setting base url to: https://upwell.pfeg.noaa.gov/erddap\n```\n:::\n\n```{.r .cell-code}\n#})\nf <- attr(murSST, \"path\")\n#unlink(f)\n\n## the murSST (it's a GHRSST L4  foundational SST product ) is an extremely detailed raster source, it's really the only\n## daily blended (remote sensing + model) and interpolated (no-missing values)\n## Sea Surface Temperature for global general usage that is high resolution.\n## The other daily blended product Optimally Interpolated (OISST) is only 0.25 degree resolution\n## The GHRSST product is available since 2002, whereas OISST is available since\n## 1981 (the start of the AVHRR sensor era)\nmaxpixels <- 50000\ndres <- c(mean(diff(sort(unique(murSST$data$lon)))), mean(diff(sort(unique(murSST$data$lat)))))\nlibrary(raster)\nr <- raster(extent(range(murSST$data$lon) + c(-1, 1) * dres[1]/2, range(murSST$data$lat) + c(-1, 1) * dres[2]/2),\n     res = dres, crs = \"+init=epsg:4326\")\n\ndim(r) <- dim(r)[1:2] %/% sqrt(ceiling(ncell(r) / maxpixels))\n\ndat <- murSST$data %>%\n mutate(bigcell = cellFromXY(r, cbind(lon, lat))) %>%\n    group_by(time, bigcell) %>%\n  summarize(analysed_sst = mean(analysed_sst, na.rm = FALSE)) %>%\n  ungroup() %>%\n  mutate(lon = xFromCell(r, bigcell), lat = yFromCell(r, bigcell))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`summarise()` has grouped output by 'time'. You can override using the\n`.groups` argument.\n```\n:::\n\n```{.r .cell-code}\nr[] <- NA\nr[dat$bigcell] <- dat$analysed_sst\nnames(r) <- \"analysed_sst\"\ndat$bigcell <- NA\n\n#m <- sf::st_as_sf(maps::map(\"world\", region = \"USA\"))\nbgMap <- sf::st_as_sf( maps::map('world', plot = FALSE, fill = TRUE))\n\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_raster(data = dat, aes(x = lon, y = lat, fill = analysed_sst))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\n## now, what happened before?\n\n#system.time({p <- sf::st_as_sf(raster::rasterToPolygons(r))})\n## should be a bit faster due to use of implicit coordinate mesh\nsystem.time({p <- sf::st_as_sf(spex::polygonize(r, na.rm = TRUE))})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  0.766   0.004   0.770 \n```\n:::\n\n```{.r .cell-code}\n## plot(p, border = NA)\nggplot() + geom_sf(data = bgMap) + xlim(xmin(r), xmax(r)) + ylim(ymin(r), ymax(r)) +\n  geom_sf(data = p, aes(fill = analysed_sst), colour = \"transparent\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nold-style crs object detected; please recreate object with a recent sf::st_crs()\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nu <- \"http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41\"\n\nlibrary(tidync)\nlibrary(dplyr)\ntnc <- tidync::tidync(u)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nnot a file: \n' http://coastwatch.pfeg.noaa.gov/erddap/griddap/jplMURSST41 '\n\n... attempting remote connection\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nConnection succeeded.\n```\n:::\n\n```{.r .cell-code}\ntnc  ## notice there are four variables in this active space\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.707458e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name    length     min    max start count    dmin   dmax unlim coord_dim \n  <chr> <chr>    <dbl>   <dbl>  <dbl> <int> <int>   <dbl>  <dbl> <lgl> <lgl>     \n1 D0    latitu…  17999 -9.00e1 9.00e1     1 17999 -9.00e1 9.00e1 FALSE TRUE      \n2 D1    longit…  36000 -1.80e2 1.8 e2     1 36000 -1.80e2 1.8 e2 FALSE TRUE      \n3 D2    time      7265  1.02e9 1.65e9     1  7265  1.02e9 1.65e9 FALSE TRUE      \n```\n:::\n\n```{.r .cell-code}\nhf <- tnc %>% hyper_filter(longitude = longitude >= -140 & longitude <= -105, latitude = latitude >= 22 & latitude <= 51,\n                       time = index == max(index))\nhf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nData Source (1): jplMURSST41 ...\n\nGrids (4) <dimension family> : <associated variables> \n\n[1]   D1,D0,D2 : analysed_sst, analysis_error, mask, sea_ice_fraction    **ACTIVE GRID** ( 4.707458e+12  values per variable)\n[2]   D0       : latitude\n[3]   D1       : longitude\n[4]   D2       : time\n\nDimensions 3 (all active): \n  \n  dim   name   length     min    max start count    dmin    dmax unlim coord_dim \n  <chr> <chr>   <dbl>   <dbl>  <dbl> <int> <int>   <dbl>   <dbl> <lgl> <lgl>     \n1 D0    latit…  17999 -9.00e1 9.00e1 11200  2901  2.2 e1  5.1 e1 FALSE TRUE      \n2 D1    longi…  36000 -1.80e2 1.8 e2  4000  3501 -1.4 e2 -1.05e2 FALSE TRUE      \n3 D2    time     7265  1.02e9 1.65e9  7265     1  1.65e9  1.65e9 FALSE TRUE      \n```\n:::\n\n```{.r .cell-code}\n## looking ok, so let's go for gold!\n## specify just sst, otherwise we will get all four\n## hyper_tibble gets the raw arrays with ncvar_get(conn, start = , count = ) calls\n## then expands out the axes based on the values from the filtered axis tables\nsystem.time({\n tab <- hf %>% hyper_tibble(select_var = \"analysed_sst\")\n})\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   user  system elapsed \n  3.290   2.338  22.892 \n```\n:::\n\n```{.r .cell-code}\n# system.time({  ## 210 seconds\n#   hs <- hyper_slice(hf, select_var = \"analysed_sst\")\n# })\n# hyper_index(hf)\n# nc <- ncdf4::nc_open(u)\n# system.time({  ## 144 seconds\n#   l <- ncdf4::ncvar_get(nc, \"analysed_sst\", start = c(4000, 2901, 5531), count = c(3501, 2901, 1))\n# })\n```\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": null
  }
}