{
  "hash": "7fadb5663b40aa1fd79652dc52a0ed96",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Integrated Digital East Antarctica, software and tools\"\nauthor: \"Michael Sumner\"\nformat: html\neditor: visual\neditor_options: \n  chunk_output_type: console\ndraft: true\n---\n\n\n\n## raadtools and bowerbird\n\nFor some time the [{raadtools}](https://github.com/AustralianAntarcticDivision/raadtools) and [{bowerbird}](https://docs.ropensci.org/bowerbird/) packages have been used by data science admins at the Australian Antarctic Division.\n\nThe adminstrator experience is one of maintaining a collection of data-getter configurations that results in a local copy of heavily-used environmental data products.\n\nThe user experience is one of loading the raadtools package for access to functions that read time-steps from the data library of various environmental products. These products include sea surface temperature, sea ice concentration, ocean altimetry, ocean colour, bathymetry and topography, and ocean currents.\n\nThe user can invoke functions such as 'readice()', 'readsst()', 'readghrsst()', 'read_adt_daily()' that by default return the latest available time step, or a specific time step. We can easily find the earliest available by setting `latest = FALSE`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(raadtools)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: raster\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: sp\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nglobal option 'raadfiles.data.roots' set:\n'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n/rdsi/PRIVATE/raad/data               \n /rdsi/PRIVATE/raad/data_local         \n /rdsi/PRIVATE/raad/data_staging       \n /rdsi/PRIVATE/raad/data_deprecated    \n /rdsi/PUBLIC/raad/data                \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nUploading raad file cache as at 2025-02-06 12:04:40 (1649115 files listed) \n```\n\n\n:::\n\n```{.r .cell-code}\nreadice()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass      : RasterLayer \ndimensions : 332, 316, 104912  (nrow, ncol, ncell)\nresolution : 25000, 25000  (x, y)\nextent     : -3950000, 3950000, -3950000, 4350000  (xmin, xmax, ymin, ymax)\ncrs        : +proj=stere +lat_0=-90 +lat_ts=-70 +lon_0=0 +x_0=0 +y_0=0 +a=6378273 +rf=298.279411123061 +units=m +no_defs \nsource     : memory\nnames      : layer \nvalues     : 0.4, 100  (min, max)\ntime       : 2025-02-03 \n```\n\n\n:::\n\n```{.r .cell-code}\nreadsst()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass      : RasterLayer \ndimensions : 720, 1440, 1036800  (nrow, ncol, ncell)\nresolution : 0.25, 0.25  (x, y)\nextent     : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : memory\nnames      : Daily.sea.surface.temperature \nvalues     : -1.8, 32.79  (min, max)\ntime       : 2025-02-03 \n```\n\n\n:::\n\n```{.r .cell-code}\n#readghrsst(latest = FALSE)\nread_adt_daily(latest = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass      : RasterBrick \ndimensions : 720, 1440, 1036800, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.25, 0.25  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5009 \nmax values :                      1.6582 \ntime       : 1993-01-01 \n```\n\n\n:::\n\n```{.r .cell-code}\nread_adt_daily(\"2025-01-30\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass      : RasterBrick \ndimensions : 1440, 2880, 4147200, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.125, 0.125  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5703 \nmax values :                      1.9685 \ntime       : 2025-01-30 \n```\n\n\n:::\n:::\n\n\n\nThese data objects are geospatial-standard raster layers, for extracting values at points, aggregating values under polygons and lines, and calculating other aggregate summaries over time. There are helpers specific to raadtools that will extract values at long,lat,time points, this is a convenience enabled by the geospatial-standard status of these layers (extent, shape, crs).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnuyina$adt <- extract(read_adt_daily, nuyina)\ndplyr::sample_n(nuyina[, c(\"longitude\", \"latitude\", \"adt\")], 100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 100 × 3\n   longitude latitude    adt\n       <dbl>    <dbl>  <dbl>\n 1      148.    -43.5  0.565\n 2      129.    -53.2 -0.379\n 3      147.    -42.9 NA    \n 4      111.    -59.9 -1.12 \n 5      140.    -47.0  0.569\n 6      110.    -61.6 -1.26 \n 7      131.    -52.1 -0.222\n 8      151.    -45.4  0.575\n 9      147.    -42.9 NA    \n10      117.    -58.7 -0.874\n# ℹ 90 more rows\n```\n\n\n:::\n:::\n\n\n\nJust under the hood of raadtools are file-collection lists that implicitly define a \"data cube\" that exists for a given variable. These look like this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(files <- sstfiles(returnfiles = TRUE))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15,862 × 3\n   date                fullname                                            root \n   <dttm>              <chr>                                               <chr>\n 1 1981-09-01 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 2 1981-09-02 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 3 1981-09-03 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 4 1981-09-04 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 5 1981-09-05 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 6 1981-09-06 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 7 1981-09-07 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 8 1981-09-08 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n 9 1981-09-09 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n10 1981-09-10 00:00:00 /rdsi/PUBLIC/raad/data/www.ncei.noaa.gov/data/sea-… /rds…\n# ℹ 15,852 more rows\n```\n\n\n:::\n:::\n\n\n\nThe 'fullname' is the local path to the file, the 'date' is the derived time stamp of the data layer, and there's implicit 'band=1' for these where its assumed one-file is one-time-step. When that's not true we simply copy the file name and set explicity \"band=*i*\" for whatever local position in the file the time step lives.\n\nThis relationship between the file set and the user-access function is informal, it's obviously not a general solution - we're assuming time series of 2D raster layers indexed by date, but this covers a huge amount of use cases and has been a helpful model for us.\n\nIn most part we can update the local root '/rdsi/PUBLIC/raad/data' to 'https://' and that allows us to know where the source file was obtained from.\n\nAnyone can setup and run raadtools, but we are already in a post-download-data world. In practice it is used on curated cloud computing that we manage in a small-scale way. No on external can just install raadtools and use it. (Administrators can follow our lead, but again only a handful of people ever do this),\n\nAdministrators manage the data library by setting up configurations such as this one, and {bowerbird} uses this to go-get the data every day. Files are downloaded if they are not already available locally, or if they have been updated at the source. There is an ongoing slow change in most datasets as they get reprocessed, added to, or conventions change orientation, resolution, coverage in time and space.\n\nThe {blueant} package contains a set of pre-configured templates for data sets we use routinely.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmysrc <- blueant::sources(\"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\", hemisphere = \"south\",time_resolutions = \"day\")\n\nstr(mysrc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [1 × 16] (S3: tbl_df/tbl/data.frame)\n $ id                 : chr \"10.5067/MPYG15WAA4WX\"\n $ name               : chr \"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 2\"\n $ description        : chr \"Passive microwave estimates of sea ice concentration at 25km spatial resolution. Daily and monthly resolution, \"| __truncated__\n $ doc_url            : chr \"https://nsidc.org/data/nsidc-0051/versions/2\"\n $ source_url         :List of 1\n  ..$ : chr \"https://n5eil01u.ecs.nsidc.org/PM/NSIDC-0051.002/\"\n $ citation           : chr \"DiGirolamo NE, Parkinson CL, Cavalieri DJ, Gloersen P, Zwally HJ (2022, updated yearly). Sea Ice Concentrations\"| __truncated__\n $ license            : chr \"As a condition of using these data, you must include a citation.\"\n $ comment            : chr NA\n $ method             :List of 1\n  ..$ :List of 7\n  .. ..$                        : chr \"bb_handler_earthdata\"\n  .. ..$ relative               : logi TRUE\n  .. ..$ accept_follow          : chr \"[[:digit:]]{4}\\\\.[[:digit:]]{2}\\\\.[[:digit:]]{2}/\"\n  .. ..$ accept_download        : chr \"PS_S25km.*\\\\.nc$\"\n  .. ..$ reject_download        : chr \"\\\\.(xml|png)$\"\n  .. ..$ level                  : num 2\n  .. ..$ allow_unrestricted_auth: logi TRUE\n $ postprocess        :List of 1\n  ..$ : list()\n $ authentication_note: chr \"Requires Earthdata login, see https://urs.earthdata.nasa.gov/. Note that you will also need to authorize the ap\"| __truncated__\n $ user               : chr \"\"\n $ password           : chr \"\"\n $ access_function    : chr \"raadtools::readice\"\n $ data_group         : chr \"Sea ice\"\n $ collection_size    : num 5\n```\n\n\n:::\n:::\n\n\n\nEach configuration contains details about the source, where to find the source urls, how to filter for files of interest, slots for authentication, and some provenance. This can be use to set up a spidering job as well, to simply find files that are available without downloading them.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(bowerbird)\n## we won't download anything\nmy_directory <- tempdir()\ncf <- bb_config(local_file_root = my_directory)\n\nmysrc <-  bb_modify_source(mysrc, user = Sys.getenv(\"EARTHDATA_USER\"), password = Sys.getenv(\"EARTHDATA_PASS\"),\n          method = list(accept_follow = \"/(2025.02|2025.01)\", accept_download = \".*nc$\", no_host = FALSE))\n\ncf <- bb_add(cf, mysrc)\nresult <- bb_sync(cf, dry_run = TRUE)\n```\n:::\n\n\n\nAs a result we have a list of available files, ... (edit: leaving this as WIP for the moment).\n\nThis blog post shows a more complete example of setting up a mini-raadtools environment from scratch: https://ropensci.org/blog/2018/11/13/antarctic/\n\n## Modern perspectives\n\nWe recognize that this is a dated model and doesn't mesh well with modern practice.\n\nData cubes: we now expect that \"whole datasets\" from the earliest time available to the very latest available can be loaded in one abstract object. This is a reasonable expectation and has been a long time coming.\n\nWe also expect that non-aligned data can be streamed at use-time into a standardized grid, this has been most clearly demonstrated by STAC and use of Sentinel and Landsat, but is also true for any disparate sources of data that are measured for a given area. We can define an output grid for analysis, and regrid on-the-fly using GDAL's warp engine or tools like ESMF.\n\nWe want to maintain a toolkit like this for our existing and future R users, but at every point we are reviewing how much commitment we make and what changes we will actually support. We can also provide python tooling, but that is well-defined and we just enable that usage where we need.\n\n## A prototype raadtools-replacement\n\nThe most valuable thing we have from the bowerbird/raadtools system is bowerbird itself (a huge body of knowledge for spidering and downloading from a diverse set of data providers).\n\n{bowerbird} is really the magic behind this system, and as adminstrators and data-accessors we will continue to use it to discover data that is not already well catalogued in modern ways ...\n\nWhat would we update if raadtols as-is was to be upgraded?\n\nWe should use {terra} as the default output format. (It is currently still {raster} which was born on top of {rgdal}/{sp}, down defunct and disfavoured respectively).\n\nUse of more GDAL VRT. There are tasks that can be easily replaced by VRT, that include\n\n-   augmenting missing metadata (netcdf typically doesn't store an explicit crs for longlat data)\n-   swapping hemispheres from 0,360 or -180,180 convention data\n-   augmenting incorrect georeferencing (GHRSST/MURSST benefits from an assigned geotransform, to fix the broken 1D coordinate arrays that aren't sufficiently precise in Float32)\n-   combining disparate layers, the two polar grids of sea ice concentration can be easily streamed into a global grid (transverse merctor, longlat)\n-   many others, specific to dataset vagaries\n\nWe explored many of these options, and then developed quite a few of them into GDAL itself, if GDAL can recognized or workaround a problem data set then we can remove R code that understands them and it will also work in other programming languages. For us these included\n\n-   GRIB files that weren't recognized as being in polar projections\n-   issues with the warper when using degenerate rectlinear coordinates\n-   many new \"gdal_translate\" options for the \"vrt://\" connection that provides a general mechanism to easily inline \"VRT fixes\" in a single line of text.\n\nThe resource should publically available on any machine. We could do this by exposing our data library via web services, but we have concerns about permission to do that and so we are working on a case-by-case basis to determine some of these overal questions.\n\nIt should work as well in Python. R and Python are very different in terms of their assumed support for gridded data. They have a lot in common, but we can't assumed GDAL-layers via terra in Python (rasterio is close functionally but it wouldn't be sensible to equate a SpatRaster with active GDAL bindings to a rasterio object with active GDAL bindings, they are idiomatically different because of differences in how people have used those languages). This is a primary reason why we have settled on the barest-in-common features. What we can share seamlessly between R and Python is a set of data sources (files or objects) with some kind of catalogue.\n\n## Example of working with problem datasets\n\nOn 20 November 2024 the 0.25 degree grids from Copernicus altimetry had finished being published. This means that there is a new dataset that is on a 0.125 degree grid, and these are now separate datasets.\n\nIt looks like this in raadtools:\n\n``` r\nlibrary(raadtools)\nread_adt_daily(\"2024-11-19\")\nclass      : RasterBrick \ndimensions : 720, 1440, 1036800, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.25, 0.25  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.5285 \nmax values :                      2.1569 \ntime       : 2024-11-19 \n\nread_adt_daily(\"2024-11-20\")\nclass      : RasterBrick \ndimensions : 1440, 2880, 4147200, 1  (nrow, ncol, ncell, nlayers)\nresolution : 0.125, 0.125  (x, y)\nextent     : 0, 360, -90, 90  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +a=6378136.3 +rf=298.257 +no_defs \nsource     : memory\nnames      : Absolute.dynamic.topography \nmin values :                     -1.6685 \nmax values :             \n```\n\nThat is a huge problem if someone tries to read a multi-day dataset with this function, but otherwise won't cause problems for value extraction. It's a break in the assumptions our file-handling has made though and we need to fix it.\n\nIf we get these files from our object storage we can do this now in a less language-specific way.\n\n(here we are accessing files that are otherwise available directly from Copernicus marine, but please treat this as an example of how we can increment support in our existing tools rather than as an example of \"best practice\" if we were to choose the best options off the shelf today)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobjects <- arrow::read_parquet(\"https://projects.pawsey.org.au/idea-objects/idea-curated-objects.parquet\")\n## dataset identifier we want\ndsid <- \"SEALEVEL_GLO_PHY_L4\"\n\n(altimetryfiles <- dplyr::filter(objects, Dataset == dsid))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11,610 × 4\n   Bucket                                      Key   date                Dataset\n   <chr>                                       <chr> <dttm>              <chr>  \n 1 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-01 00:00:00 SEALEV…\n 2 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-02 00:00:00 SEALEV…\n 3 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-03 00:00:00 SEALEV…\n 4 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-04 00:00:00 SEALEV…\n 5 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-05 00:00:00 SEALEV…\n 6 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-06 00:00:00 SEALEV…\n 7 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-07 00:00:00 SEALEV…\n 8 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-08 00:00:00 SEALEV…\n 9 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-09 00:00:00 SEALEV…\n10 idea-sealevel-glo-phy-l4-rep-observations-… data… 1993-01-10 00:00:00 SEALEV…\n# ℹ 11,600 more rows\n```\n\n\n:::\n:::\n\n\n\nFirst a function to match input date/date-time/date-timestring to a set of reference dates. We won't go into the details of this here, but this captures all the logic used in raadtools to give a consistent and reasonable set of date-layers for user input.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## validation for input dates (qdate) against reference dates (fdate)\n## returns the index of qdate within fdate where fdate are the actual time steps, strictly monotonic increasing\n## this function returns an index into fdate for every qdate that is 1) valid 2) near enough to an fdate (tolerance 2x minimum interval)\n## sort and uniqueness are forced on the input, returns 0-length quietly if none are valid \nifun <- function(qdate, fdate) {\n  if (length(qdate) < 1 || length(fdate) < 1) return(NULL)\n qdate <-  try(as.POSIXct(qdate, tz = \"UTC\"), silent = TRUE)\n fdate <- try(as.POSIXct(fdate, tz = \"UTC\"), silent = TRUE)\n if (inherits(qdate, \"try-error\") || inherits(fdate, \"try-error\")) return(NULL)\n mintimeres <- min(diff(unclass(as.POSIXct(fdate, tz = \"UTC\"))))\n \n df <- data.frame(qdate = qdate, idate = findInterval(qdate, fdate))\n df <- df[!is.na(df$idate), ]\n df <- df[!duplicated(df$idate), ]\n df <- df[order(df$idate), ]\n idate <- df$idate\n ## clamp index\n idate[idate < 1] <- 1\n idate[idate > length(fdate)] <- length(fdate)\n difdate <- abs(unclass(fdate[idate]) - unclass(df$qdate))\n ## now finally kill all matches that are different by 2x the minimum interval\n bad <- difdate > (2 * mintimeres)\n idate[!bad]\n}\n```\n:::\n\n\n\nNow, we can write a function to take user input and return the relevant files.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nget_altimetry <- function(date) {\n  objects <- arrow::read_parquet(\"https://projects.pawsey.org.au/idea-objects/idea-curated-objects.parquet\")\n  ## dataset identifier we want\n  dsid <- \"SEALEVEL_GLO_PHY_L4\"\n\n  altimetryfiles <- dplyr::filter(objects, Dataset == dsid)\n  index <- ifun(date, altimetryfiles$date)\n  if (length(index) < 1) stop(\"no input dates match available altimetry data\")\n  \n  altimetryfiles[index, ]\n}\n\nget_altimetry(\"2024-11-11\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  Bucket                               Key           date                Dataset\n  <chr>                                <chr>         <dttm>              <chr>  \n1 idea-sealevel-glo-phy-l4-nrt-008-046 data.marine.… 2024-11-11 00:00:00 SEALEV…\n```\n\n\n:::\n\n```{.r .cell-code}\nget_altimetry(seq(as.Date(\"1994-01-01\"), as.Date(\"2024-01-01\"), by = \"1 year\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 31 × 4\n   Bucket                                      Key   date                Dataset\n   <chr>                                       <chr> <dttm>              <chr>  \n 1 idea-sealevel-glo-phy-l4-rep-observations-… data… 1994-01-01 00:00:00 SEALEV…\n 2 idea-sealevel-glo-phy-l4-rep-observations-… data… 1995-01-01 00:00:00 SEALEV…\n 3 idea-sealevel-glo-phy-l4-rep-observations-… data… 1996-01-01 00:00:00 SEALEV…\n 4 idea-sealevel-glo-phy-l4-rep-observations-… data… 1997-01-01 00:00:00 SEALEV…\n 5 idea-sealevel-glo-phy-l4-rep-observations-… data… 1998-01-01 00:00:00 SEALEV…\n 6 idea-sealevel-glo-phy-l4-rep-observations-… data… 1999-01-01 00:00:00 SEALEV…\n 7 idea-sealevel-glo-phy-l4-rep-observations-… data… 2000-01-01 00:00:00 SEALEV…\n 8 idea-sealevel-glo-phy-l4-rep-observations-… data… 2001-01-01 00:00:00 SEALEV…\n 9 idea-sealevel-glo-phy-l4-rep-observations-… data… 2002-01-01 00:00:00 SEALEV…\n10 idea-sealevel-glo-phy-l4-rep-observations-… data… 2003-01-01 00:00:00 SEALEV…\n# ℹ 21 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nget_altimetry(\"2025-01-01\")$Key\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_NRT_008_046/cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.125deg_P1D_202411/2025/01/nrt_global_allsat_phy_l4_20250101_20250104.nc\"\n```\n\n\n:::\n:::\n\n\n\nWe know that files with `0.125` in them are different grids to those with `0.25deg`, but as a workaround we can standardize the output to always be of the higher resolution.\n\nWe do this for an example with only one of the available variables, `adt`. (absolute dynamic topography, which is sea surface height).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(\"AWS_S3_ENDPOINT\" = \"projects.pawsey.org.au\")\nSys.setenv(\"AWS_VIRTUAL_HOSTING\" = \"FALSE\")\nget_adt <- function(date) {\n  files <- get_altimetry(date)\n  sourcepath <- sprintf(\"/vsis3/%s/%s\", files$Bucket, files$Key)\n  tibble::tibble(date = files$date, source = sprintf(\"vrt://%s?sd_name=adt&outsize=2880,1440\", sourcepath))\n}\nget_adt(\"2003-01-01\")$source\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"vrt:///vsis3/idea-sealevel-glo-phy-l4-rep-observations-008-047/data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_MY_008_047/cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D_202112/2003/01/dt_global_allsat_phy_l4_20030101_20210726.nc?sd_name=adt&outsize=2880,1440\"\n```\n\n\n:::\n\n```{.r .cell-code}\nget_adt(\"2025-01-01\")$source\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"vrt:///vsis3/idea-sealevel-glo-phy-l4-nrt-008-046/data.marine.copernicus.eu/SEALEVEL_GLO_PHY_L4_NRT_008_046/cmems_obs-sl_glo_phy-ssh_nrt_allsat-l4-duacs-0.125deg_P1D_202411/2025/01/nrt_global_allsat_phy_l4_20250101_20250104.nc?sd_name=adt&outsize=2880,1440\"\n```\n\n\n:::\n\n```{.r .cell-code}\n## with which we can now write a standard and entirely lazy reader with geospatial tools\nread_adt <- function(date, grid = NULL) {\n  files <- get_adt(date)\n  if (!is.null(grid)) {\n    out <- terra::project(terra::rast(files$source), grid)\n  } else {\n    out <- terra::rast(files$source)\n  }\n  out\n}\n```\n:::\n\n\n\n## sooty demo\n\nTaking what we have back to its foundations, the set of files that we have knowledge of for each dataset is very general and cross-language. We can get the set of curated files for each dataset and then create an object-oriented interface like this in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(S7)\ndataset <- new_class(name = \"dataset\",                   \n                     properties = list(\n                       id = new_property(class = class_character, default = \"id\"),\n                       n = new_property(class = class_integer, getter = function(self) nrow(self@source)),\n                       mindate = new_property(class = class_POSIXct, getter = function(self) min(self@source$date)), \n                       maxdate = new_property(class = class_POSIXct, getter = function(self) max(self@source$date)),\n                       source = new_property(\n                         class = class_data.frame,\n                         getter = function(self) {\n                           sooty::sooty_files(curated = TRUE) |> dplyr::filter(Dataset == self@id)\n                         } \n                       ))\n                     \n)\n\nd <- dataset()\n\nd@id <- \"NSIDC_SEAICE_PS_S25km\"\n\n\n\n\nfiles <- d@source\nifun(c(\"2025-02-02\"), files$date)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 16896\n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}